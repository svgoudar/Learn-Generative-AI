{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2e834d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Quantization\n",
    "\n",
    "Quantization is a core **model compression and acceleration technique** that converts high-precision numerical representations (e.g., FP32) into lower-precision formats (e.g., INT8, INT4) to make LLMs **faster, smaller, cheaper, and deployable** without major accuracy loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Quantization Matters for LLMs\n",
    "\n",
    "LLMs are dominated by **matrix multiplications** and **memory movement**.\n",
    "\n",
    "| Bottleneck       | Effect                     |\n",
    "| ---------------- | -------------------------- |\n",
    "| Model size       | Does not fit on single GPU |\n",
    "| Memory bandwidth | Slows inference            |\n",
    "| Compute cost     | Expensive deployment       |\n",
    "| Energy           | High power consumption     |\n",
    "\n",
    "**Quantization reduces:**\n",
    "\n",
    "* Model size (≈ 2–8×)\n",
    "* Memory bandwidth\n",
    "* Latency\n",
    "* Power consumption\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Idea\n",
    "\n",
    "Instead of storing weights as 32-bit floats:\n",
    "\n",
    "[\n",
    "W_{fp32} \\rightarrow W_{int8/int4}\n",
    "]\n",
    "\n",
    "We store and compute with **lower precision**, using scale factors to preserve numerical meaning:\n",
    "\n",
    "[\n",
    "x \\approx s \\cdot q\n",
    "]\n",
    "\n",
    "where\n",
    "\n",
    "* (q) = integer value\n",
    "* (s) = scale factor\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Quantization Workflow\n",
    "\n",
    "```\n",
    "FP32 Model\n",
    "   ↓\n",
    "Calibration (collect statistics)\n",
    "   ↓\n",
    "Choose precision (INT8 / INT4)\n",
    "   ↓\n",
    "Compute scale & zero-point\n",
    "   ↓\n",
    "Quantize weights & activations\n",
    "   ↓\n",
    "Optimized Inference Kernel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Types of Quantization in LLMs\n",
    "\n",
    "| Category                          | Description                           |\n",
    "| --------------------------------- | ------------------------------------- |\n",
    "| Post-Training Quantization (PTQ)  | Quantize a pretrained model           |\n",
    "| Quantization-Aware Training (QAT) | Train while simulating quantization   |\n",
    "| Static Quantization               | Fixed ranges                          |\n",
    "| Dynamic Quantization              | Compute ranges at runtime             |\n",
    "| Weight-only Quantization          | Only weights are quantized            |\n",
    "| Activation Quantization           | Weights + activations                 |\n",
    "| Per-Tensor Quantization           | One scale per tensor                  |\n",
    "| Per-Channel Quantization          | One scale per channel (more accurate) |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Common LLM Quantization Formats\n",
    "\n",
    "| Format | Bits | Memory Reduction | Accuracy      |\n",
    "| ------ | ---- | ---------------- | ------------- |\n",
    "| FP16   | 16   | 2×               | High          |\n",
    "| INT8   | 8    | 4×               | Very high     |\n",
    "| INT4   | 4    | 8×               | Moderate–High |\n",
    "| NF4    | 4    | 8×               | High          |\n",
    "| GPTQ   | 4–8  | 4–8×             | High          |\n",
    "| AWQ    | 4    | 8×               | Very high     |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Mathematical View\n",
    "\n",
    "Uniform quantization:\n",
    "\n",
    "[\n",
    "q = \\text{round}\\left(\\frac{x}{s}\\right)\n",
    "\\quad,\\quad\n",
    "x \\approx s \\cdot q\n",
    "]\n",
    "\n",
    "With zero-point (z):\n",
    "\n",
    "[\n",
    "q = \\text{round}\\left(\\frac{x}{s}\\right) + z\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Quantization for Transformer Layers\n",
    "\n",
    "LLM weight matrices:\n",
    "\n",
    "* (W_Q, W_K, W_V)\n",
    "* FFN matrices\n",
    "* Output projection\n",
    "\n",
    "Dominant cost:\n",
    "\n",
    "[\n",
    "XW\n",
    "]\n",
    "\n",
    "After quantization:\n",
    "\n",
    "[\n",
    "X_{int} W_{int} \\Rightarrow \\text{INT kernels} \\Rightarrow \\text{dequantize}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Demonstration (PyTorch Dynamic INT8)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "print(model_int8)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. 4-bit Quantization with BitsAndBytes\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization_config=config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Accuracy vs Performance Tradeoff\n",
    "\n",
    "| Precision | Speed     | Memory     | Accuracy    |\n",
    "| --------- | --------- | ---------- | ----------- |\n",
    "| FP32      | Slow      | Huge       | Baseline    |\n",
    "| FP16      | Fast      | Large      | ~Baseline   |\n",
    "| INT8      | Faster    | Small      | ~Baseline   |\n",
    "| INT4      | Very Fast | Very Small | Slight drop |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. When to Use Which\n",
    "\n",
    "| Scenario        | Recommendation |\n",
    "| --------------- | -------------- |\n",
    "| Training        | FP16 / BF16    |\n",
    "| Fine-tuning     | FP16 + QLoRA   |\n",
    "| Inference (GPU) | INT8 / AWQ     |\n",
    "| Edge deployment | INT4 / GPTQ    |\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Modern LLM Quantization Techniques\n",
    "\n",
    "| Method      | Key Idea                              |\n",
    "| ----------- | ------------------------------------- |\n",
    "| GPTQ        | Layer-wise Hessian-aware quantization |\n",
    "| AWQ         | Activation-aware weight scaling       |\n",
    "| QLoRA       | 4-bit base model + low-rank adapters  |\n",
    "| SmoothQuant | Balance weight & activation ranges    |\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Practical Impact (LLaMA-7B Example)\n",
    "\n",
    "| Precision | VRAM    |\n",
    "| --------- | ------- |\n",
    "| FP16      | ~14 GB  |\n",
    "| INT8      | ~7 GB   |\n",
    "| INT4      | ~3.5 GB |\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Summary\n",
    "\n",
    "Quantization enables:\n",
    "\n",
    "* **Massive model compression**\n",
    "* **Production-grade inference**\n",
    "* **Edge and consumer deployment**\n",
    "* **Minimal accuracy degradation**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
