{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b6e6ce",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Knowledge Distillation\n",
    "\n",
    "**Knowledge Distillation** is a model compression and training paradigm where a **large, powerful model (teacher)** transfers its learned knowledge to a **smaller, faster model (student)** so the student achieves high performance with much lower computational cost.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Distillation Exists\n",
    "\n",
    "Modern models are accurate but expensive:\n",
    "\n",
    "| Problem         | Effect                       |\n",
    "| --------------- | ---------------------------- |\n",
    "| Large models    | High latency, memory, energy |\n",
    "| Edge deployment | Often impossible             |\n",
    "| Cloud cost      | Expensive at scale           |\n",
    "\n",
    "**Goal of KD:**\n",
    "\n",
    "> Preserve **teacher-level performance** while achieving **student-level efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Intuition\n",
    "\n",
    "Instead of training the student only on hard labels:\n",
    "\n",
    "```\n",
    "cat → 1\n",
    "dog → 0\n",
    "```\n",
    "\n",
    "we train it using the **teacher’s probability distribution**:\n",
    "\n",
    "```\n",
    "Teacher: [cat: 0.72, tiger: 0.18, fox: 0.06, dog: 0.04]\n",
    "```\n",
    "\n",
    "This distribution contains **dark knowledge**:\n",
    "\n",
    "* inter-class similarity\n",
    "* decision boundaries\n",
    "* uncertainty information\n",
    "\n",
    "The student learns **how** the teacher thinks, not just what it predicts.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Formal Definition\n",
    "\n",
    "Let:\n",
    "\n",
    "* Teacher: ( T(x) )\n",
    "* Student: ( S(x) )\n",
    "* Ground truth: ( y )\n",
    "* Temperature: ( \\tau )\n",
    "\n",
    "**Soft targets:**\n",
    "\n",
    "[\n",
    "p_T = \\text{softmax}(z_T / \\tau)\n",
    "\\quad\n",
    "p_S = \\text{softmax}(z_S / \\tau)\n",
    "]\n",
    "\n",
    "**Distillation Loss:**\n",
    "\n",
    "[\n",
    "\\mathcal{L} = \\alpha \\cdot CE(y, S) + (1-\\alpha) \\cdot \\tau^2 \\cdot KL(p_T | p_S)\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Training Workflow\n",
    "\n",
    "```\n",
    "1. Train Teacher model\n",
    "2. Freeze Teacher\n",
    "3. For each batch:\n",
    "      - Teacher produces soft labels\n",
    "      - Student predicts\n",
    "      - Compute distillation loss\n",
    "      - Update Student\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Minimal PyTorch Example\n",
    "\n",
    "```python\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=4.0, alpha=0.7):\n",
    "    hard_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    soft_student = F.log_softmax(student_logits / T, dim=1)\n",
    "    soft_teacher = F.softmax(teacher_logits / T, dim=1)\n",
    "    \n",
    "    soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (T * T)\n",
    "    \n",
    "    return alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Types of Distillation\n",
    "\n",
    "| Type                | Description                             |\n",
    "| ------------------- | --------------------------------------- |\n",
    "| Response-based      | Match output probabilities (classic KD) |\n",
    "| Feature-based       | Match internal representations          |\n",
    "| Relation-based      | Match relationships between samples     |\n",
    "| Self-distillation   | Model teaches its smaller version       |\n",
    "| Online distillation | Teacher and student trained together    |\n",
    "| Multi-teacher       | Ensemble of teachers                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Feature Distillation Example\n",
    "\n",
    "```python\n",
    "loss = mse(student_feature_map, teacher_feature_map)\n",
    "```\n",
    "\n",
    "Used heavily in **CNN compression** and **vision transformers**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Where Distillation Is Used\n",
    "\n",
    "| Domain  | Use Case                 |\n",
    "| ------- | ------------------------ |\n",
    "| NLP     | BERT → DistilBERT        |\n",
    "| Vision  | ResNet-152 → ResNet-18   |\n",
    "| Speech  | Large ASR → mobile ASR   |\n",
    "| LLMs    | GPT → on-device LLMs     |\n",
    "| Edge AI | Cloud → embedded devices |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Distillation vs Fine-tuning\n",
    "\n",
    "| Aspect             | Distillation | Fine-tuning |\n",
    "| ------------------ | ------------ | ----------- |\n",
    "| Teacher used       | Yes          | No          |\n",
    "| Model size change  | Yes          | No          |\n",
    "| Knowledge transfer | Explicit     | Implicit    |\n",
    "| Goal               | Compression  | Adaptation  |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Benefits and Limitations\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Large speedup\n",
    "* Lower memory\n",
    "* Often improves generalization\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* Requires strong teacher\n",
    "* Careful tuning of ( \\tau ) and ( \\alpha )\n",
    "* Student architecture still matters\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Conceptual Summary\n",
    "\n",
    "```\n",
    "Big model → distilled knowledge → Small model\n",
    "Accuracy preserved, cost reduced\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
