{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0590900a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## A/B Testing\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**A/B testing in Generative AI** is a controlled experimental framework for comparing two or more versions of a generative system (models, prompts, pipelines, or policies) under real usage to determine which produces better outcomes according to defined metrics.\n",
    "\n",
    "Formally:\n",
    "\n",
    "> Given variants ( A ) and ( B ), expose users or tasks randomly and independently, measure performance, and perform statistical inference to select the superior system.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why A/B Testing is Special for Generative AI\n",
    "\n",
    "| Classical A/B        | Generative AI A/B                     |\n",
    "| -------------------- | ------------------------------------- |\n",
    "| Deterministic output | **Stochastic output**                 |\n",
    "| Single scalar metric | **Multi-dimensional quality metrics** |\n",
    "| Stable responses     | **High variance responses**           |\n",
    "| Easy ground truth    | **Often no single ground truth**      |\n",
    "\n",
    "Challenges unique to GenAI:\n",
    "\n",
    "* Non-determinism from sampling (temperature, top-p)\n",
    "* Human-in-the-loop evaluation\n",
    "* Subjective quality metrics\n",
    "* Prompt and context sensitivity\n",
    "* Latency–quality–cost tradeoffs\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What Can Be A/B Tested in GenAI\n",
    "\n",
    "| Layer           | Examples                    |\n",
    "| --------------- | --------------------------- |\n",
    "| Model           | GPT-4 vs fine-tuned GPT-3.5 |\n",
    "| Prompt          | Prompt A vs Prompt B        |\n",
    "| Decoding        | temperature=0.7 vs 0.2      |\n",
    "| Retrieval       | Vector DB v1 vs v2          |\n",
    "| Tool policy     | With tools vs without tools |\n",
    "| Safety filters  | Strict vs relaxed           |\n",
    "| System pipeline | RAG vs non-RAG              |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Core Workflow\n",
    "\n",
    "```\n",
    "Design → Randomize → Deploy → Collect → Evaluate → Decide\n",
    "```\n",
    "\n",
    "### Step-by-step\n",
    "\n",
    "1. **Define objective**\n",
    "\n",
    "   * e.g., maximize helpfulness while keeping latency < 2s\n",
    "\n",
    "2. **Select metrics**\n",
    "\n",
    "| Category | Example Metrics        |\n",
    "| -------- | ---------------------- |\n",
    "| Quality  | Human rating, win-rate |\n",
    "| Safety   | Toxicity score         |\n",
    "| Cost     | Tokens / request       |\n",
    "| Latency  | P95 response time      |\n",
    "| Business | Conversion rate        |\n",
    "\n",
    "3. **Random assignment**\n",
    "\n",
    "[\n",
    "P(\\text{variant A}) = P(\\text{variant B}) = 0.5\n",
    "]\n",
    "\n",
    "4. **Run experiment**\n",
    "5. **Collect logs**\n",
    "6. **Statistical analysis**\n",
    "7. **Ship winner**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Metrics for Generative AI\n",
    "\n",
    "| Metric             | Description               |\n",
    "| ------------------ | ------------------------- |\n",
    "| Human Preference   | A > B pairwise judgments  |\n",
    "| Win Rate           | % times variant wins      |\n",
    "| LLM-as-Judge       | Automated evaluator model |\n",
    "| Task Success       | Did user goal complete    |\n",
    "| Hallucination Rate | False factual statements  |\n",
    "| Cost Efficiency    | Quality / $               |\n",
    "\n",
    "**Important:** Always combine **automatic + human** evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Statistical Evaluation\n",
    "\n",
    "Because outputs are noisy, GenAI A/B tests require:\n",
    "\n",
    "* Large sample sizes\n",
    "* Paired comparison when possible\n",
    "* Non-parametric tests\n",
    "\n",
    "Common tests:\n",
    "\n",
    "| Scenario          | Test                  |\n",
    "| ----------------- | --------------------- |\n",
    "| Binary win/loss   | Chi-square            |\n",
    "| Paired judgments  | Wilcoxon signed-rank  |\n",
    "| Continuous scores | t-test / Mann-Whitney |\n",
    "| Multiple variants | ANOVA                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Example: Prompt A/B Test (Code)\n",
    "\n",
    "```python\n",
    "import random\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated human preference data\n",
    "# 1 = A wins, 0 = B wins\n",
    "results = [1,1,0,1,1,0,1,1,1,0,1,1,1,1,0]\n",
    "\n",
    "win_rate_A = sum(results)/len(results)\n",
    "\n",
    "# Hypothesis test\n",
    "stat, p = stats.binomtest(sum(results), len(results), 0.5).statistic, \\\n",
    "          stats.binomtest(sum(results), len(results), 0.5).pvalue\n",
    "\n",
    "print(\"Win rate A:\", win_rate_A)\n",
    "print(\"p-value:\", p)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. LLM-as-Judge Evaluation\n",
    "\n",
    "```python\n",
    "def judge(output_A, output_B, reference):\n",
    "    prompt = f\"\"\"\n",
    "    Compare two answers and choose the better one.\n",
    "\n",
    "    Reference: {reference}\n",
    "    A: {output_A}\n",
    "    B: {output_B}\n",
    "\n",
    "    Answer only: A or B\n",
    "    \"\"\"\n",
    "    return llm(prompt)\n",
    "```\n",
    "\n",
    "This enables **scalable automated A/B testing** before human review.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Multi-Metric Decision Table\n",
    "\n",
    "| Variant | Win Rate | Hallucination | Cost      | Latency  | Decision |\n",
    "| ------- | -------- | ------------- | --------- | -------- | -------- |\n",
    "| A       | 62%      | 3%            | $0.02     | 1.3s     | ❌        |\n",
    "| B       | 58%      | **1%**        | **$0.01** | **0.9s** | ✅        |\n",
    "\n",
    "Final choice optimizes **overall system utility**, not just quality.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Advanced Techniques\n",
    "\n",
    "* **Contextual bandits**: adaptive traffic allocation\n",
    "* **Sequential testing**: stop early when confident\n",
    "* **Offline evaluation** with replay logs\n",
    "* **Prompt ensembles**\n",
    "* **Pareto frontier optimization**\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Failure Modes\n",
    "\n",
    "* Overfitting to short-term metrics\n",
    "* Ignoring variance from sampling\n",
    "* Inadequate sample size\n",
    "* Single-metric optimization\n",
    "* No human validation\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Summary\n",
    "\n",
    "> **A/B testing is the core scientific instrument for improving Generative AI systems in production.**\n",
    "\n",
    "It enables:\n",
    "\n",
    "* Objective model comparison\n",
    "* Safe deployment\n",
    "* Continuous improvement\n",
    "* Measurable product gains\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
