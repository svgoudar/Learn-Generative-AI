{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7156d59b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Architecture & Infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Big Picture: What Is “Architecture & Infrastructure” in Generative AI?\n",
    "\n",
    "| Layer                     | Purpose                                                                      |\n",
    "| ------------------------- | ---------------------------------------------------------------------------- |\n",
    "| **Model Architecture**    | Mathematical structure of the model (e.g., Transformer, Diffusion)           |\n",
    "| **Training Architecture** | How the model is trained (distributed systems, optimization, data pipelines) |\n",
    "| **Serving Architecture**  | How models are deployed and used in production                               |\n",
    "| **Infrastructure**        | Hardware, networking, storage, orchestration, security, scaling              |\n",
    "\n",
    "Generative AI systems are **full-stack systems**, not just neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Model Architectures\n",
    "\n",
    "| Model Type         | Used For                | Core Components                                   |\n",
    "| ------------------ | ----------------------- | ------------------------------------------------- |\n",
    "| Transformer (LLMs) | Text, code              | Self-attention, feed-forward, positional encoding |\n",
    "| Diffusion Models   | Images, video           | U-Net, noise scheduler                            |\n",
    "| VAEs               | Representation learning | Encoder, decoder, latent space                    |\n",
    "| GANs               | Image generation        | Generator, discriminator                          |\n",
    "\n",
    "**Transformer (LLM) Anatomy**\n",
    "\n",
    "```\n",
    "Input → Embedding → N × [Attention → MLP] → LayerNorm → Output Head\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Training Architecture (Offline Pipeline)\n",
    "\n",
    "```\n",
    "Data → Cleaning → Tokenization → Sharding → Distributed Training → Checkpointing\n",
    "```\n",
    "\n",
    "#### Distributed Training\n",
    "\n",
    "| Parallelism          | Purpose                     |\n",
    "| -------------------- | --------------------------- |\n",
    "| Data Parallelism     | Split data across GPUs      |\n",
    "| Model Parallelism    | Split model across GPUs     |\n",
    "| Pipeline Parallelism | Split layers across devices |\n",
    "| ZeRO / FSDP          | Memory optimization         |\n",
    "\n",
    "**Training Stack**\n",
    "\n",
    "| Layer               | Example              |\n",
    "| ------------------- | -------------------- |\n",
    "| Framework           | PyTorch, JAX         |\n",
    "| Distributed Runtime | NCCL, Ray, DeepSpeed |\n",
    "| Storage             | S3, GCS, HDFS        |\n",
    "| Scheduler           | Kubernetes, Slurm    |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Inference & Serving Architecture\n",
    "\n",
    "```\n",
    "User → API Gateway → Load Balancer → Model Server → GPU → Response\n",
    "```\n",
    "\n",
    "**Model Serving Components**\n",
    "\n",
    "| Component        | Role                   |\n",
    "| ---------------- | ---------------------- |\n",
    "| Tokenizer        | Convert text to tokens |\n",
    "| Inference Engine | Executes model         |\n",
    "| KV Cache         | Speeds up decoding     |\n",
    "| Batching Engine  | Groups requests        |\n",
    "| Autoscaler       | Adds/removes replicas  |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Infrastructure Stack\n",
    "\n",
    "| Layer         | Examples                |\n",
    "| ------------- | ----------------------- |\n",
    "| Compute       | GPUs (A100, H100), TPUs |\n",
    "| Networking    | InfiniBand, NVLink      |\n",
    "| Storage       | Object stores, SSDs     |\n",
    "| Orchestration | Kubernetes              |\n",
    "| Observability | Prometheus, Grafana     |\n",
    "| Security      | IAM, secrets manager    |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. End-to-End System Flow\n",
    "\n",
    "```\n",
    "[User]\n",
    "   ↓\n",
    "[API Gateway]\n",
    "   ↓\n",
    "[Request Router]\n",
    "   ↓\n",
    "[Inference Cluster]\n",
    "   ↓\n",
    "[GPU Workers]\n",
    "   ↓\n",
    "[Postprocessing]\n",
    "   ↓\n",
    "[Response]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Minimal Deployment Example (HuggingFace + FastAPI)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(prompt: str):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    return tokenizer.decode(outputs[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Common Architecture Patterns\n",
    "\n",
    "| Pattern                  | Description                 |\n",
    "| ------------------------ | --------------------------- |\n",
    "| Single-Model API         | One large foundation model  |\n",
    "| MoE (Mixture of Experts) | Sparse expert routing       |\n",
    "| RAG                      | Retrieval + Generation      |\n",
    "| Multi-Model Pipelines    | Chain of specialized models |\n",
    "| Edge + Cloud             | On-device inference + cloud |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Scalability & Optimization Techniques\n",
    "\n",
    "| Problem       | Solution                           |\n",
    "| ------------- | ---------------------------------- |\n",
    "| High latency  | Quantization, batching, KV caching |\n",
    "| Memory limits | ZeRO, offloading                   |\n",
    "| Cost          | Spot instances, autoscaling        |\n",
    "| Throughput    | Tensor parallelism                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why This Architecture Matters\n",
    "\n",
    "* Enables **training trillion-parameter models**\n",
    "* Supports **millions of concurrent users**\n",
    "* Balances **latency, cost, accuracy, reliability**\n",
    "\n",
    "Generative AI is fundamentally a **distributed systems problem** wrapped around deep learning.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
