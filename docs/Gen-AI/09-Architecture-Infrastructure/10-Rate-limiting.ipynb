{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b881ff6a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Rate Limiting\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**Rate limiting** is a control mechanism that restricts how many requests a client (user, application, API key, IP) can make to a **Generative AI service** within a fixed time window.\n",
    "\n",
    "It protects:\n",
    "\n",
    "* **Model availability**\n",
    "* **Infrastructure stability**\n",
    "* **Fair usage**\n",
    "* **Cost predictability**\n",
    "* **Abuse prevention**\n",
    "\n",
    "Formally:\n",
    "\n",
    "> A policy that enforces an upper bound on request volume per identity per time interval.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Rate Limiting Is Critical for Generative AI\n",
    "\n",
    "Generative models are **compute-intensive** and **stateful**.\n",
    "\n",
    "| Risk Without Rate Limiting | Consequence          |\n",
    "| -------------------------- | -------------------- |\n",
    "| DDoS or abuse              | Service outage       |\n",
    "| Runaway loops / agents     | Unbounded cost       |\n",
    "| Single tenant overload     | Starvation of others |\n",
    "| Prompt spamming            | Model degradation    |\n",
    "| Cost explosion             | Budget failure       |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What Is Being Limited?\n",
    "\n",
    "Generative AI systems typically limit **multiple dimensions** simultaneously:\n",
    "\n",
    "| Dimension           | Example              |\n",
    "| ------------------- | -------------------- |\n",
    "| Requests            | 60 requests / minute |\n",
    "| Tokens in           | 100K tokens / minute |\n",
    "| Tokens out          | 50K tokens / minute  |\n",
    "| Concurrent requests | 5 parallel calls     |\n",
    "| Compute             | GPU-seconds / minute |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Core Rate Limiting Strategies\n",
    "\n",
    "| Strategy           | Description                   | When Used              |\n",
    "| ------------------ | ----------------------------- | ---------------------- |\n",
    "| **Fixed Window**   | Count requests per interval   | Simple APIs            |\n",
    "| **Sliding Window** | Continuous rolling window     | Smoother control       |\n",
    "| **Token Bucket**   | Accumulate tokens over time   | Bursty traffic         |\n",
    "| **Leaky Bucket**   | Enforces constant output rate | Streaming stability    |\n",
    "| **Adaptive**       | Dynamically adjusts limits    | AI workload management |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Conceptual Architecture in Generative AI APIs\n",
    "\n",
    "```text\n",
    "Client → API Gateway → Rate Limiter → Prompt Router → Model Server → Response\n",
    "```\n",
    "\n",
    "The **rate limiter** executes **before** the model is invoked.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Example Policy\n",
    "\n",
    "| Metric           | Limit            |\n",
    "| ---------------- | ---------------- |\n",
    "| Requests         | 60 / minute      |\n",
    "| Input tokens     | 100,000 / minute |\n",
    "| Output tokens    | 50,000 / minute  |\n",
    "| Concurrent calls | 3                |\n",
    "\n",
    "Violation triggers:\n",
    "\n",
    "* HTTP **429 Too Many Requests**\n",
    "* Optional `Retry-After` header\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Demonstration with Code\n",
    "\n",
    "#### Token Bucket Implementation (Python)\n",
    "\n",
    "```python\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate, capacity):\n",
    "        self.rate = rate              # tokens per second\n",
    "        self.capacity = capacity\n",
    "        self.tokens = capacity\n",
    "        self.timestamp = time.time()\n",
    "\n",
    "    def allow(self, cost=1):\n",
    "        now = time.time()\n",
    "        delta = now - self.timestamp\n",
    "        self.tokens = min(self.capacity, self.tokens + delta * self.rate)\n",
    "        self.timestamp = now\n",
    "\n",
    "        if self.tokens >= cost:\n",
    "            self.tokens -= cost\n",
    "            return True\n",
    "        return False\n",
    "```\n",
    "\n",
    "#### Applying to a Generative AI Request\n",
    "\n",
    "```python\n",
    "bucket = TokenBucket(rate=10, capacity=20)\n",
    "\n",
    "def call_model(prompt_tokens):\n",
    "    if not bucket.allow(cost=prompt_tokens):\n",
    "        raise Exception(\"Rate limit exceeded\")\n",
    "    return generate_text()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Token-Based Rate Limiting (LLM-Specific)\n",
    "\n",
    "Unlike traditional APIs, LLMs limit **tokens**, not just requests.\n",
    "\n",
    "| Why tokens?                                                          |\n",
    "| -------------------------------------------------------------------- |\n",
    "| Long prompts and long generations cost more compute than short ones. |\n",
    "\n",
    "Example policy:\n",
    "\n",
    "```\n",
    "100,000 input tokens / minute\n",
    "50,000 output tokens / minute\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Multi-Tier Rate Limiting in AI Platforms\n",
    "\n",
    "| Tier       | Typical Limits              |\n",
    "| ---------- | --------------------------- |\n",
    "| Free       | 10 req/min, 10K tokens/min  |\n",
    "| Pro        | 60 req/min, 100K tokens/min |\n",
    "| Enterprise | Custom quotas               |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Interaction with Agents & Tools\n",
    "\n",
    "Autonomous agents can accidentally violate limits via loops:\n",
    "\n",
    "```text\n",
    "Planner → Tool → Model → Tool → Model → ...\n",
    "```\n",
    "\n",
    "Mitigation:\n",
    "\n",
    "* **Per-agent quotas**\n",
    "* **Global token budget**\n",
    "* **Hard execution caps**\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Relationship to Cost Control\n",
    "\n",
    "Since LLM cost ∝ tokens processed:\n",
    "\n",
    "> **Rate limiting is also budget limiting.**\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Summary Table\n",
    "\n",
    "| Aspect          | Role                   |\n",
    "| --------------- | ---------------------- |\n",
    "| Protects system | Prevents overload      |\n",
    "| Protects users  | Fair resource sharing  |\n",
    "| Protects budget | Predictable spend      |\n",
    "| Protects model  | Prevents misuse        |\n",
    "| Controls agents | Stops runaway behavior |\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Key Takeaway\n",
    "\n",
    "> **Rate limiting is the primary safety valve of large-scale generative AI infrastructure.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
