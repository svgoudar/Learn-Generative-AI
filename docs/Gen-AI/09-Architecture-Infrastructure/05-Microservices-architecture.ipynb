{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06c0678",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Microservice Architecture \n",
    "\n",
    "### 1. Motivation\n",
    "\n",
    "Modern **Generative AI systems** (LLMs, multimodal models, agents, RAG pipelines) are:\n",
    "\n",
    "* **Large-scale**\n",
    "* **Latency-sensitive**\n",
    "* **Rapidly evolving**\n",
    "* **Cost-intensive**\n",
    "\n",
    "A **microservice architecture** enables:\n",
    "\n",
    "| Requirement     | Benefit                                                           |\n",
    "| --------------- | ----------------------------------------------------------------- |\n",
    "| Scalability     | Independent scaling of heavy components (LLM, embeddings, search) |\n",
    "| Modularity      | Swap models, tools, vector DBs without rewriting the system       |\n",
    "| Reliability     | Failures isolated to individual services                          |\n",
    "| Experimentation | A/B test models & prompts safely                                  |\n",
    "| Cost control    | Scale expensive services only when needed                         |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Concept\n",
    "\n",
    "**Microservice Architecture** decomposes a GenAI system into **small, independent services** that communicate over APIs.\n",
    "\n",
    "Each service owns **one responsibility**.\n",
    "\n",
    "```\n",
    "User → API Gateway → Orchestrator → {LLM | Retrieval | Memory | Tools | Safety | Logging}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Typical GenAI Microservice Decomposition\n",
    "\n",
    "| Service               | Responsibility                              |\n",
    "| --------------------- | ------------------------------------------- |\n",
    "| API Gateway           | Authentication, rate limits, routing        |\n",
    "| Orchestrator / Agent  | Workflow control, tool selection            |\n",
    "| Prompt Service        | Prompt templates, versioning                |\n",
    "| LLM Service           | Model inference (OpenAI, local, fine-tuned) |\n",
    "| Embedding Service     | Text → vector                               |\n",
    "| Retrieval Service     | Vector search / RAG                         |\n",
    "| Memory Service        | Long-term / conversation memory             |\n",
    "| Tool Service          | External APIs, functions                    |\n",
    "| Safety Service        | Moderation, PII filtering                   |\n",
    "| Observability Service | Logs, traces, metrics                       |\n",
    "| Caching Service       | Response & embedding cache                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reference Architecture\n",
    "\n",
    "```\n",
    "                 ┌──────────────┐\n",
    "User ──HTTPS──▶  │ API Gateway  │\n",
    "                 └─────┬────────┘\n",
    "                       ▼\n",
    "                ┌─────────────┐\n",
    "                │ Orchestrator│\n",
    "                └─────┬───────┘\n",
    "        ┌──────────────┼──────────────────┐\n",
    "        ▼              ▼                  ▼\n",
    "  Prompt Svc     Retrieval Svc       Tool Svc\n",
    "        │              │                  │\n",
    "        ▼              ▼                  ▼\n",
    "   LLM Service   Vector DB / RAG     External APIs\n",
    "        │\n",
    "        ▼\n",
    "  Safety + Logging + Cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Communication Patterns\n",
    "\n",
    "| Pattern                           | Usage                               |\n",
    "| --------------------------------- | ----------------------------------- |\n",
    "| REST / gRPC                       | Low-latency synchronous calls       |\n",
    "| Async Messaging (Kafka, RabbitMQ) | Long workflows, event-driven agents |\n",
    "| Streaming (WebSockets, SSE)       | Token streaming from LLM            |\n",
    "| Service Mesh                      | Observability, retries, security    |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Workflow Example: RAG Query\n",
    "\n",
    "**User Question → Answer**\n",
    "\n",
    "1. API Gateway receives request\n",
    "2. Orchestrator requests embedding\n",
    "3. Retrieval Service fetches documents\n",
    "4. Prompt Service assembles context\n",
    "5. LLM Service generates response\n",
    "6. Safety filters output\n",
    "7. Cache & Logging store results\n",
    "\n",
    "```\n",
    "Question\n",
    "   ↓\n",
    "Embedding → Vector DB → Context\n",
    "   ↓\n",
    "Prompt Assembly\n",
    "   ↓\n",
    "LLM Inference\n",
    "   ↓\n",
    "Safety → Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Example: Minimal Python Microservices (FastAPI)\n",
    "\n",
    "**LLM Service**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from openai import OpenAI\n",
    "\n",
    "app = FastAPI()\n",
    "client = OpenAI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(prompt: str):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "```\n",
    "\n",
    "**Embedding Service**\n",
    "\n",
    "```python\n",
    "@app.post(\"/embed\")\n",
    "def embed(text: str):\n",
    "    return client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        input=text\n",
    "    )\n",
    "```\n",
    "\n",
    "**Orchestrator**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def answer(query):\n",
    "    emb = requests.post(\"http://embed:8000/embed\", json=query).json()\n",
    "    docs = search_vector_db(emb)\n",
    "    prompt = build_prompt(query, docs)\n",
    "    return requests.post(\"http://llm:8000/generate\", json=prompt).json()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Deployment & Scaling\n",
    "\n",
    "| Layer             | Technology                         |\n",
    "| ----------------- | ---------------------------------- |\n",
    "| Containers        | Docker                             |\n",
    "| Orchestration     | Kubernetes                         |\n",
    "| Service Discovery | Consul, Kubernetes DNS             |\n",
    "| API Gateway       | Kong, NGINX                        |\n",
    "| Autoscaling       | HPA, KEDA                          |\n",
    "| Observability     | Prometheus, Grafana, OpenTelemetry |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Patterns Specific to Generative AI\n",
    "\n",
    "| Pattern                 | Purpose                       |\n",
    "| ----------------------- | ----------------------------- |\n",
    "| Model Abstraction Layer | Swap OpenAI ↔ local models    |\n",
    "| Prompt Versioning       | Reproducible experiments      |\n",
    "| Response Caching        | Reduce LLM cost               |\n",
    "| Model Router            | Route queries by cost/quality |\n",
    "| Tool-augmented Agents   | Dynamic workflows             |\n",
    "| Human-in-the-loop       | Safety & review               |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Comparison with Monolithic Design\n",
    "\n",
    "| Aspect            | Monolith     | Microservices |\n",
    "| ----------------- | ------------ | ------------- |\n",
    "| Scaling           | Whole system | Per-service   |\n",
    "| Model upgrades    | Risky        | Isolated      |\n",
    "| Failure impact    | System-wide  | Contained     |\n",
    "| Experimentation   | Hard         | Easy          |\n",
    "| Cost optimization | Coarse       | Fine-grained  |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. When Microservices Become Necessary\n",
    "\n",
    "Use microservices when:\n",
    "\n",
    "* Multiple models are in production\n",
    "* RAG pipelines are complex\n",
    "* High traffic and strict latency SLAs\n",
    "* Rapid experimentation & iteration\n",
    "* Multi-team development\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Key Takeaways\n",
    "\n",
    "* Microservices provide **control, scalability, and reliability** for GenAI systems.\n",
    "* They decouple **models, data, prompts, tools, memory, and safety**.\n",
    "* They enable **continuous improvement** without downtime.\n",
    "* They are foundational for **agentic systems and enterprise GenAI platforms**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
