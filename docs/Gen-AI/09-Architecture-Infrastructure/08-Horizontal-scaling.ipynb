{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f54979",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Horizontal Scaling \n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**Horizontal scaling** (a.k.a. *scale-out*) in Generative AI is the practice of increasing system capacity by **adding more machines or nodes** rather than making individual machines more powerful.\n",
    "\n",
    "> *Goal:* Support larger models, higher throughput, lower latency, and higher reliability by distributing computation and data across many devices.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Horizontal Scaling is Essential in GenAI\n",
    "\n",
    "Generative AI workloads are characterized by:\n",
    "\n",
    "| Challenge             | Why Horizontal Scaling Helps          |\n",
    "| --------------------- | ------------------------------------- |\n",
    "| Massive models        | Models exceed single-machine memory   |\n",
    "| High training cost    | Parallelizes compute across GPUs      |\n",
    "| High inference demand | Serves many users simultaneously      |\n",
    "| Fault tolerance       | Failures isolated to individual nodes |\n",
    "| Elastic workloads     | Add/remove nodes based on demand      |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Where Horizontal Scaling is Used\n",
    "\n",
    "| Layer                | Purpose                                                 |\n",
    "| -------------------- | ------------------------------------------------------- |\n",
    "| Training             | Distribute model training across GPUs/nodes             |\n",
    "| Inference            | Handle large concurrent user requests                   |\n",
    "| Data processing      | Parallelize dataset loading, tokenization, augmentation |\n",
    "| Retrieval systems    | Scale vector databases & search                         |\n",
    "| Monitoring & logging | Scale observability pipelines                           |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Horizontal vs Vertical Scaling\n",
    "\n",
    "| Feature          | Horizontal Scaling | Vertical Scaling        |\n",
    "| ---------------- | ------------------ | ----------------------- |\n",
    "| How              | Add more machines  | Make machines larger    |\n",
    "| Cost efficiency  | High               | Limited                 |\n",
    "| Fault tolerance  | Strong             | Weak                    |\n",
    "| Upper bound      | Very high          | Hardware-limited        |\n",
    "| Typical in GenAI | **Yes**            | Rarely sufficient alone |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Core Horizontal Scaling Patterns in GenAI\n",
    "\n",
    "### 5.1 Data Parallelism\n",
    "\n",
    "Each worker holds a full model copy and processes different data batches.\n",
    "\n",
    "```\n",
    "Dataset → split → [GPU1] [GPU2] [GPU3] [GPU4]\n",
    "```\n",
    "\n",
    "**Gradient aggregation** keeps models synchronized.\n",
    "\n",
    "```python\n",
    "# PyTorch DDP example\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "model = DDP(model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Model Parallelism\n",
    "\n",
    "Split the model itself across devices.\n",
    "\n",
    "```\n",
    "[Layers 1-10] → GPU1\n",
    "[Layers 11-20] → GPU2\n",
    "```\n",
    "\n",
    "Used when the model cannot fit on a single device.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Pipeline Parallelism\n",
    "\n",
    "Different micro-batches flow through sequential model partitions.\n",
    "\n",
    "```\n",
    "GPU1 → GPU2 → GPU3 → GPU4\n",
    "```\n",
    "\n",
    "Improves device utilization for very deep models.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Tensor Parallelism\n",
    "\n",
    "Split large matrix operations across GPUs.\n",
    "\n",
    "```\n",
    "Huge Matrix Multiply → shard across GPUs\n",
    "```\n",
    "\n",
    "Used heavily in LLM training (Megatron-LM style).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Horizontal Scaling in Training Workflow\n",
    "\n",
    "```\n",
    "Data Storage\n",
    "    ↓\n",
    "Distributed DataLoader\n",
    "    ↓\n",
    "[ Worker Node 1 ] ----\\\n",
    "[ Worker Node 2 ] -----→ Gradient Sync → Parameter Update\n",
    "[ Worker Node 3 ] ----/\n",
    "    ↓\n",
    "Checkpoint Store\n",
    "```\n",
    "\n",
    "Key technologies:\n",
    "\n",
    "* NCCL / Gloo (communication backends)\n",
    "* Ray, DeepSpeed, Horovod, FSDP, Megatron-LM\n",
    "* Kubernetes for cluster orchestration\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Horizontal Scaling in Inference\n",
    "\n",
    "**Request-based scaling:**\n",
    "\n",
    "```\n",
    "Client Requests → Load Balancer → Inference Nodes (replicas)\n",
    "```\n",
    "\n",
    "**Key mechanisms:**\n",
    "\n",
    "* Auto-scaling based on QPS / latency\n",
    "* Model sharding across GPUs\n",
    "* KV-cache partitioning for LLMs\n",
    "* Batch inference for throughput optimization\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Example: Horizontally Scaled Inference with Ray\n",
    "\n",
    "```python\n",
    "import ray\n",
    "\n",
    "@ray.remote(num_gpus=1)\n",
    "class LLMWorker:\n",
    "    def generate(self, prompt):\n",
    "        return model.generate(prompt)\n",
    "\n",
    "workers = [LLMWorker.remote() for _ in range(8)]\n",
    "\n",
    "results = ray.get([w.generate.remote(\"Hello\") for w in workers])\n",
    "```\n",
    "\n",
    "Adds throughput by simply increasing worker count.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Benefits\n",
    "\n",
    "* Near-linear scaling of compute\n",
    "* Supports trillion-parameter models\n",
    "* Enables global deployment\n",
    "* High availability & fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Practical Limitations\n",
    "\n",
    "| Bottleneck    | Cause                                |\n",
    "| ------------- | ------------------------------------ |\n",
    "| Communication | Gradient & parameter synchronization |\n",
    "| Network       | Bandwidth & latency                  |\n",
    "| Memory        | Activations & optimizer states       |\n",
    "| Stragglers    | Slow nodes reduce overall speed      |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Summary\n",
    "\n",
    "Horizontal scaling is the **foundation** of modern Generative AI:\n",
    "\n",
    "* Makes training of massive models feasible\n",
    "* Enables real-time inference at internet scale\n",
    "* Provides elasticity, reliability, and performance\n",
    "* Requires careful orchestration of compute, memory, and communication\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
