{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8e6f56",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Distributed Systems\n",
    "\n",
    "### 1. Motivation & Intuition\n",
    "\n",
    "Modern **Generative AI** models (LLMs, diffusion models, multimodal models) are **too large and too computationally intensive** to be trained or served on a single machine.\n",
    "\n",
    "Distributed systems provide:\n",
    "\n",
    "| Challenge     | Why Distribution is Required                       |\n",
    "| ------------- | -------------------------------------------------- |\n",
    "| Model size    | Models exceed single-GPU memory (100B+ parameters) |\n",
    "| Training time | Months → days via parallelism                      |\n",
    "| Dataset size  | Web-scale data cannot fit on one node              |\n",
    "| Reliability   | Failures inevitable in large clusters              |\n",
    "| Throughput    | Millions of inference requests                     |\n",
    "\n",
    "**Key idea:**\n",
    "\n",
    "> *Split computation, memory, and data across many machines and coordinate them efficiently.*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Components of a GenAI Distributed System\n",
    "\n",
    "```\n",
    "Users → API Gateway → Inference Cluster\n",
    "                 ↘ Training Cluster\n",
    "                 ↘ Data Storage Cluster\n",
    "                 ↘ Parameter Servers / Checkpoint Store\n",
    "```\n",
    "\n",
    "| Layer         | Responsibility                               |\n",
    "| ------------- | -------------------------------------------- |\n",
    "| Compute nodes | GPUs/TPUs executing model code               |\n",
    "| Networking    | High-speed interconnect (NVLink, InfiniBand) |\n",
    "| Storage       | Object stores, distributed filesystems       |\n",
    "| Coordination  | Schedulers, RPC, fault tolerance             |\n",
    "| Orchestration | Kubernetes, Slurm, Ray                       |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Forms of Parallelism in Generative AI\n",
    "\n",
    "| Type                     | What is split              | Used when                  |\n",
    "| ------------------------ | -------------------------- | -------------------------- |\n",
    "| **Data Parallelism**     | Training samples           | Large datasets             |\n",
    "| **Model Parallelism**    | Model layers / tensors     | Models too big for one GPU |\n",
    "| **Pipeline Parallelism** | Sequential stages of model | Deep transformer stacks    |\n",
    "| **Tensor Parallelism**   | Matrix operations          | Transformer attention/MLP  |\n",
    "| **Hybrid Parallelism**   | Combination                | Large-scale LLM training   |\n",
    "\n",
    "#### Example: Hybrid Parallelism\n",
    "\n",
    "```\n",
    "Node Group 1: Data parallel\n",
    " ├─ GPU0: Layer1-4\n",
    " ├─ GPU1: Layer1-4\n",
    "Node Group 2: Model parallel\n",
    " ├─ GPU2: Layer5-8\n",
    " ├─ GPU3: Layer5-8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Training Workflow\n",
    "\n",
    "```\n",
    "Dataset → Sharded Loader\n",
    "       → Distributed Forward Pass\n",
    "       → Gradient All-Reduce\n",
    "       → Optimizer Update\n",
    "       → Checkpoint Sync\n",
    "```\n",
    "\n",
    "**Communication primitives**\n",
    "\n",
    "| Operation        | Purpose           |\n",
    "| ---------------- | ----------------- |\n",
    "| All-Reduce       | Average gradients |\n",
    "| Broadcast        | Share parameters  |\n",
    "| Scatter / Gather | Tensor sharding   |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Inference at Scale\n",
    "\n",
    "| Technique             | Benefit                             |\n",
    "| --------------------- | ----------------------------------- |\n",
    "| Model sharding        | Serve models larger than GPU memory |\n",
    "| KV-cache distribution | Efficient long-context inference    |\n",
    "| Batching              | Maximize GPU utilization            |\n",
    "| Speculative decoding  | Reduce latency                      |\n",
    "| Autoscaling           | Handle variable traffic             |\n",
    "\n",
    "```\n",
    "Load Balancer → Inference Workers → KV Cache Store\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Fault Tolerance & Reliability\n",
    "\n",
    "| Mechanism            | Function                      |\n",
    "| -------------------- | ----------------------------- |\n",
    "| Checkpointing        | Resume training after failure |\n",
    "| Replication          | High availability             |\n",
    "| Heartbeat monitoring | Detect failures               |\n",
    "| Elastic training     | Nodes can join/leave          |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Distributed Data Management\n",
    "\n",
    "| Layer             | Tools                |\n",
    "| ----------------- | -------------------- |\n",
    "| Distributed FS    | HDFS, Ceph           |\n",
    "| Object store      | S3, GCS              |\n",
    "| Streaming         | Kafka, Pulsar        |\n",
    "| Dataset pipelines | WebDataset, TFRecord |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Example: PyTorch Distributed Training\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "dist.init_process_group(\"nccl\")\n",
    "\n",
    "model = MyTransformer().cuda()\n",
    "model = DDP(model)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "for batch in loader:\n",
    "    loss = model(batch).loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Architecture Comparison\n",
    "\n",
    "| Scale                | Typical Stack                |\n",
    "| -------------------- | ---------------------------- |\n",
    "| Single node          | PyTorch + CUDA               |\n",
    "| Small cluster        | PyTorch DDP + NCCL           |\n",
    "| Large cluster        | DeepSpeed, Megatron-LM, FSDP |\n",
    "| Production inference | Ray, Triton, vLLM            |\n",
    "| Cloud orchestration  | Kubernetes, Slurm            |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why Distributed Systems Are Central to GenAI\n",
    "\n",
    "Without distributed systems:\n",
    "\n",
    "| Capability                    | Possible? |\n",
    "| ----------------------------- | --------- |\n",
    "| Train 100B+ LLM               | ❌         |\n",
    "| Real-time global inference    | ❌         |\n",
    "| Multi-modal foundation models | ❌         |\n",
    "| Continuous learning           | ❌         |\n",
    "\n",
    "Distributed systems are **the computational backbone** of modern Generative AI.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Mental Model Summary\n",
    "\n",
    "> **Generative AI = Models × Data × Compute × Coordination**\n",
    "\n",
    "Distributed systems provide the **coordination layer** that makes modern AI possible.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
