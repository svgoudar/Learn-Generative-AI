{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2750730",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Document Stores \n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "A **Document Store** is a specialized data system designed to store, index, retrieve, and serve large collections of unstructured or semi-structured documents for **Generative AI** applications, especially **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "**Core purpose:**\n",
    "\n",
    "> Provide relevant context to language models by retrieving the most useful documents for a given query.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Document Stores Matter in Generative AI\n",
    "\n",
    "Large language models:\n",
    "\n",
    "* Do **not** have access to your private or dynamic data\n",
    "* Have a **fixed knowledge cutoff**\n",
    "* Cannot remember large external corpora\n",
    "\n",
    "Document stores solve this by enabling:\n",
    "\n",
    "| Capability                      | Benefit                          |\n",
    "| ------------------------------- | -------------------------------- |\n",
    "| Fast semantic search            | Finds meaning, not just keywords |\n",
    "| Long-term knowledge persistence | Model answers from your data     |\n",
    "| Dynamic updates                 | New data without retraining      |\n",
    "| Controlled grounding            | Reduces hallucinations           |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Position in the RAG Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "   ↓\n",
    "Embedding Model\n",
    "   ↓\n",
    "Vector Search in Document Store\n",
    "   ↓\n",
    "Top-k Relevant Documents\n",
    "   ↓\n",
    "Prompt Construction\n",
    "   ↓\n",
    "LLM Generation\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Types of Document Stores\n",
    "\n",
    "| Type               | Purpose                       | Examples                  |\n",
    "| ------------------ | ----------------------------- | ------------------------- |\n",
    "| **Vector Store**   | Semantic similarity search    | FAISS, Pinecone, Weaviate |\n",
    "| **Keyword Store**  | Exact / boolean search        | Elasticsearch, OpenSearch |\n",
    "| **Hybrid Store**   | Combine both                  | Vespa, OpenSearch Hybrid  |\n",
    "| **Metadata Store** | Filters, permissions, routing | SQL, MongoDB              |\n",
    "\n",
    "Most production systems combine **vector + keyword + metadata**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Internal Data Model\n",
    "\n",
    "Each stored document is represented as:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"doc_001\",\n",
    "  \"text\": \"Transformers use self-attention...\",\n",
    "  \"embedding\": [0.012, -0.33, ...],\n",
    "  \"metadata\": {\n",
    "      \"source\": \"ml_book.pdf\",\n",
    "      \"page\": 42,\n",
    "      \"date\": \"2024-01-01\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Core Operations\n",
    "\n",
    "| Operation    | Description                                 |\n",
    "| ------------ | ------------------------------------------- |\n",
    "| **Ingest**   | Chunk documents, generate embeddings        |\n",
    "| **Index**    | Organize vectors for fast similarity search |\n",
    "| **Retrieve** | Find top-k relevant chunks                  |\n",
    "| **Filter**   | Apply metadata constraints                  |\n",
    "| **Update**   | Insert, delete, refresh documents           |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Document Ingestion Workflow\n",
    "\n",
    "```\n",
    "Raw Files → Text Extraction → Chunking → Embedding → Store\n",
    "```\n",
    "\n",
    "**Chunking guidelines:**\n",
    "\n",
    "* 200–1000 tokens per chunk\n",
    "* Overlap: 10–20%\n",
    "* Preserve semantic boundaries (paragraphs, sections)\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Retrieval Workflow\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "docs = [\"Transformers use self-attention\", \"CNNs use convolutions\"]\n",
    "embeddings = model.encode(docs)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "query = \"How do transformers work?\"\n",
    "q_emb = model.encode([query])\n",
    "\n",
    "D, I = index.search(q_emb, k=1)\n",
    "print(docs[I[0][0]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How Document Stores Improve Generation\n",
    "\n",
    "| Without Document Store | With Document Store       |\n",
    "| ---------------------- | ------------------------- |\n",
    "| Hallucinations         | Grounded answers          |\n",
    "| Stale knowledge        | Live knowledge            |\n",
    "| Limited memory         | External long-term memory |\n",
    "| Generic output         | Context-specific output   |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Evaluation Metrics\n",
    "\n",
    "| Metric       | Meaning                            |\n",
    "| ------------ | ---------------------------------- |\n",
    "| Recall@k     | Were relevant docs retrieved?      |\n",
    "| MRR          | Ranking quality                    |\n",
    "| Faithfulness | Does answer use retrieved context? |\n",
    "| Latency      | Query speed                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Common Design Patterns\n",
    "\n",
    "| Pattern                 | Use Case               |\n",
    "| ----------------------- | ---------------------- |\n",
    "| **RAG**                 | QA, chatbots, copilots |\n",
    "| **Agent Memory Store**  | Tool-using agents      |\n",
    "| **Knowledge Base**      | Enterprise search      |\n",
    "| **Conversation Memory** | Long-term dialogue     |\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Summary\n",
    "\n",
    "A **Document Store** is the **external memory and knowledge substrate** of Generative AI systems.\n",
    "It enables models to operate over real, private, and continuously evolving information with high accuracy and low hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next topics that naturally follow are:\n",
    "\n",
    "* Advanced RAG architectures\n",
    "* Hybrid retrieval strategies\n",
    "* Document re-ranking and cross-encoders\n",
    "* Memory systems for AI agents\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
