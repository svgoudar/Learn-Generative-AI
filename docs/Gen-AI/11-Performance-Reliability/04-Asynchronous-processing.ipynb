{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65bc7b7",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Asynchronous Processing\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Motivation and Intuition\n",
    "\n",
    "**Asynchronous processing** allows a Generative AI system to **continue operating without waiting** for long-running tasks (model inference, data retrieval, tool calls, streaming responses) to finish.\n",
    "\n",
    "> **Core idea:**\n",
    "> *Do not block the main execution flow while expensive AI operations are running.*\n",
    "\n",
    "This is essential because modern GenAI workloads involve:\n",
    "\n",
    "* Large neural networks (slow inference)\n",
    "* External APIs and tools\n",
    "* Network communication\n",
    "* Streaming tokens to users\n",
    "\n",
    "Without asynchrony, systems become **slow, unscalable, and unresponsive**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Synchronous vs Asynchronous Execution\n",
    "\n",
    "| Feature        | Synchronous             | Asynchronous         |\n",
    "| -------------- | ----------------------- | -------------------- |\n",
    "| Execution      | Blocks until completion | Non-blocking         |\n",
    "| Resource usage | Inefficient             | Efficient            |\n",
    "| Throughput     | Low                     | High                 |\n",
    "| Latency hiding | No                      | Yes                  |\n",
    "| Scalability    | Poor                    | Excellent            |\n",
    "| UX for GenAI   | Sluggish                | Real-time, streaming |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Where Asynchrony Appears in Generative AI\n",
    "\n",
    "| Layer               | Example                               |\n",
    "| ------------------- | ------------------------------------- |\n",
    "| Model Inference     | GPU kernels, batching                 |\n",
    "| Token Streaming     | Streaming partial text responses      |\n",
    "| Retrieval           | Async database / vector store queries |\n",
    "| Tool Calls          | Async API calls                       |\n",
    "| Multi-agent Systems | Concurrent agents                     |\n",
    "| Orchestration       | Workflow engines, task schedulers     |\n",
    "| Serving             | Async web servers (FastAPI, asyncio)  |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Conceptual Workflow\n",
    "\n",
    "```\n",
    "User Request\n",
    "     |\n",
    "     v\n",
    "Async Controller\n",
    "     |\n",
    "     +---> Retrieval Task (async)\n",
    "     |\n",
    "     +---> Tool Call (async)\n",
    "     |\n",
    "     +---> Model Inference (async)\n",
    "     |\n",
    "     v\n",
    "Response Stream → User\n",
    "```\n",
    "\n",
    "Multiple tasks execute (1) without blocking each other.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mathematical View (Queueing Perspective)\n",
    "\n",
    "Let:\n",
    "\n",
    "* ( T_i ) = time of task ( i )\n",
    "* ( n ) tasks per request\n",
    "\n",
    "**Synchronous latency**\n",
    "[\n",
    "T_{sync} = \\sum_{i=1}^{n} T_i\n",
    "]\n",
    "\n",
    "**Asynchronous latency**\n",
    "[\n",
    "T_{async} \\approx \\max(T_1, T_2, ..., T_n)\n",
    "]\n",
    "\n",
    "This yields dramatic latency reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Implementation Example (Python `asyncio`)\n",
    "\n",
    "### Basic Asynchronous Inference Pipeline\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def retrieve_context():\n",
    "    await asyncio.sleep(2)   # simulate database\n",
    "    return \"retrieved knowledge\"\n",
    "\n",
    "async def call_model(prompt):\n",
    "    await asyncio.sleep(3)   # simulate model inference\n",
    "    return f\"model output for: {prompt}\"\n",
    "\n",
    "async def main():\n",
    "    retrieval_task = asyncio.create_task(retrieve_context())\n",
    "    model_task = asyncio.create_task(call_model(\"Explain transformers\"))\n",
    "\n",
    "    context = await retrieval_task\n",
    "    output = await model_task\n",
    "\n",
    "    return context, output\n",
    "\n",
    "result = asyncio.run(main())\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Execution time ≈ 3 seconds** (not 5).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Streaming Tokens Asynchronously\n",
    "\n",
    "```python\n",
    "async def generate():\n",
    "    for token in [\"Hello\", \",\", \" world\", \"!\"]:\n",
    "        await asyncio.sleep(0.5)\n",
    "        yield token\n",
    "\n",
    "async def stream():\n",
    "    async for t in generate():\n",
    "        print(t, end=\"\", flush=True)\n",
    "\n",
    "asyncio.run(stream())\n",
    "```\n",
    "\n",
    "Used by LLM servers to deliver **real-time responses**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Asynchronous in Production GenAI Systems\n",
    "\n",
    "| Component     | Async Mechanism         |\n",
    "| ------------- | ----------------------- |\n",
    "| Web Serving   | FastAPI, Starlette      |\n",
    "| Model Serving | Triton, vLLM, Ray       |\n",
    "| Retrieval     | Async DB drivers        |\n",
    "| Orchestration | Celery, Ray, Temporal   |\n",
    "| Streaming     | WebSockets, SSE         |\n",
    "| Agents        | Event loops, task pools |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Common Asynchronous Patterns\n",
    "\n",
    "| Pattern           | Purpose              |\n",
    "| ----------------- | -------------------- |\n",
    "| Fan-out / Fan-in  | Parallel tool calls  |\n",
    "| Pipelines         | Stage-by-stage async |\n",
    "| Producer–Consumer | Token streaming      |\n",
    "| Task Queues       | Background inference |\n",
    "| Backpressure      | Flow control         |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why It Matters for Generative AI\n",
    "\n",
    "Asynchrony enables:\n",
    "\n",
    "* High throughput LLM serving\n",
    "* Low latency chat systems\n",
    "* Streaming user experience\n",
    "* Efficient GPU utilization\n",
    "* Multi-agent concurrency\n",
    "* Tool-augmented reasoning at scale\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Failure Handling & Reliability\n",
    "\n",
    "Asynchronous systems must handle:\n",
    "\n",
    "* Timeouts\n",
    "* Retries\n",
    "* Cancellation\n",
    "* Partial results\n",
    "* Backpressure\n",
    "\n",
    "These are essential for **robust GenAI infrastructure**.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Summary\n",
    "\n",
    "| Aspect            | Role of Asynchrony |\n",
    "| ----------------- | ------------------ |\n",
    "| Latency           | Minimized          |\n",
    "| Throughput        | Maximized          |\n",
    "| Scalability       | Linear with load   |\n",
    "| User Experience   | Real-time          |\n",
    "| System Efficiency | Optimal            |\n",
    "\n",
    "**Asynchronous processing is the backbone of modern Generative AI systems.**\n",
    "## Asynchronous Processing in Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Motivation and Intuition\n",
    "\n",
    "**Asynchronous processing** allows a Generative AI system to **continue operating without waiting** for long-running tasks (model inference, data retrieval, tool calls, streaming responses) to finish.\n",
    "\n",
    "> **Core idea:**\n",
    "> *Do not block the main execution flow while expensive AI operations are running.*\n",
    "\n",
    "This is essential because modern GenAI workloads involve:\n",
    "\n",
    "* Large neural networks (slow inference)\n",
    "* External APIs and tools\n",
    "* Network communication\n",
    "* Streaming tokens to users\n",
    "\n",
    "Without asynchrony, systems become **slow, unscalable, and unresponsive**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Synchronous vs Asynchronous Execution\n",
    "\n",
    "| Feature        | Synchronous             | Asynchronous         |\n",
    "| -------------- | ----------------------- | -------------------- |\n",
    "| Execution      | Blocks until completion | Non-blocking         |\n",
    "| Resource usage | Inefficient             | Efficient            |\n",
    "| Throughput     | Low                     | High                 |\n",
    "| Latency hiding | No                      | Yes                  |\n",
    "| Scalability    | Poor                    | Excellent            |\n",
    "| UX for GenAI   | Sluggish                | Real-time, streaming |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Where Asynchrony Appears in Generative AI\n",
    "\n",
    "| Layer               | Example                               |\n",
    "| ------------------- | ------------------------------------- |\n",
    "| Model Inference     | GPU kernels, batching                 |\n",
    "| Token Streaming     | Streaming partial text responses      |\n",
    "| Retrieval           | Async database / vector store queries |\n",
    "| Tool Calls          | Async API calls                       |\n",
    "| Multi-agent Systems | Concurrent agents                     |\n",
    "| Orchestration       | Workflow engines, task schedulers     |\n",
    "| Serving             | Async web servers (FastAPI, asyncio)  |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Conceptual Workflow\n",
    "\n",
    "```\n",
    "User Request\n",
    "     |\n",
    "     v\n",
    "Async Controller\n",
    "     |\n",
    "     +---> Retrieval Task (async)\n",
    "     |\n",
    "     +---> Tool Call (async)\n",
    "     |\n",
    "     +---> Model Inference (async)\n",
    "     |\n",
    "     v\n",
    "Response Stream → User\n",
    "```\n",
    "\n",
    "Multiple tasks execute (1) without blocking each other.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mathematical View (Queueing Perspective)\n",
    "\n",
    "Let:\n",
    "\n",
    "* ( T_i ) = time of task ( i )\n",
    "* ( n ) tasks per request\n",
    "\n",
    "**Synchronous latency**\n",
    "[\n",
    "T_{sync} = \\sum_{i=1}^{n} T_i\n",
    "]\n",
    "\n",
    "**Asynchronous latency**\n",
    "[\n",
    "T_{async} \\approx \\max(T_1, T_2, ..., T_n)\n",
    "]\n",
    "\n",
    "This yields dramatic latency reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Implementation Example (Python `asyncio`)\n",
    "\n",
    "### Basic Asynchronous Inference Pipeline\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "async def retrieve_context():\n",
    "    await asyncio.sleep(2)   # simulate database\n",
    "    return \"retrieved knowledge\"\n",
    "\n",
    "async def call_model(prompt):\n",
    "    await asyncio.sleep(3)   # simulate model inference\n",
    "    return f\"model output for: {prompt}\"\n",
    "\n",
    "async def main():\n",
    "    retrieval_task = asyncio.create_task(retrieve_context())\n",
    "    model_task = asyncio.create_task(call_model(\"Explain transformers\"))\n",
    "\n",
    "    context = await retrieval_task\n",
    "    output = await model_task\n",
    "\n",
    "    return context, output\n",
    "\n",
    "result = asyncio.run(main())\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Execution time ≈ 3 seconds** (not 5).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Streaming Tokens Asynchronously\n",
    "\n",
    "```python\n",
    "async def generate():\n",
    "    for token in [\"Hello\", \",\", \" world\", \"!\"]:\n",
    "        await asyncio.sleep(0.5)\n",
    "        yield token\n",
    "\n",
    "async def stream():\n",
    "    async for t in generate():\n",
    "        print(t, end=\"\", flush=True)\n",
    "\n",
    "asyncio.run(stream())\n",
    "```\n",
    "\n",
    "Used by LLM servers to deliver **real-time responses**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Asynchronous in Production GenAI Systems\n",
    "\n",
    "| Component     | Async Mechanism         |\n",
    "| ------------- | ----------------------- |\n",
    "| Web Serving   | FastAPI, Starlette      |\n",
    "| Model Serving | Triton, vLLM, Ray       |\n",
    "| Retrieval     | Async DB drivers        |\n",
    "| Orchestration | Celery, Ray, Temporal   |\n",
    "| Streaming     | WebSockets, SSE         |\n",
    "| Agents        | Event loops, task pools |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Common Asynchronous Patterns\n",
    "\n",
    "| Pattern           | Purpose              |\n",
    "| ----------------- | -------------------- |\n",
    "| Fan-out / Fan-in  | Parallel tool calls  |\n",
    "| Pipelines         | Stage-by-stage async |\n",
    "| Producer–Consumer | Token streaming      |\n",
    "| Task Queues       | Background inference |\n",
    "| Backpressure      | Flow control         |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why It Matters for Generative AI\n",
    "\n",
    "Asynchrony enables:\n",
    "\n",
    "* High throughput LLM serving\n",
    "* Low latency chat systems\n",
    "* Streaming user experience\n",
    "* Efficient GPU utilization\n",
    "* Multi-agent concurrency\n",
    "* Tool-augmented reasoning at scale\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Failure Handling & Reliability\n",
    "\n",
    "Asynchronous systems must handle:\n",
    "\n",
    "* Timeouts\n",
    "* Retries\n",
    "* Cancellation\n",
    "* Partial results\n",
    "* Backpressure\n",
    "\n",
    "These are essential for **robust GenAI infrastructure**.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Summary\n",
    "\n",
    "| Aspect            | Role of Asynchrony |\n",
    "| ----------------- | ------------------ |\n",
    "| Latency           | Minimized          |\n",
    "| Throughput        | Maximized          |\n",
    "| Scalability       | Linear with load   |\n",
    "| User Experience   | Real-time          |\n",
    "| System Efficiency | Optimal            |\n",
    "\n",
    "**Asynchronous processing is the backbone of modern Generative AI systems.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
