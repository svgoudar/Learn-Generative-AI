{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff7af17",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Streaming \n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**Streaming** in Generative AI is the technique of **incrementally delivering model outputs token-by-token (or chunk-by-chunk) as they are generated**, instead of waiting for the entire response to complete.\n",
    "\n",
    "Formally:\n",
    "\n",
    "> Let a generative model produce a sequence\n",
    "> ( Y = (y_1, y_2, ..., y_T) ).\n",
    "> Streaming exposes each ( y_t ) immediately after it is sampled.\n",
    "\n",
    "This converts generation from a **batch process** into a **real-time interactive process**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Streaming Matters\n",
    "\n",
    "| Problem Without Streaming           | How Streaming Solves It          |\n",
    "| ----------------------------------- | -------------------------------- |\n",
    "| High latency for long responses     | User sees output instantly       |\n",
    "| Poor UX in chat systems             | Natural conversational flow      |\n",
    "| Wasted compute if user cancels      | Generation can stop early        |\n",
    "| Hard to build voice / realtime apps | Enables speech, agents, copilots |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Intuition\n",
    "\n",
    "Autoregressive LLMs generate text sequentially:\n",
    "\n",
    "[\n",
    "P(Y|X) = \\prod_{t=1}^{T} P(y_t | X, y_{<t})\n",
    "]\n",
    "\n",
    "Since token ( y_t ) does **not** depend on future tokens, it can be emitted immediately.\n",
    "\n",
    "**Streaming = exposing the autoregressive loop to the user.**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Generation Workflow With Streaming\n",
    "\n",
    "```\n",
    "User Prompt → Tokenization → Model Forward Pass → Sample Token y1 → Emit y1\n",
    "                                                 ↓\n",
    "                                            Sample y2 → Emit y2\n",
    "                                                 ↓\n",
    "                                                ...\n",
    "                                                 ↓\n",
    "                                            Sample yT → Emit yT\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Architecture Diagram\n",
    "\n",
    "```\n",
    "Client UI\n",
    "   ↑   ↓ (WebSocket / HTTP chunked)\n",
    "Streaming Server\n",
    "   ↓\n",
    "LLM Inference Engine\n",
    "   ↓\n",
    "Token Sampler\n",
    "   ↓\n",
    "Token Decoder\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Streaming vs Non-Streaming\n",
    "\n",
    "| Aspect             | Non-Streaming     | Streaming   |\n",
    "| ------------------ | ----------------- | ----------- |\n",
    "| Response latency   | High              | Very low    |\n",
    "| User perception    | Blocking          | Interactive |\n",
    "| Cancel generation  | Hard              | Easy        |\n",
    "| Compute efficiency | Wasteful on abort | Efficient   |\n",
    "| Realtime apps      | Not suitable      | Ideal       |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Types of Streaming\n",
    "\n",
    "| Type                    | Description                | Use Case                |\n",
    "| ----------------------- | -------------------------- | ----------------------- |\n",
    "| Token streaming         | Emit every token           | Chat, coding assistants |\n",
    "| Chunk streaming         | Emit groups of tokens      | APIs, web UI            |\n",
    "| Sentence streaming      | Emit after punctuation     | Voice systems           |\n",
    "| Multimodal streaming    | Text + audio + vision      | Agents, copilots        |\n",
    "| Bidirectional streaming | Both user and model stream | Voice assistants        |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Core Implementation Pattern\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "```python\n",
    "def generate_stream(prompt):\n",
    "    state = model.initialize(prompt)\n",
    "    while not state.done:\n",
    "        logits = model.forward(state)\n",
    "        token = sample(logits)\n",
    "        state.update(token)\n",
    "        yield token\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Python Example (Transformers)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
    "import threading\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Explain transformers:\", return_tensors=\"pt\")\n",
    "streamer = TextIteratorStreamer(tokenizer)\n",
    "\n",
    "def generate():\n",
    "    model.generate(**inputs, streamer=streamer, max_new_tokens=100)\n",
    "\n",
    "threading.Thread(target=generate).start()\n",
    "\n",
    "for token in streamer:\n",
    "    print(token, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. HTTP Streaming Example (Conceptual)\n",
    "\n",
    "```\n",
    "POST /generate\n",
    "Transfer-Encoding: chunked\n",
    "\n",
    "data: \"The\"\n",
    "data: \" transformer\"\n",
    "data: \" architecture\"\n",
    "data: \" was introduced\"\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Streaming in Production Systems\n",
    "\n",
    "| Layer            | Responsibility              |\n",
    "| ---------------- | --------------------------- |\n",
    "| Model            | Autoregressive generation   |\n",
    "| Inference Engine | Sampling + state tracking   |\n",
    "| Streaming Server | Token transport             |\n",
    "| Protocol         | WebSocket / HTTP SSE / gRPC |\n",
    "| Client           | Render partial output       |\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Applications\n",
    "\n",
    "| Domain            | Role of Streaming        |\n",
    "| ----------------- | ------------------------ |\n",
    "| Chatbots          | Instant replies          |\n",
    "| Code assistants   | Progressive code writing |\n",
    "| Voice agents      | Real-time speech         |\n",
    "| Search copilots   | Fast perceived results   |\n",
    "| Autonomous agents | Continuous planning      |\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Key Engineering Challenges\n",
    "\n",
    "| Challenge              | Solution              |\n",
    "| ---------------------- | --------------------- |\n",
    "| Backpressure           | Flow control          |\n",
    "| Partial token decoding | Byte-level tokenizers |\n",
    "| Stopping control       | User abort hooks      |\n",
    "| Latency spikes         | KV caching            |\n",
    "| Synchronization        | Async event loops     |\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Summary\n",
    "\n",
    "**Streaming transforms generative models from offline batch systems into real-time interactive engines.**\n",
    "\n",
    "It exposes the model’s **autoregressive nature**, enabling:\n",
    "\n",
    "* Low latency UX\n",
    "* Efficient compute usage\n",
    "* Real-time conversational AI\n",
    "* Scalable agent systems\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
