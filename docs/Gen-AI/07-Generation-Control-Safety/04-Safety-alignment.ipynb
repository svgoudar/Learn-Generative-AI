{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0beae7cd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Safety Alignment\n",
    "\n",
    "### 1. Definition & Motivation\n",
    "\n",
    "**Safety Alignment** is the discipline of ensuring that an AI system’s behavior remains:\n",
    "\n",
    "* **Helpful** — performs its intended function\n",
    "* **Harmless** — avoids causing physical, psychological, social, or economic harm\n",
    "* **Honest** — does not mislead, hallucinate, or manipulate\n",
    "\n",
    "Formally, we want the model’s policy\n",
    "[\n",
    "\\pi_{\\text{model}} \\approx \\pi_{\\text{human values}}\n",
    "]\n",
    "even when the model generalizes beyond its training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Safety Alignment Is Hard\n",
    "\n",
    "| Challenge                    | Description                               |\n",
    "| ---------------------------- | ----------------------------------------- |\n",
    "| **Objective mismatch**       | Training loss ≠ human values              |\n",
    "| **Specification gaming**     | Model finds loopholes in reward           |\n",
    "| **Distribution shift**       | Model encounters unseen situations        |\n",
    "| **Emergent behavior**        | Capabilities arise not explicitly trained |\n",
    "| **Scalability of oversight** | Humans cannot label everything            |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Components of Safety Alignment\n",
    "\n",
    "#### 3.1 Value Learning\n",
    "\n",
    "Learning what humans *prefer*.\n",
    "\n",
    "* Supervised labeling\n",
    "* Preference comparisons\n",
    "* Implicit behavioral signals\n",
    "\n",
    "#### 3.2 Robustness\n",
    "\n",
    "Model remains safe under:\n",
    "\n",
    "* Adversarial inputs\n",
    "* Distribution shift\n",
    "* Prompt manipulation\n",
    "\n",
    "#### 3.3 Interpretability\n",
    "\n",
    "Understanding **why** a model behaves a certain way:\n",
    "\n",
    "* Feature attribution\n",
    "* Circuit analysis\n",
    "* Probing internal representations\n",
    "\n",
    "#### 3.4 Governance & Monitoring\n",
    "\n",
    "* Logging and auditing\n",
    "* Deployment safeguards\n",
    "* Continuous evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Alignment Training Pipeline\n",
    "\n",
    "```\n",
    "Raw Data → Pretraining → Supervised Fine-Tuning → \n",
    "Human Feedback → Reward Modeling → RL Optimization → Safety Filters\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Key Techniques\n",
    "\n",
    "#### 5.1 Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Train on human-written demonstrations.\n",
    "\n",
    "```python\n",
    "loss = CrossEntropy(model(x), y_human)\n",
    "```\n",
    "\n",
    "Provides base alignment.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2 Reward Modeling\n",
    "\n",
    "Learn a reward function from human preferences.\n",
    "\n",
    "```python\n",
    "r = RewardModel(response, context)\n",
    "```\n",
    "\n",
    "Humans label which outputs are better → model learns to score.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3 Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "Optimize the model to maximize learned reward.\n",
    "\n",
    "[\n",
    "\\max_\\pi ; \\mathbb{E}*{x \\sim \\pi}[R*\\theta(x)]\n",
    "]\n",
    "\n",
    "Common algorithm: **PPO**\n",
    "\n",
    "```python\n",
    "for batch in data:\n",
    "    reward = R(model(batch))\n",
    "    model = PPO_update(model, reward)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.4 Constitutional AI\n",
    "\n",
    "Replace some human feedback with explicit rules.\n",
    "\n",
    "Example rule:\n",
    "\n",
    "> “The assistant should not provide instructions for wrongdoing.”\n",
    "\n",
    "Model self-critiques and revises.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Safety Failure Modes\n",
    "\n",
    "| Failure               | Description                              |\n",
    "| --------------------- | ---------------------------------------- |\n",
    "| **Hallucination**     | Confidently incorrect output             |\n",
    "| **Toxicity**          | Harmful language                         |\n",
    "| **Jailbreaks**        | Bypassing safety constraints             |\n",
    "| **Deception**         | Model learns to hide intentions          |\n",
    "| **Over-optimization** | Maximizing reward while violating intent |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Evaluation of Alignment\n",
    "\n",
    "#### 7.1 Offline Evaluation\n",
    "\n",
    "* Toxicity benchmarks\n",
    "* Bias tests\n",
    "* Red-teaming datasets\n",
    "\n",
    "#### 7.2 Online Monitoring\n",
    "\n",
    "* Real-time anomaly detection\n",
    "* User feedback loops\n",
    "* Automatic policy enforcement\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Alignment vs Capability Tradeoff\n",
    "\n",
    "| Goal                      | Risk                  |\n",
    "| ------------------------- | --------------------- |\n",
    "| Higher capability         | More potential misuse |\n",
    "| Strong safety constraints | Reduced usefulness    |\n",
    "\n",
    "Modern systems aim for **Pareto-optimal frontier** between the two.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Minimal Working Example: Toy Alignment\n",
    "\n",
    "```python\n",
    "# Preference learning example\n",
    "pairs = [(x1, better_y1), (x2, better_y2)]\n",
    "\n",
    "reward_model.train(pairs)\n",
    "\n",
    "for step in range(1000):\n",
    "    y = model.generate(x)\n",
    "    r = reward_model(x, y)\n",
    "    model = reinforce(model, r)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Research Frontiers\n",
    "\n",
    "* Scalable oversight\n",
    "* Mechanistic interpretability\n",
    "* AI-assisted alignment\n",
    "* Alignment for autonomous agents\n",
    "* Alignment under self-improvement\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Conceptual Summary\n",
    "\n",
    "| Aspect    | Role                                     |\n",
    "| --------- | ---------------------------------------- |\n",
    "| Objective | Align behavior with human values         |\n",
    "| Method    | SFT → Reward modeling → RLHF             |\n",
    "| Tools     | Interpretability, robustness, governance |\n",
    "| Risk      | Misalignment grows with capability       |\n",
    "\n",
    "---\n",
    "\n",
    "**Safety Alignment is not a feature — it is the central engineering problem of advanced AI systems.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
