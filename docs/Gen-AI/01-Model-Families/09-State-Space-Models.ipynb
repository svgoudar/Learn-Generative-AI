{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "786b27cd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## State Space Models (SSM)\n",
    "\n",
    "State Space Models are a class of sequence models that represent a system using **hidden internal states** that evolve over time and generate observable outputs.\n",
    "They model how the **present arises from the past through an evolving internal memory**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Think of the system as having a **memory state** that summarizes everything important about the past.\n",
    "\n",
    "At each time step:\n",
    "\n",
    "> **Previous state + new input → updated state → output**\n",
    "\n",
    "Instead of remembering the full history (as in attention), the model carries forward a **compressed dynamic state**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "For time step $t$:\n",
    "\n",
    "$$\n",
    "\\text{State update: } s_t = A s_{t-1} + B x_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output: } y_t = C s_t + D x_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $s_t$ = hidden state\n",
    "* $x_t$ = input\n",
    "* $y_t$ = output\n",
    "* $A, B, C, D$ = learned parameters\n",
    "\n",
    "This defines a **linear dynamical system**, often extended with nonlinearities in neural SSMs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why SSMs Matter in GenAI**\n",
    "\n",
    "Transformers rely on **attention over the entire context** — powerful but expensive.\n",
    "SSMs replace attention with **continuous state evolution**, achieving:\n",
    "\n",
    "* Long-context memory\n",
    "* Linear-time computation\n",
    "* High efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### **Modern Neural SSMs**\n",
    "\n",
    "Recent models like **S4, Mamba, RetNet** integrate SSMs into deep learning and compete with Transformers on language tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Overview**\n",
    "\n",
    "```\n",
    "Input x₁ → State s₁ → Output y₁\n",
    "Input x₂ → State s₂ → Output y₂\n",
    "Input x₃ → State s₃ → Output y₃\n",
    "...\n",
    "```\n",
    "\n",
    "Only the **current state** is carried forward.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training**\n",
    "\n",
    "SSMs are trained end-to-end using backpropagation, similar to other neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "\n",
    "| Property                | Benefit                  |\n",
    "| ----------------------- | ------------------------ |\n",
    "| Linear time & memory    | Scales to long sequences |\n",
    "| Long-range dependencies | No attention bottleneck  |\n",
    "| Streaming friendly      | Real-time processing     |\n",
    "| Hardware efficient      | Low latency inference    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "| Issue                       | Explanation              |\n",
    "| --------------------------- | ------------------------ |\n",
    "| Weaker short-term precision | Compared to attention    |\n",
    "| Harder expressiveness       | For complex reasoning    |\n",
    "| Less mature ecosystem       | Compared to Transformers |\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Natural Language Processing\n",
    "\n",
    "* Long-context language models\n",
    "* Document modeling\n",
    "* Speech recognition\n",
    "\n",
    "#### Time-Series Analysis\n",
    "\n",
    "* Financial forecasting\n",
    "* Sensor data modeling\n",
    "* Weather prediction\n",
    "\n",
    "#### Audio & Signal Processing\n",
    "\n",
    "* Speech synthesis\n",
    "* Music modeling\n",
    "* Radar and control systems\n",
    "\n",
    "#### Control & Robotics\n",
    "\n",
    "* System dynamics modeling\n",
    "* Reinforcement learning environments\n",
    "\n",
    "---\n",
    "\n",
    "### **SSM vs Transformer**\n",
    "\n",
    "| Feature           | SSM              | Transformer  |\n",
    "| ----------------- | ---------------- | ------------ |\n",
    "| Memory            | Compressed state | Full context |\n",
    "| Complexity        | O(n)             | O(n²)        |\n",
    "| Long sequences    | Excellent        | Expensive    |\n",
    "| Inference latency | Low              | High         |\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use SSMs**\n",
    "\n",
    "* Ultra-long sequences\n",
    "* Real-time streaming\n",
    "* Low-latency environments\n",
    "* Resource-constrained devices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce9978",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
