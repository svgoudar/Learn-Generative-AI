{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0b0deb",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Autoregressive Models\n",
    "\n",
    "An **autoregressive (AR) model** generates data **one step at a time**, where **each output depends on previous outputs**.\n",
    "\n",
    "In Generative AI, this means:\n",
    "\n",
    "> **Predict the next token given all previous tokens.**\n",
    "\n",
    "---\n",
    "\n",
    "### Core Intuition\n",
    "\n",
    "Think of writing a sentence:\n",
    "\n",
    "You don’t write the entire paragraph at once.\n",
    "You choose each next word based on what you’ve already written.\n",
    "\n",
    "That is exactly how autoregressive models work.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a sequence (x_1, x_2, ..., x_T):\n",
    "\n",
    "$$\n",
    "P(x) = \\prod_{t=1}^{T} P(x_t | x_1, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "Each token is generated conditionally on previous tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Modern autoregressive models are usually **Transformer decoder-only models**:\n",
    "\n",
    "```\n",
    "x₁ → x₂ → x₃ → ... → xₜ → Next Token\n",
    "        ↑\n",
    "   Masked Self-Attention\n",
    "```\n",
    "\n",
    "**Masked attention** ensures the model cannot see future tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Process\n",
    "\n",
    "Train by next-token prediction:\n",
    "\n",
    "Input: \"The cat is\"\n",
    "Target: \"sleeping\"\n",
    "\n",
    "The model learns probability distributions over the vocabulary at each step.\n",
    "\n",
    "---\n",
    "\n",
    "### Generation Process\n",
    "\n",
    "1. Start with prompt\n",
    "2. Predict next token\n",
    "3. Append token\n",
    "4. Repeat until completion\n",
    "\n",
    "---\n",
    "\n",
    "### Why Autoregressive Models Are Powerful\n",
    "\n",
    "| Advantage                 | Explanation                          |\n",
    "| ------------------------- | ------------------------------------ |\n",
    "| General-purpose           | Works for any sequence               |\n",
    "| Flexible                  | Text, code, audio, images            |\n",
    "| Scales well               | Large models show emergent abilities |\n",
    "| Simple training objective | Next-token prediction                |\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "| Domain | Application                            |\n",
    "| ------ | -------------------------------------- |\n",
    "| Text   | Chatbots, translation, summarization   |\n",
    "| Code   | Code generation, review, debugging     |\n",
    "| Speech | Text-to-speech, speech synthesis       |\n",
    "| Music  | Melody generation                      |\n",
    "| Images | PixelCNN, autoregressive vision models |\n",
    "| Video  | Frame-by-frame generation              |\n",
    "\n",
    "---\n",
    "\n",
    "### Strengths & Limitations\n",
    "\n",
    "| Strengths                 | Limitations                  |\n",
    "| ------------------------- | ---------------------------- |\n",
    "| Very high quality outputs | Slow generation (sequential) |\n",
    "| Stable training           | Costly for long sequences    |\n",
    "| General-purpose           | Context window limit         |\n",
    "\n",
    "---\n",
    "\n",
    "### Autoregressive vs Non-Autoregressive\n",
    "\n",
    "| Feature    | Autoregressive | Non-Autoregressive |\n",
    "| ---------- | -------------- | ------------------ |\n",
    "| Generation | Sequential     | Parallel           |\n",
    "| Quality    | Very high      | Moderate           |\n",
    "| Latency    | Higher         | Lower              |\n",
    "\n",
    "---\n",
    "\n",
    "### Why Modern LLMs Are Autoregressive\n",
    "\n",
    "LLMs like GPT, Claude, and LLaMA are all **autoregressive**, because this framework scales extremely well and supports:\n",
    "\n",
    "* Natural language\n",
    "* Reasoning\n",
    "* Tool use\n",
    "* Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e120d675",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
