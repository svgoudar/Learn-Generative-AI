{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6f9644",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Encoder–Decoder Model (Seq2Seq Architecture)\n",
    "\n",
    "An **Encoder–Decoder model** is a neural network architecture designed to transform one sequence into another sequence.\n",
    "It is the foundation of machine translation, summarization, question answering, and many multimodal systems.\n",
    "\n",
    "---\n",
    "\n",
    "### High-Level Idea\n",
    "\n",
    "**Input sequence → Encoder → Context representation → Decoder → Output sequence**\n",
    "\n",
    "Example:\n",
    "English sentence → Encoder → semantic representation → Decoder → French sentence\n",
    "\n",
    "---\n",
    "\n",
    "### Components\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "The **encoder** reads the entire input sequence and converts it into a **set of internal representations**.\n",
    "\n",
    "**Responsibilities:**\n",
    "\n",
    "* Understand input meaning\n",
    "* Capture relationships between tokens\n",
    "* Produce hidden states representing the input\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "* Token embeddings\n",
    "* Positional encoding\n",
    "* Multi-head self-attention\n",
    "* Feedforward layers\n",
    "\n",
    "Output: A sequence of contextual vectors\n",
    "\n",
    "---\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "The **decoder** generates the output sequence **one token at a time**, conditioned on:\n",
    "\n",
    "1. Its own previous outputs\n",
    "2. The encoder’s representations\n",
    "\n",
    "**Operations:**\n",
    "\n",
    "* Masked self-attention (prevents seeing future tokens)\n",
    "* Cross-attention (attends to encoder outputs)\n",
    "* Feedforward layers\n",
    "* Softmax over vocabulary\n",
    "\n",
    "---\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "#### Self-Attention (Encoder & Decoder)\n",
    "\n",
    "Learns relationships **within the same sequence**.\n",
    "\n",
    "#### Cross-Attention (Decoder only)\n",
    "\n",
    "Connects output generation to input meaning by attending to encoder states.\n",
    "\n",
    "---\n",
    "\n",
    "### Transformer Encoder–Decoder Example\n",
    "\n",
    "```\n",
    "Input Tokens\n",
    "   ↓\n",
    "[ Encoder Blocks ]\n",
    "   ↓\n",
    "Encoded Representations\n",
    "   ↓        ↑\n",
    "   → Cross-Attention ←\n",
    "   ↓\n",
    "[ Decoder Blocks ]\n",
    "   ↓\n",
    "Output Tokens\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Encoder–Decoder Works Well\n",
    "\n",
    "| Benefit                       | Explanation                                  |\n",
    "| ----------------------------- | -------------------------------------------- |\n",
    "| Flexible input/output lengths | Works for translation, summarization         |\n",
    "| Strong context understanding  | Encoder builds full semantic map             |\n",
    "| Precise generation control    | Decoder conditions on both context & history |\n",
    "\n",
    "---\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "* Machine Translation\n",
    "* Document Summarization\n",
    "* Speech-to-Text\n",
    "* Image Captioning\n",
    "* Question Answering\n",
    "* Multimodal AI systems\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison with Decoder-Only Models\n",
    "\n",
    "| Feature                           | Encoder–Decoder | Decoder-Only |\n",
    "| --------------------------------- | --------------- | ------------ |\n",
    "| Handles input & output separately | Yes             | No           |\n",
    "| Cross-attention                   | Yes             | No           |\n",
    "| Better for transformation tasks   | Yes             | Moderate     |\n",
    "| Prompt simplicity                 | Medium          | High         |\n",
    "\n",
    "---\n",
    "\n",
    "### Examples of Encoder–Decoder Models\n",
    "\n",
    "* T5\n",
    "* BART\n",
    "* MarianMT\n",
    "* Pegasus\n",
    "* Whisper (speech → text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f8914",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
