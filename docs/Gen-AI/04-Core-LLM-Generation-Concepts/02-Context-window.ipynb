{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b160badc",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Context Window and Sliding Window\n",
    "\n",
    "### 1. Context Window\n",
    "\n",
    "A **context window** is the *maximum number of tokens* an LLM or Transformer model can read **at once**.\n",
    "\n",
    "Example:\n",
    "\n",
    "* GPT-4 (old versions): ~8k tokens\n",
    "* GPT-4-Turbo: 128k tokens\n",
    "* Claude 3 Opus: 200k tokens\n",
    "* GPT-5-level models: multi-million token windows\n",
    "\n",
    "#### Why does a context window exist?\n",
    "\n",
    "Because the Transformer architecture uses:\n",
    "\n",
    "* **Self-attention → O(n²) memory and compute**\n",
    "* Increasing sequence length grows cost quadratically\n",
    "\n",
    "Thus, models cannot take unlimited text.\n",
    "They read only **a limited chunk** → the context window.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Context Windows Matter\n",
    "\n",
    "The context window determines:\n",
    "\n",
    "* **How much text the model can understand at once**\n",
    "* **How much previous conversation is remembered**\n",
    "* **How much long document processing is possible**\n",
    "\n",
    "Example:\n",
    "If a model has a 4,096-token window:\n",
    "\n",
    "```\n",
    "Input > 4096 tokens → older tokens get truncated\n",
    "```\n",
    "\n",
    "Meaning the model:\n",
    "\n",
    "* Cannot remember earlier conversation\n",
    "* Cannot process long PDFs directly\n",
    "* Cannot operate on extremely long code files at once\n",
    "\n",
    "This is why you sometimes see “model forgot earlier lines” → context overflow.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. What Happens When the Context Window Is Exceeded?\n",
    "\n",
    "Two scenarios:\n",
    "\n",
    "##### A. **Truncation (Current LLM behavior)**\n",
    "\n",
    "Oldest tokens are dropped first.\n",
    "\n",
    "```\n",
    "[ Too old ] [ middle ] [ latest prompt ]\n",
    "      X         ✓            ✓\n",
    "```\n",
    "\n",
    "##### B. **Window Reset**\n",
    "\n",
    "Models sometimes treat it as a fresh conversation when too much history is lost.\n",
    "\n",
    "---\n",
    "\n",
    "### Sliding Window (Intuition)\n",
    "\n",
    "A **sliding window** is a technique to process long documents that exceed the context window **by splitting text into overlapping chunks**.\n",
    "\n",
    "Think of it as reading a long book using a limited-size magnifying glass.\n",
    "You read section by section, moving the magnifying glass forward.\n",
    "\n",
    "Example with window size = 100 tokens, step size = 50:\n",
    "\n",
    "```\n",
    "Chunk 1: tokens 0–99\n",
    "Chunk 2: tokens 50–149\n",
    "Chunk 3: tokens 100–199\n",
    "```\n",
    "\n",
    "Each chunk:\n",
    "\n",
    "* Shares some overlap (to preserve continuity)\n",
    "* Is processed independently or sequentially\n",
    "\n",
    "Sliding window solves:\n",
    "\n",
    "* Long document summarization\n",
    "* Large codebase analysis\n",
    "* Long context retrieval\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Sliding Windows Are Important\n",
    "\n",
    "Transformers cannot natively handle long sequences, but sliding windows allow:\n",
    "\n",
    "#### **Scalable document processing**\n",
    "\n",
    "Summarizing a 300k-token PDF with a model that has only 50k window.\n",
    "\n",
    "#### **Retrieval augmentation**\n",
    "\n",
    "Extracting relevant chunks for RAG pipelines.\n",
    "\n",
    "#### **Local attention in LLMs**\n",
    "\n",
    "Many efficient Transformers use internal sliding windows:\n",
    "\n",
    "* Longformer\n",
    "* BigBird\n",
    "* Mistral\n",
    "* FlashAttention-2 windowed mode\n",
    "\n",
    "These models attend only to nearby tokens, not the entire sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### Concrete Example of Sliding Window\n",
    "\n",
    "Say you want to process 1000-token text but the model only supports 200-token windows.\n",
    "\n",
    "Let:\n",
    "\n",
    "* window size = 200\n",
    "* stride = 100\n",
    "\n",
    "Chunks:\n",
    "\n",
    "```\n",
    "0–199\n",
    "100–299\n",
    "200–399\n",
    "300–499\n",
    "...\n",
    "800–999\n",
    "```\n",
    "\n",
    "Each window is fed to the model separately, e.g. for:\n",
    "\n",
    "* Embedding\n",
    "* Summarization\n",
    "* Classification\n",
    "\n",
    "Then the outputs are combined.\n",
    "\n",
    "---\n",
    "\n",
    "### Sliding Window vs Context Window\n",
    "\n",
    "| Concept            | Meaning                                              | Purpose                                    |\n",
    "| ------------------ | ---------------------------------------------------- | ------------------------------------------ |\n",
    "| **Context Window** | Max tokens model can see at once                     | Model capability limit                     |\n",
    "| **Sliding Window** | Technique to break long text into overlapping chunks | Process text longer than the model’s limit |\n",
    "\n",
    "---\n",
    "\n",
    "###  Visual Summary\n",
    "\n",
    "#### Context window:\n",
    "\n",
    "```\n",
    "[ A fixed-size box around text the model can see ]\n",
    "```\n",
    "\n",
    "#### Sliding window:\n",
    "\n",
    "```\n",
    "[Chunk1: 0–100]  \n",
    "          [Chunk2: 50–150]  \n",
    "                    [Chunk3: 100–200]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdae134",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
