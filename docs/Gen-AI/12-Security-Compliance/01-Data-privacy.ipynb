{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbb4228",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Data Privacy\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Motivation and Problem Setting\n",
    "\n",
    "Generative AI systems (LLMs, diffusion models, multimodal models) are trained on massive datasets and often interact directly with users.\n",
    "This creates **unique privacy risks** because the model can:\n",
    "\n",
    "* **Absorb sensitive data during training**\n",
    "* **Reveal sensitive data during inference**\n",
    "* **Accidentally memorize individuals' information**\n",
    "\n",
    "**Goal of Data Privacy in Generative AI**\n",
    "\n",
    "> Ensure that **no individual's private information is leaked, reconstructed, or inferred**, while preserving model utility.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Privacy Threat Model\n",
    "\n",
    "| Threat Type               | Description                                | Example                            |\n",
    "| ------------------------- | ------------------------------------------ | ---------------------------------- |\n",
    "| **Training Data Leakage** | Model memorizes sensitive training samples | Reconstructing a person's SSN      |\n",
    "| **Inference Leakage**     | Sensitive info inferred from outputs       | Predicting medical condition       |\n",
    "| **Membership Inference**  | Detect if a person was in training set     | \"Was Alice in dataset?\"            |\n",
    "| **Model Inversion**       | Reconstruct private inputs from model      | Recover face image from embeddings |\n",
    "| **Prompt Leakage**        | Revealing private user conversations       | Exposing chat logs                 |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why Generative Models Are Especially Risky\n",
    "\n",
    "Generative models:\n",
    "\n",
    "* Are **high-capacity** and **memorization-prone**\n",
    "* Produce **human-readable content**\n",
    "* Are often trained on **unfiltered web-scale data**\n",
    "* Are deployed in **interactive environments**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Core Privacy Principles\n",
    "\n",
    "| Principle              | Meaning                                 |\n",
    "| ---------------------- | --------------------------------------- |\n",
    "| **Data Minimization**  | Collect only necessary data             |\n",
    "| **Purpose Limitation** | Use data only for defined objectives    |\n",
    "| **Anonymization**      | Remove identifiers before training      |\n",
    "| **Access Control**     | Limit data and model access             |\n",
    "| **Auditable Training** | Trace data sources and usage            |\n",
    "| **Privacy by Design**  | Embed privacy mechanisms into pipelines |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Technical Privacy Mechanisms\n",
    "\n",
    "### 5.1 Differential Privacy (DP)\n",
    "\n",
    "Formal privacy guarantee ensuring that **no single record significantly influences the model**.\n",
    "\n",
    "**Definition (Simplified):**\n",
    "\n",
    "A training algorithm is ((\\varepsilon, \\delta))-DP if output distributions are nearly identical whether or not any one individual's data is included.\n",
    "\n",
    "#### DP-SGD Workflow\n",
    "\n",
    "1. Compute per-example gradients\n",
    "2. Clip gradients to norm (C)\n",
    "3. Add Gaussian noise\n",
    "4. Update parameters\n",
    "\n",
    "```python\n",
    "# Simplified DP-SGD example\n",
    "for x, y in loader:\n",
    "    grads = compute_per_example_gradients(model, x, y)\n",
    "    clipped = clip_by_norm(grads, C=1.0)\n",
    "    noise = torch.normal(0, sigma, size=clipped.shape)\n",
    "    noisy_grad = clipped.mean(0) + noise\n",
    "    optimizer.step(noisy_grad)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Data Anonymization & De-identification\n",
    "\n",
    "Remove direct and indirect identifiers before training.\n",
    "\n",
    "| Data Type     | Example Treatment    |\n",
    "| ------------- | -------------------- |\n",
    "| Names         | Replace with tokens  |\n",
    "| Addresses     | Generalize to region |\n",
    "| IDs           | Remove               |\n",
    "| Free-text PII | Detect and mask      |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Federated Learning (FL)\n",
    "\n",
    "Model trains **without centralizing raw data**.\n",
    "\n",
    "**Workflow:**\n",
    "\n",
    "1. Send model to clients\n",
    "2. Train locally on private data\n",
    "3. Send encrypted updates back\n",
    "4. Aggregate centrally\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Secure Computation\n",
    "\n",
    "| Technique              | Purpose                        |\n",
    "| ---------------------- | ------------------------------ |\n",
    "| Secure Enclaves        | Protect memory during training |\n",
    "| Homomorphic Encryption | Train on encrypted data        |\n",
    "| Secure Aggregation     | Hide individual updates        |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Privacy Risks During Inference\n",
    "\n",
    "| Risk             | Example                               |\n",
    "| ---------------- | ------------------------------------- |\n",
    "| Prompt Injection | Force model to reveal private content |\n",
    "| Overfitting      | Regurgitation of training examples    |\n",
    "| Memorization     | Quoting private conversations         |\n",
    "\n",
    "**Mitigations**\n",
    "\n",
    "* Output filtering\n",
    "* Memorization testing\n",
    "* Prompt sanitization\n",
    "* Redaction models\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Measuring Privacy\n",
    "\n",
    "| Metric                        | Purpose                     |\n",
    "| ----------------------------- | --------------------------- |\n",
    "| (\\varepsilon) (DP budget)     | Formal privacy loss         |\n",
    "| Membership Inference Accuracy | Leakage detection           |\n",
    "| Exposure Metric               | Measures memorization       |\n",
    "| Canary Insertion              | Track memorization behavior |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Regulatory Landscape\n",
    "\n",
    "| Regulation        | Key Requirements                        |\n",
    "| ----------------- | --------------------------------------- |\n",
    "| GDPR (EU)         | Right to erasure, consent, transparency |\n",
    "| HIPAA (US)        | Health data protection                  |\n",
    "| CCPA (California) | Consumer data rights                    |\n",
    "| DPDP Act (India)  | Data fiduciary responsibility           |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Practical Privacy Pipeline for Generative AI\n",
    "\n",
    "```text\n",
    "Data Collection\n",
    "   ↓\n",
    "PII Detection & Masking\n",
    "   ↓\n",
    "Anonymization\n",
    "   ↓\n",
    "DP-SGD Training\n",
    "   ↓\n",
    "Privacy Auditing\n",
    "   ↓\n",
    "Deployment with Output Filters\n",
    "   ↓\n",
    "Continuous Monitoring\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Summary\n",
    "\n",
    "| Aspect     | Key Takeaway                                         |\n",
    "| ---------- | ---------------------------------------------------- |\n",
    "| Risk       | Generative models can memorize and leak private data |\n",
    "| Solution   | Combine DP, anonymization, FL, secure computation    |\n",
    "| Evaluation | Measure privacy leakage explicitly                   |\n",
    "| Design     | Privacy must be embedded from data → deployment      |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Key Insight\n",
    "\n",
    "> **In Generative AI, privacy is not a policy layer — it is a core learning-system property.**\n",
    "\n",
    "Well-designed generative systems treat privacy as **a mathematical constraint on learning**, not merely a compliance checkbox.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
