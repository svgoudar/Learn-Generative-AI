{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc59e7e4",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Feedforward Neural Network (FNN)\n",
    "\n",
    "A **Feedforward Neural Network** is the most fundamental type of neural network where information flows strictly in **one direction** — from input to output — with **no cycles or memory**.\n",
    "\n",
    "It is the foundation upon which almost all modern deep learning architectures are built.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "An FNN learns a function that maps inputs to outputs by stacking multiple layers of simple transformations.\n",
    "\n",
    "> **Input → computation → output**\n",
    "\n",
    "Each layer extracts higher-level features from the previous one.\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Structure**\n",
    "\n",
    "```\n",
    "Input Layer → Hidden Layer(s) → Output Layer\n",
    "```\n",
    "\n",
    "Each connection has a weight, and each neuron applies:\n",
    "\n",
    "$$\n",
    "y = \\sigma(Wx + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $W$ = weights\n",
    "* $b$ = bias\n",
    "* $\\sigma$ = activation function\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Learns**\n",
    "\n",
    "1. Forward pass: compute output\n",
    "2. Compute loss\n",
    "3. Backpropagate error\n",
    "4. Update weights using gradient descent\n",
    "\n",
    "Repeated until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Properties**\n",
    "\n",
    "| Property               | Description                          |\n",
    "| ---------------------- | ------------------------------------ |\n",
    "| No memory              | Each input processed independently   |\n",
    "| Universal approximator | Can approximate any function         |\n",
    "| Building block         | Used inside CNNs, Transformers, VAEs |\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Feedforward Networks Matter**\n",
    "\n",
    "Every modern model contains feedforward layers:\n",
    "\n",
    "* Transformers use large FFN blocks\n",
    "* VAEs and GANs use MLPs\n",
    "* CNNs end with feedforward classifiers\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Classification & Regression\n",
    "\n",
    "* Image classification\n",
    "* Spam detection\n",
    "* Credit scoring\n",
    "\n",
    "#### Representation Learning\n",
    "\n",
    "* Feature extraction\n",
    "* Dimensionality reduction\n",
    "\n",
    "#### Control & Optimization\n",
    "\n",
    "* Robotics controllers\n",
    "* Game playing systems\n",
    "\n",
    "### Inside Larger Models\n",
    "\n",
    "* Transformer feedforward blocks\n",
    "* Autoencoder encoder/decoder networks\n",
    "\n",
    "---\n",
    "\n",
    "### **Strengths & Limitations**\n",
    "\n",
    "| Strengths                       | Limitations                    |\n",
    "| ------------------------------- | ------------------------------ |\n",
    "| Simple & efficient              | No sequence memory             |\n",
    "| Highly parallelizable           | Not suitable for temporal data |\n",
    "| Powerful function approximation | Cannot handle context          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Recurrent & Attention Models**\n",
    "\n",
    "| Model       | Memory     | Best For          |\n",
    "| ----------- | ---------- | ----------------- |\n",
    "| Feedforward | None       | Static inputs     |\n",
    "| RNN         | Short-term | Sequences         |\n",
    "| Transformer | Long-term  | Complex sequences |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Feedforward networks are the **mathematical engines** of deep learning — fast, powerful, and universally applicable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
