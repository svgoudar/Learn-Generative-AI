{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7867e732",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "**Rotary Embeddings** are a method of encoding token positions directly into the **attention mechanism** by rotating the query and key vectors based on their position.\n",
    "\n",
    "They are the default positional encoding used in many modern LLMs such as **LLaMA, GPT-NeoX, Mistral, Falcon, and PaLM**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Language understanding depends on **relative position**:\n",
    "how far apart two tokens are matters more than their absolute index.\n",
    "\n",
    "RoPE injects position information by **rotating vectors in a geometric space**, so that the dot-product attention score naturally encodes relative distance.\n",
    "\n",
    "> **Position becomes part of the geometry of attention.**\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works (Conceptually)**\n",
    "\n",
    "Each pair of dimensions in a query or key vector is treated as a 2D vector and **rotated** by an angle determined by the token’s position.\n",
    "\n",
    "For position (p):\n",
    "\n",
    "[\n",
    "R(p) =\n",
    "\\begin{pmatrix}\n",
    "\\cos(\\theta p) & -\\sin(\\theta p) \\\n",
    "\\sin(\\theta p) & \\cos(\\theta p)\n",
    "\\end{pmatrix}\n",
    "]\n",
    "\n",
    "This rotation is applied to the query and key vectors **before** computing attention.\n",
    "\n",
    "As a result, the dot-product between two tokens depends on their **relative distance**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why RoPE Is Powerful**\n",
    "\n",
    "| Property                           | Benefit                     |\n",
    "| ---------------------------------- | --------------------------- |\n",
    "| Encodes relative positions         | Better long-range reasoning |\n",
    "| No extra parameters                | Memory efficient            |\n",
    "| Extrapolates to longer context     | Supports long sequences     |\n",
    "| Integrates directly into attention | No additive embeddings      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Placement**\n",
    "\n",
    "```\n",
    "Token Embedding\n",
    "   ↓\n",
    "Linear Projection → Q, K\n",
    "   ↓\n",
    "Apply Rotary Rotation (position-dependent)\n",
    "   ↓\n",
    "Attention(Q, K, V)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Large Language Models\n",
    "\n",
    "GPT-NeoX, LLaMA, Mistral, Falcon, PaLM\n",
    "\n",
    "#### Long-Context Modeling\n",
    "\n",
    "Documents, books, codebases\n",
    "\n",
    "#### Multimodal Transformers\n",
    "\n",
    "Vision-language models and audio models\n",
    "\n",
    "#### Efficient Attention Systems\n",
    "\n",
    "Helps preserve ordering information without large embedding tables\n",
    "\n",
    "---\n",
    "\n",
    "### **RoPE vs Traditional Positional Encoding**\n",
    "\n",
    "| Feature                      | Sinusoidal | Learned | RoPE   |\n",
    "| ---------------------------- | ---------- | ------- | ------ |\n",
    "| Encodes relative position    | Weak       | Weak    | Strong |\n",
    "| Extrapolates to long context | Poor       | Poor    | Good   |\n",
    "| Integrated into attention    | No         | No      | Yes    |\n",
    "| Parameter free               | Yes        | No      | Yes    |\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition Summary**\n",
    "\n",
    "Rotary embeddings allow attention to **feel distance and direction between tokens** by turning position into geometry.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
