{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c07af59",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Layer Normalization\n",
    "\n",
    "**Layer Normalization (LayerNorm)** is a technique used in deep neural networks to **stabilize and accelerate training** by normalizing the activations **within each layer for each data sample**.\n",
    "\n",
    "It is a core component of modern architectures such as **Transformers, LLMs, VAEs, GANs, and RNNs**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Neural networks train faster and more reliably when the scale of activations is controlled.\n",
    "\n",
    "LayerNorm ensures that:\n",
    "\n",
    "> **Each layer sees inputs with consistent scale and distribution.**\n",
    "\n",
    "Instead of the network constantly adjusting to shifting internal values, it receives **well-conditioned signals**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Exactly Does LayerNorm Do?**\n",
    "\n",
    "For a given input vector ( x ) of a single training example:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "Then normalize:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma + \\epsilon}\n",
    "$$\n",
    "\n",
    "Then apply learnable scale and shift:\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( H ) = hidden dimension\n",
    "* ( \\gamma, \\beta ) = learned parameters\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Not Batch Normalization?**\n",
    "\n",
    "| Batch Norm                      | Layer Norm                        |\n",
    "| ------------------------------- | --------------------------------- |\n",
    "| Depends on batch statistics     | Independent of batch              |\n",
    "| Fails for small batch sizes     | Works for any batch size          |\n",
    "| Unstable for sequence models    | Ideal for sequence models         |\n",
    "| Difficult with variable lengths | Naturally handles variable length |\n",
    "\n",
    "This is why **Transformers and LLMs use LayerNorm, not BatchNorm**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Transformers & LLMs\n",
    "\n",
    "Used before or after attention and feedforward layers to stabilize training.\n",
    "\n",
    "#### Recurrent Neural Networks\n",
    "\n",
    "Improves training stability for long sequences.\n",
    "\n",
    "#### Generative Models\n",
    "\n",
    "Used in VAEs and GANs for smoother optimization.\n",
    "\n",
    "#### Reinforcement Learning\n",
    "\n",
    "Stabilizes policy networks.\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits**\n",
    "\n",
    "| Benefit                      | Explanation                           |\n",
    "| ---------------------------- | ------------------------------------- |\n",
    "| Faster convergence           | Well-scaled gradients                 |\n",
    "| Improved stability           | Reduces exploding/vanishing gradients |\n",
    "| Model robustness             | Less sensitivity to initialization    |\n",
    "| Consistent training behavior | Across batch sizes                    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Pre-Norm vs Post-Norm**\n",
    "\n",
    "Modern Transformers use **Pre-Norm**:\n",
    "\n",
    "```\n",
    "x → LayerNorm → Attention → + → LayerNorm → FFN → +\n",
    "```\n",
    "\n",
    "Pre-Norm enables **deeper and more stable models**.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuitive Summary**\n",
    "\n",
    "LayerNorm acts like an **automatic volume control** inside the network — keeping every layer's output at a healthy operating level.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can next explain **Positional Encoding** and why it is essential for Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdec29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
