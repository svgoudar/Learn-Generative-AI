{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f60a635",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Instruction Tuning\n",
    "\n",
    "**Instruction Tuning** is a training technique where an LLM is taught to follow *natural-language instructions* by fine-tuning it on a large dataset of:\n",
    "\n",
    "```\n",
    "Instruction → Input → Output\n",
    "```\n",
    "\n",
    "It turns a *plain language model* (which only predicts the next token) into an **instruction-following model** (like ChatGPT, LLaMA-Instruct, Mistral-Instruct, Falcon-Instruct).\n",
    "\n",
    "It is the foundational step that makes a model behave like an **assistant** rather than a text generator.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Instruction Tuning Is Needed (Problem Statement)\n",
    "\n",
    "A pretrained LLM (before tuning):\n",
    "\n",
    "* Predicts next tokens\n",
    "* Has no concept of “tasks”\n",
    "* Does not follow user instructions\n",
    "* Cannot reliably perform reasoning\n",
    "* Cannot give step-by-step outputs\n",
    "* May ignore user requests or ramble\n",
    "\n",
    "Example (untuned model):\n",
    "\n",
    "**User:** “Summarize this paragraph.”\n",
    "**Model:** *continues the paragraph* (because its objective is next-token prediction)\n",
    "\n",
    "Instruction tuning solves this by explicitly training the model on **how to respond to instructions**.\n",
    "\n",
    "---\n",
    "\n",
    "### How Instruction Tuning Works (Process)\n",
    "\n",
    "Instruction tuning adds a supervised fine-tuning (SFT) step:\n",
    "\n",
    "#### **Step 1 — Collect Instruction Dataset**\n",
    "\n",
    "Examples:\n",
    "\n",
    "* “Translate English to French.”\n",
    "* “Summarize this article.”\n",
    "* “Write a SQL query to get top 10 orders.”\n",
    "* “Explain gradient descent in simple words.”\n",
    "* “Sort this list.”\n",
    "* “Act as a customer support agent.”\n",
    "\n",
    "Datasets used:\n",
    "\n",
    "* FLAN\n",
    "* Dolly 15k\n",
    "* OIG / Pythia\n",
    "* OpenOrca\n",
    "* ShareGPT\n",
    "* Self-Instruct\n",
    "\n",
    "Each example looks like:\n",
    "\n",
    "```json\n",
    "{\n",
    " \"instruction\": \"Summarize the following text.\",\n",
    " \"input\": \"Deep learning is a subset of machine learning...\",\n",
    " \"output\": \"Deep learning is a technique that uses neural networks...\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2 — Format examples into a chat template**\n",
    "\n",
    "Example training sample:\n",
    "\n",
    "```\n",
    "Instruction: Summarize the text\n",
    "Input: Deep learning is a subset of...\n",
    "Response: Deep learning is a neural-network based...\n",
    "```\n",
    "\n",
    "The model learns the pattern:\n",
    "\n",
    "```\n",
    "User asks → Model responds helpfully\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3 — Fine-Tune the Pretrained LLM**\n",
    "\n",
    "Supervised fine-tuning (SFT):\n",
    "\n",
    "* Backprop on cross-entropy loss\n",
    "* Only on the **response tokens**\n",
    "* Model learns how to interpret instructions\n",
    "\n",
    "This changes the model’s behavior drastically.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4 — (Optional) RLHF or DPO**\n",
    "\n",
    "After instruction tuning:\n",
    "\n",
    "* RLHF (ChatGPT style reward learning)\n",
    "* DPO (Direct Preference Optimization)\n",
    "* PPO, RLAIF (AI-generated rewards)\n",
    "\n",
    "This step improves:\n",
    "\n",
    "* helpfullness\n",
    "* safety\n",
    "* refusal behavior\n",
    "* multi-step reasoning\n",
    "\n",
    "But instruction tuning is the **first and essential** step before RLHF.\n",
    "\n",
    "---\n",
    "\n",
    "### What Instruction Tuning Makes the Model Learn\n",
    "\n",
    "Instruction tuning teaches the model to:\n",
    "\n",
    "#### **1. Understand instructions**\n",
    "\n",
    "* “Explain”\n",
    "* “Compare”\n",
    "* “Write a function”\n",
    "* “Generate an email”\n",
    "* “Translate”\n",
    "* “Solve step-by-step”\n",
    "\n",
    "#### **2. Produce structured outputs**\n",
    "\n",
    "* SQL queries\n",
    "* Python code\n",
    "* JSON\n",
    "* Lists & bullet points\n",
    "\n",
    "#### **3. Follow task boundaries**\n",
    "\n",
    "It learns to **stop rambling** and return **precise outputs**.\n",
    "\n",
    "#### **4. Refuse unethical or dangerous requests**\n",
    "\n",
    "(When combined with safety data.)\n",
    "\n",
    "#### **5. Become conversational**\n",
    "\n",
    "Instruction tuning leads to conversational abilities like ChatGPT.\n",
    "\n",
    "---\n",
    "\n",
    "### Example (Before vs After Instruction Tuning)\n",
    "\n",
    "#### ❌ **Before Instruction Tuning**\n",
    "\n",
    "User: \"Write a summary of this article.\"\n",
    "Model (pretrained): *continues from “Write a summary…”*\n",
    "\n",
    "### ✅ **After Instruction Tuning**\n",
    "\n",
    "User: \"Write a summary of this article.\"\n",
    "Model: “Here is a short summary…”\n",
    "\n",
    "---\n",
    "\n",
    "### Techniques Used in Instruction Tuning\n",
    "\n",
    "#### **1. SFT (Supervised Fine-Tuning)**\n",
    "\n",
    "Most common method using instruction → answer pairs.\n",
    "\n",
    "#### **2. Self-Instruct**\n",
    "\n",
    "Model generates new instruction-following examples.\n",
    "\n",
    "#### **3. Chain-of-Thought Instruction Tuning**\n",
    "\n",
    "Train with:\n",
    "\n",
    "* “Let’s think step-by-step”\n",
    "* Reasoning traces\n",
    "\n",
    "#### **4. Multi-task Instruction Tuning**\n",
    "\n",
    "Train on translation, QA, summarization, reasoning, code, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Instruction Tuning vs Fine-Tuning vs LoRA**\n",
    "\n",
    "| Method                 | Purpose                    | Data Needed            | Cost           |\n",
    "| ---------------------- | -------------------------- | ---------------------- | -------------- |\n",
    "| **Pretraining**        | Learn language             | Trillions of tokens    | Very expensive |\n",
    "| **Instruction tuning** | Teach tasks & instructions | 50k–1M examples        | Medium         |\n",
    "| **LoRA finetuning**    | Specialize model           | 1k–100k samples        | Cheap          |\n",
    "| **RLHF**               | Improve behavior           | human preference pairs | Medium-high    |\n",
    "\n",
    "Instruction tuning is a **general-purpose alignment step**.\n",
    "\n",
    "---\n",
    "\n",
    "**Real Examples of Instruction-Tuned Models**\n",
    "\n",
    "| Model   | Base        | Instruction Tuned Name |\n",
    "| ------- | ----------- | ---------------------- |\n",
    "| LLaMA   | LLaMA       | LLaMA-Instruct         |\n",
    "| Mistral | Mistral     | Mistral-Instruct       |\n",
    "| GPT     | GPT-3       | InstructGPT            |\n",
    "| Falcon  | Falcon      | Falcon-Instruct        |\n",
    "| Gemini  | Gemini Base | Gemini Chat            |\n",
    "\n",
    "Instruction tuning creates the “assistant” version.\n",
    "\n",
    "---\n",
    "\n",
    " **Why Instruction Tuning Is Important for Generative AI**\n",
    "\n",
    "Because it enables:\n",
    "\n",
    "* Chatbots\n",
    "* Agents\n",
    "* Reasoning models\n",
    "* Tools like ChatGPT\n",
    "* Personal assistants\n",
    "* Document QA\n",
    "* RAG systems\n",
    "* Customer support models\n",
    "\n",
    "Without instruction tuning, LLMs would NOT follow human instructions reliably.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Instruction tuning is a supervised fine-tuning process that teaches a pretrained LLM to follow natural-language tasks, making it behave like a helpful assistant.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
