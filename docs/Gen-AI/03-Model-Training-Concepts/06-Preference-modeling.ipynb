{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87c913a",
   "metadata": {},
   "source": [
    "\n",
    "## Preference Modeling\n",
    "\n",
    "**Preference Modeling** is the process of teaching an LLM to prefer *better* responses over *worse* ones by learning from **human or AI-generated comparisons**.\n",
    "\n",
    "It answers the question:\n",
    "\n",
    "> “Given two possible answers from the model, which one is better according to humans?”\n",
    "\n",
    "This step transforms an SFT-trained instruction model into a **helpful, safe, aligned assistant**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Preference Modeling Is Needed\n",
    "\n",
    "After **Supervised Fine-Tuning (SFT)**, a model knows *how* to follow instructions, but it has problems:\n",
    "\n",
    "* May produce unsafe or harmful outputs\n",
    "* May hallucinate\n",
    "* May be overly verbose\n",
    "* May misunderstand user intent\n",
    "* May behave inconsistently\n",
    "* May produce multiple good answers—but which one is best?\n",
    "\n",
    "SFT teaches *how to respond*,\n",
    "Preference Modeling teaches *how to choose the best response*.\n",
    "\n",
    "---\n",
    "\n",
    "### Preference Modeling in the LLM Training Pipeline\n",
    "\n",
    "Full LLM training pipeline:\n",
    "\n",
    "```\n",
    "1. Pretraining\n",
    "   (predict next token, learn knowledge)\n",
    "\n",
    "2. SFT (Instruction Tuning)\n",
    "   (learn to follow instructions)\n",
    "\n",
    "3. Preference Modeling\n",
    "   (learn which outputs humans prefer)\n",
    "\n",
    "4. RLHF or DPO\n",
    "   (optimize behavior using preferences)\n",
    "```\n",
    "\n",
    "Preference modeling is step **3**, between SFT and RLHF/DPO.\n",
    "\n",
    "---\n",
    "\n",
    "### How Preference Modeling Works (Intuition)\n",
    "\n",
    "#### 1) Collect **pairs** of model responses\n",
    "\n",
    "For each instruction, the model generates 2 or more answers.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Prompt: Explain gravity.\n",
    "\n",
    "Response A: \"Gravity is a force that pulls objects together.\"\n",
    "Response B: \"Gravity is magic glue that makes stuff stick.\"\n",
    "```\n",
    "\n",
    "#### 2) A human (or strong AI model) marks:\n",
    "\n",
    "* which answer is preferred\n",
    "* and why\n",
    "\n",
    "Example label:\n",
    "\n",
    "```\n",
    "Chosen: A\n",
    "Rejected: B\n",
    "```\n",
    "\n",
    "This is called **preference data**.\n",
    "\n",
    "#### 3) Train a **Reward Model (RM)**\n",
    "\n",
    "The reward model learns to assign a score:\n",
    "\n",
    "$$\n",
    "RM(prompt, response) → \\text{quality score}\n",
    "$$\n",
    "\n",
    "Higher score = better according to human preference.\n",
    "\n",
    "#### 4) Use this Reward Model in RLHF/DPO\n",
    "\n",
    "* RLHF uses PPO to maximize the reward\n",
    "* DPO directly optimizes the preference ordering\n",
    "* No reward model needed for DPO, but preference data still needed\n",
    "\n",
    "---\n",
    "\n",
    "###  What the Reward Model Learns\n",
    "\n",
    "The Reward Model learns human preferences about:\n",
    "\n",
    "* helpfulness\n",
    "* harmlessness\n",
    "* truthfulness\n",
    "* politeness\n",
    "* formatting (JSON, code blocks, lists)\n",
    "* style and conciseness\n",
    "* avoidance of harmful content\n",
    "* refusal behaviors\n",
    "\n",
    "It becomes a **human-simulator of preferences**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Preference Modeling Works\n",
    "\n",
    "Because pretrained + SFT LLMs often produce *several* plausible answers, but humans prefer:\n",
    "\n",
    "* clearer writing\n",
    "* simpler explanations\n",
    "* safer answers\n",
    "* more accurate steps\n",
    "* shorter or more detailed answers depending on intent\n",
    "\n",
    "Preference modeling explicitly teaches the model to:\n",
    "\n",
    "> “Choose the answer that humans like more.”\n",
    "\n",
    "---\n",
    "\n",
    "### Example: What Preference Modeling Fixes\n",
    "\n",
    "#### Without preference modeling:\n",
    "\n",
    "```\n",
    "User: How do I make a bomb?\n",
    "Model: Here is how to make...\n",
    "```\n",
    "\n",
    "#### With preference modeling:\n",
    "\n",
    "```\n",
    "User: How do I make a bomb?\n",
    "Model: I cannot assist with dangerous actions...\n",
    "```\n",
    "\n",
    "Human annotators mark these safety-focused replies as *preferred*.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Preference Modeling\n",
    "\n",
    "#### **RM-based Preferences (RLHF pipeline)**\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Train a Reward Model\n",
    "* Use PPO to optimize LLM based on reward\n",
    "\n",
    "Used by OpenAI (InstructGPT → GPT-3.5 → GPT-4).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Direct Preference Optimization (DPO)**\n",
    "\n",
    "Newer method:\n",
    "\n",
    "* Skips training a reward model\n",
    "* Uses preference pairs directly\n",
    "* Cheaper, simpler, same or better quality\n",
    "* Most open-source models now use DPO\n",
    "\n",
    "---\n",
    "\n",
    "### **3. RLAIF**\n",
    "\n",
    "Preferences generated by AI, not humans.\n",
    "\n",
    "Used by:\n",
    "\n",
    "* Anthropic Claude models\n",
    "* Some LLaMA models\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of Preference Modeling\n",
    "\n",
    "| Benefit            | Why It Matters                                     |\n",
    "| ------------------ | -------------------------------------------------- |\n",
    "| Better alignment   | Helps model behave according to human expectations |\n",
    "| Better safety      | Avoids harmful, toxic, illegal, biased outputs     |\n",
    "| Better reasoning   | Models prefer answers with good logic              |\n",
    "| Better formatting  | JSON, code, stepwise solutions                     |\n",
    "| Better helpfulness | Creates ChatGPT-like behavior                      |\n",
    "\n",
    "Preference modeling tunes *behavior*, not *knowledge*.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Diagram\n",
    "\n",
    "```\n",
    "     Human-Labeled Comparisons\n",
    "       ↓         ↓\n",
    "  (Answer A)   (Answer B)\n",
    "       ↘       ↙\n",
    "         Preference Label\n",
    "       (\"A is better than B\")\n",
    "                 ↓\n",
    "       Train Reward Model\n",
    "                 ↓\n",
    "     Optimize LLM using RM\n",
    " (RLHF, PPO) or (DPO directly)\n",
    "                 ↓\n",
    "     Aligned, safe, helpful model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Preference modeling teaches an LLM to produce answers humans prefer by training it on response comparisons, forming the foundation of RLHF, DPO, and modern aligned assistant models.**\n",
    "\n",
    "| Step                    | What It Teaches                  | Data Type                    | Model Learns                  |\n",
    "| ----------------------- | -------------------------------- | ---------------------------- | ----------------------------- |\n",
    "| **SFT**                 | *How to respond to instructions* | Instruction → Response pairs | Task behavior                 |\n",
    "| **Preference Modeling** | *Which response is better*       | Chosen vs Rejected responses | Human preferences & alignment |\n",
    "\n",
    "-------\n",
    "\n",
    "| Feature               | SFT                    | Preference Modeling               |\n",
    "| --------------------- | ---------------------- | --------------------------------- |\n",
    "| Data Needed           | Instruction → Response | Chosen vs Rejected                |\n",
    "| Teaches               | Skill & behavior       | Human preferences                 |\n",
    "| Fixes                 | Task execution         | Safety, style, reasoning quality  |\n",
    "| Requires ground truth | Yes                    | No                                |\n",
    "| Used in               | LLaMA-Instruct         | RLHF, DPO, Claude-style alignment |\n",
    "| Stage                 | Early                  | After SFT                         |\n",
    "| Alignment             | Weak                   | Strong                            |\n",
    "| Cost                  | Medium                 | High (human labels)               |\n",
    "\n",
    "\n",
    "### Demo\n",
    "\n",
    "1. Create **two responses** for each prompt\n",
    "2. Label one as **chosen** and the other as **rejected**\n",
    "3. Build a **Reward Model (RM)** that predicts a score for each response\n",
    "4. Train the RM so that:\n",
    "   $$\n",
    "   R(\\text{chosen}) > R(\\text{rejected})\n",
    "   $$\n",
    "5. Test the reward model\n",
    "\n",
    "This is exactly the method used in RLHF before PPO / DPO.\n",
    "\n",
    "We use a **tiny DistilBERT** model for demonstration.\n",
    "\n",
    "---\n",
    "\n",
    "#### Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets accelerate torch\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Create a Toy Preference Dataset\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain gravity.\",\n",
    "        \"chosen\": \"Gravity is a force that pulls objects toward each other.\",\n",
    "        \"rejected\": \"Gravity is when stuff goes down for no reason.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is photosynthesis?\",\n",
    "        \"chosen\": \"Plants convert sunlight into energy using chlorophyll.\",\n",
    "        \"rejected\": \"Photosynthesis means plants like the sun.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Build a Tiny Reward Model\n",
    "\n",
    "We use DistilBERT with a **single scalar head** that outputs a *reward score*.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        self.reward_head = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state[:, 0]  # CLS token\n",
    "        reward = self.reward_head(last_hidden)\n",
    "        return reward\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Tokenizer + Model\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = RewardModel()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Preference Loss Function\n",
    "\n",
    "We use a **pairwise Bradley-Terry loss**, standard in preference modeling.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\log(\\sigma(R_c - R_r))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $R_c$ = reward of chosen answer\n",
    "* $R_r$ = reward of rejected answer\n",
    "\n",
    "```python\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def preference_loss(chosen_reward, rejected_reward):\n",
    "    return -torch.mean(torch.log(torch.sigmoid(chosen_reward - rejected_reward)))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Train the Reward Model\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "\n",
    "    for row in dataset:\n",
    "        prompt = row[\"prompt\"]\n",
    "\n",
    "        # Tokenize chosen + rejected\n",
    "        chosen_text = prompt + \" \" + row[\"chosen\"]\n",
    "        rejected_text = prompt + \" \" + row[\"rejected\"]\n",
    "\n",
    "        c = tokenizer(chosen_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        r = tokenizer(rejected_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "        chosen_reward = model(**c)\n",
    "        rejected_reward = model(**r)\n",
    "\n",
    "        loss = preference_loss(chosen_reward, rejected_reward)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "```\n",
    "\n",
    "This trains the RM to give “good” answers a higher score.\n",
    "\n",
    "---\n",
    "\n",
    "#### Test the Reward Model\n",
    "\n",
    "```python\n",
    "def score(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return model(**inputs).item()\n",
    "\n",
    "prompt = \"Explain gravity.\"\n",
    "good = \"Gravity is a force that pulls objects together.\"\n",
    "bad = \"Gravity is magic glue.\"\n",
    "\n",
    "print(\"Good answer score:\", score(prompt + good))\n",
    "print(\"Bad answer score:\", score(prompt + bad))\n",
    "```\n",
    "\n",
    "Expected output:\n",
    "\n",
    "```\n",
    "Good answer score: higher number\n",
    "Bad answer score: lower number\n",
    "```\n",
    "\n",
    "Which shows the reward model learned human preference.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
