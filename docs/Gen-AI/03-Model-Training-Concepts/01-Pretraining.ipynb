{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e4e0f2",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Pretraining\n",
    "\n",
    "**Pretraining** is the process of training a large neural network on massive amounts of general data so that it learns **universal representations** before being adapted to specific tasks.\n",
    "\n",
    "It is the foundation of all modern **foundation models**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Instead of learning every task from scratch, the model first learns the **structure of the world** from huge data.\n",
    "\n",
    "> **Learn language, patterns, and knowledge once â€” then specialize later.**\n",
    "\n",
    "This is similar to how humans learn general concepts before mastering specific skills.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Happens During Pretraining**\n",
    "\n",
    "The model is exposed to enormous datasets (text, images, code, audio, video) and learns by solving a **self-supervised objective**.\n",
    "\n",
    "For language models:\n",
    "\n",
    "$$\n",
    "\\text{Predict next token}\n",
    "$$\n",
    "\n",
    "This forces the model to learn:\n",
    "\n",
    "* Grammar\n",
    "* Facts\n",
    "* Reasoning patterns\n",
    "* World knowledge\n",
    "* Representations of meaning\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Objective Example (LLM)**\n",
    "\n",
    "Given:\n",
    "\n",
    "```\n",
    "\"The capital of France is ___\"\n",
    "```\n",
    "\n",
    "The model learns to predict:\n",
    "\n",
    "```\n",
    "\"Paris\"\n",
    "```\n",
    "\n",
    "Over billions of examples, this builds deep understanding.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Pretraining Works**\n",
    "\n",
    "| Benefit               | Explanation                |\n",
    "| --------------------- | -------------------------- |\n",
    "| Knowledge acquisition | Learns facts & concepts    |\n",
    "| Generalization        | Works across tasks         |\n",
    "| Transfer learning     | Reduces data for new tasks |\n",
    "| Emergent abilities    | Reasoning & abstraction    |\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Large Language Models\n",
    "\n",
    "GPT, Claude, LLaMA, Mistral\n",
    "\n",
    "#### Vision Models\n",
    "\n",
    "CLIP, DINO, ViT\n",
    "\n",
    "#### Multimodal Models\n",
    "\n",
    "GPT-4V, Gemini, Flamingo\n",
    "\n",
    "#### Speech & Audio\n",
    "\n",
    "Whisper, wav2vec\n",
    "\n",
    "---\n",
    "\n",
    "### **Pretraining vs Fine-Tuning**\n",
    "\n",
    "| Stage       | Role                         |\n",
    "| ----------- | ---------------------------- |\n",
    "| Pretraining | Learn general knowledge      |\n",
    "| Fine-tuning | Learn task-specific behavior |\n",
    "\n",
    "---\n",
    "\n",
    "### **Types of Pretraining**\n",
    "\n",
    "* Self-supervised pretraining\n",
    "* Contrastive pretraining\n",
    "* Masked modeling\n",
    "* Next-token prediction\n",
    "* Multimodal pretraining\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition Summary**\n",
    "\n",
    "Pretraining builds the **foundation brain** of the AI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0496208",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
