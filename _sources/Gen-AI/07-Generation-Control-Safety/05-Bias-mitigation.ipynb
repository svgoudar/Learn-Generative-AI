{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b09134",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Bias Mitigation\n",
    "\n",
    "### 1. Motivation and Definition\n",
    "\n",
    "**Bias mitigation** refers to the systematic methods used to **identify, measure, and reduce unfair biases** in machine learning systems so that decisions do not unjustly disadvantage individuals or groups based on sensitive attributes (e.g., gender, race, age).\n",
    "\n",
    "Bias arises because:\n",
    "\n",
    "* Data reflects historical and societal inequalities.\n",
    "* Learning algorithms optimize accuracy, not fairness.\n",
    "* Evaluation metrics often ignore group-level performance.\n",
    "\n",
    "**Goal:**\n",
    "Maintain high predictive performance **while enforcing fairness constraints**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Types of Bias in ML Systems\n",
    "\n",
    "| Bias Type               | Source                              | Example                                       |\n",
    "| ----------------------- | ----------------------------------- | --------------------------------------------- |\n",
    "| **Historical Bias**     | Pre-existing societal inequities    | Fewer women in tech → hiring model favors men |\n",
    "| **Representation Bias** | Imbalanced dataset                  | Face recognition worse for darker skin        |\n",
    "| **Measurement Bias**    | Flawed labels or proxies            | Arrest records used as proxy for crime        |\n",
    "| **Aggregation Bias**    | One model for heterogeneous groups  | Medical model trained mainly on adults        |\n",
    "| **Evaluation Bias**     | Inappropriate benchmarks            | Test set unbalanced across groups             |\n",
    "| **Deployment Bias**     | Mismatch between training and usage | Model used on new population                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Formal Fairness Criteria\n",
    "\n",
    "Let:\n",
    "\n",
    "* $A$ = sensitive attribute (e.g., gender)\n",
    "* $Y$ = true label\n",
    "* $\\hat{Y}$ = model prediction\n",
    "\n",
    "| Criterion                   | Mathematical Definition                            | Interpretation            |\n",
    "| --------------------------- | -------------------------------------------------- | ------------------------- |\n",
    "| **Demographic Parity**      | $P(\\hat{Y}=1 \\mid A=a) = P(\\hat{Y}=1)$           | Equal acceptance rates    |\n",
    "| **Equalized Odds**          | $P(\\hat{Y}=1 \\mid A=a, Y=y)$ equal for all $a$ | Equal error rates         |\n",
    "| **Equality of Opportunity** | $P(\\hat{Y}=1 \\mid A=a, Y=1)$ equal               | Equal true positive rates |\n",
    "| **Calibration**             | $P(Y=1 \\mid \\hat{Y}=p, A=a) = p$                 | Reliable probabilities    |\n",
    "\n",
    "> These criteria are **mutually incompatible** in many real-world scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Bias Mitigation Strategies\n",
    "\n",
    "Bias mitigation occurs at **three levels**:\n",
    "\n",
    "#### A. Pre-processing (Data-Level)\n",
    "\n",
    "Modify training data before learning.\n",
    "\n",
    "| Method                 | Description                         |\n",
    "| ---------------------- | ----------------------------------- |\n",
    "| Reweighing             | Adjust sample weights across groups |\n",
    "| Resampling             | Over/under-sample minority groups   |\n",
    "| Data augmentation      | Generate synthetic samples          |\n",
    "| Feature transformation | Remove proxy variables              |\n",
    "\n",
    "**Example: Reweighing**\n",
    "\n",
    "```python\n",
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### B. In-processing (Algorithm-Level)\n",
    "\n",
    "Modify learning algorithm to enforce fairness constraints.\n",
    "\n",
    "| Method                | Description                             |\n",
    "| --------------------- | --------------------------------------- |\n",
    "| Fairness constraints  | Optimize accuracy under fairness bounds |\n",
    "| Adversarial debiasing | Learn representations independent of A  |\n",
    "| Regularization        | Penalize unfairness in loss             |\n",
    "\n",
    "**Conceptual Objective**\n",
    "\n",
    "[\n",
    "\\min_\\theta ; \\mathcal{L}(\\theta) + \\lambda \\cdot \\text{Unfairness}(\\theta)\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Post-processing (Prediction-Level)\n",
    "\n",
    "Adjust predictions after training.\n",
    "\n",
    "| Method                       | Description                    |\n",
    "| ---------------------------- | ------------------------------ |\n",
    "| Threshold optimization       | Different thresholds per group |\n",
    "| Reject option classification | Change uncertain predictions   |\n",
    "| Calibration by group         | Align group-wise scores        |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. End-to-End Bias Mitigation Workflow\n",
    "\n",
    "1. **Define fairness objective**\n",
    "2. **Audit dataset**\n",
    "3. **Measure baseline bias**\n",
    "4. **Apply mitigation strategy**\n",
    "5. **Retrain / adjust model**\n",
    "6. **Re-evaluate fairness vs accuracy**\n",
    "7. **Deploy with monitoring**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Practical Demonstration with Code\n",
    "\n",
    "Example: Mitigating bias in binary classification.\n",
    "\n",
    "```python\n",
    "from fairlearn.metrics import demographic_parity_difference\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "constraint = DemographicParity()\n",
    "\n",
    "mitigator = ExponentiatedGradient(model, constraint)\n",
    "mitigator.fit(X_train, y_train, sensitive_features=A_train)\n",
    "\n",
    "y_pred = mitigator.predict(X_test)\n",
    "\n",
    "dp_gap = demographic_parity_difference(y_test, y_pred, sensitive_features=A_test)\n",
    "print(\"Demographic parity gap:\", dp_gap)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Trade-offs and Limitations\n",
    "\n",
    "| Challenge                         | Explanation                            |\n",
    "| --------------------------------- | -------------------------------------- |\n",
    "| Accuracy–Fairness tradeoff        | Enforcing fairness may reduce accuracy |\n",
    "| Incompatible fairness definitions | Cannot satisfy all fairness notions    |\n",
    "| Measurement error                 | Sensitive attributes often incomplete  |\n",
    "| Dynamic bias                      | Bias evolves after deployment          |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Industrial Applications\n",
    "\n",
    "| Domain              | Example                  |\n",
    "| ------------------- | ------------------------ |\n",
    "| Hiring              | Fair resume screening    |\n",
    "| Finance             | Credit approval fairness |\n",
    "| Healthcare          | Treatment equity         |\n",
    "| Criminal Justice    | Risk assessment parity   |\n",
    "| Recommender Systems | Exposure fairness        |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Summary\n",
    "\n",
    "Bias mitigation is a **core responsibility** in ML system design, requiring:\n",
    "\n",
    "* Mathematical rigor,\n",
    "* Ethical awareness,\n",
    "* Continuous monitoring,\n",
    "* Careful tradeoff management."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
