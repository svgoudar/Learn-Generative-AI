{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b170ce",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Toxicity Filtering\n",
    "\n",
    "### 1. Motivation & Intuition\n",
    "\n",
    "**Toxicity filtering** is the process of detecting and controlling harmful, offensive, abusive, or unsafe content in user inputs or model outputs.\n",
    "\n",
    "Why it matters in ML systems:\n",
    "\n",
    "| Risk              | Impact                            |\n",
    "| ----------------- | --------------------------------- |\n",
    "| Hate speech       | Legal, ethical, reputational harm |\n",
    "| Harassment        | User safety, product trust        |\n",
    "| Extremism         | Security & compliance             |\n",
    "| Self-harm content | Human safety                      |\n",
    "| Profanity         | Platform moderation               |\n",
    "\n",
    "Large Language Models (LLMs) **must** incorporate toxicity filtering to be deployable in real-world systems.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is \"Toxicity\"?\n",
    "\n",
    "Toxicity is typically defined across multiple dimensions:\n",
    "\n",
    "| Category         | Examples              |\n",
    "| ---------------- | --------------------- |\n",
    "| Hate             | racism, sexism, slurs |\n",
    "| Harassment       | insults, bullying     |\n",
    "| Threats          | violence, coercion    |\n",
    "| Sexual content   | explicit, abusive     |\n",
    "| Self-harm        | suicide encouragement |\n",
    "| Illegal activity | drugs, terrorism      |\n",
    "\n",
    "This is **multi-label classification**, not a single yes/no decision.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. System Architecture\n",
    "\n",
    "A modern toxicity filtering pipeline:\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ↓\n",
    "Pre-Filter (Fast, rule-based)\n",
    "   ↓\n",
    "Neural Toxicity Classifier\n",
    "   ↓\n",
    "Policy Engine (thresholding, action)\n",
    "   ↓\n",
    "Response Control / Blocking / Rewrite\n",
    "   ↓\n",
    "Final Output\n",
    "```\n",
    "\n",
    "At generation guarantees:\n",
    "\n",
    "```\n",
    "Prompt → LLM → Candidate Output → Toxicity Filter → Safe Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Types of Toxicity Filtering\n",
    "\n",
    "| Type                    | Description                          |\n",
    "| ----------------------- | ------------------------------------ |\n",
    "| Rule-based              | Regex, keyword blacklists            |\n",
    "| Statistical ML          | Logistic regression, SVM             |\n",
    "| Deep Learning           | Transformers fine-tuned for toxicity |\n",
    "| Hybrid                  | Rules + neural model                 |\n",
    "| Reinforcement Filtering | Reward models penalize toxicity      |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Neural Toxicity Classifiers\n",
    "\n",
    "Most production systems use **fine-tuned Transformer encoders**:\n",
    "\n",
    "Common datasets:\n",
    "\n",
    "* Jigsaw Toxic Comment\n",
    "* Hatebase\n",
    "* Civil Comments\n",
    "* OpenAI moderation data\n",
    "\n",
    "Typical labels:\n",
    "\n",
    "| Label         |\n",
    "| ------------- |\n",
    "| toxic         |\n",
    "| severe_toxic  |\n",
    "| insult        |\n",
    "| threat        |\n",
    "| obscene       |\n",
    "| identity_hate |\n",
    "| sexual        |\n",
    "| self_harm     |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Training Workflow\n",
    "\n",
    "```\n",
    "Text → Tokenizer → Transformer Encoder → Multi-head classifier → Sigmoid outputs\n",
    "```\n",
    "\n",
    "Loss:\n",
    "\n",
    "```\n",
    "Binary Cross Entropy (multi-label)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Demonstration (PyTorch)\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"I hate you and want to hurt you\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "    probs = torch.sigmoid(logits)\n",
    "\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "for label, p in zip(labels, probs[0]):\n",
    "    print(label, float(p))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Policy Layer (Decision Logic)\n",
    "\n",
    "```python\n",
    "THRESHOLD = 0.7\n",
    "\n",
    "if probs.max() > THRESHOLD:\n",
    "    action = \"BLOCK\"\n",
    "else:\n",
    "    action = \"ALLOW\"\n",
    "```\n",
    "\n",
    "More advanced policies:\n",
    "\n",
    "* Soft rewrite (rephrase safely)\n",
    "* Partial masking\n",
    "* Human review escalation\n",
    "* Safety warnings\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Filtering at Generation Time\n",
    "\n",
    "During decoding:\n",
    "\n",
    "```\n",
    "LLM logits → Apply toxicity penalty → Sample next token\n",
    "```\n",
    "\n",
    "Reward Model discourages toxic continuations.\n",
    "\n",
    "This is the foundation of **RLHF safety alignment**.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Evaluation Metrics\n",
    "\n",
    "| Metric       | Meaning               |\n",
    "| ------------ | --------------------- |\n",
    "| Precision    | Avoid false positives |\n",
    "| Recall       | Catch harmful content |\n",
    "| F1           | Balance               |\n",
    "| AUROC        | Ranking quality       |\n",
    "| Human review | Real-world validity   |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Limitations\n",
    "\n",
    "* Context dependence (sarcasm, quoting)\n",
    "* Cultural & linguistic bias\n",
    "* Adversarial attacks (\"h@te\", spacing, emojis)\n",
    "* Over-filtering harms helpfulness\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Where It Is Used\n",
    "\n",
    "* Chatbots & assistants\n",
    "* Social media moderation\n",
    "* Comment filtering\n",
    "* Search engines\n",
    "* Online gaming chat\n",
    "* Education platforms\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Key Takeaways\n",
    "\n",
    "* Toxicity filtering is a **core safety layer** of modern AI systems.\n",
    "* Implemented using **multi-label Transformer classifiers** plus policy logic.\n",
    "* Must operate **before and after generation**.\n",
    "* Balances **safety, fairness, and usability**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
