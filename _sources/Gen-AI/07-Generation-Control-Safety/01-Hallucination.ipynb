{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3078d42",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Hallucination\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**Hallucination** is the phenomenon where a machine learning model — especially a **Large Language Model (LLM)** — generates **fluent, confident, but factually incorrect or unsupported content** that is not grounded in its training data or the provided context.\n",
    "\n",
    "> In short: **The model sounds right, but is wrong.**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Hallucination Occurs\n",
    "\n",
    "LLMs are **probabilistic sequence models**:\n",
    "\n",
    "[\n",
    "P(\\text{next token} \\mid \\text{previous tokens})\n",
    "]\n",
    "\n",
    "They optimize for **likelihood of text**, **not truth**.\n",
    "\n",
    "| Root Cause           | Explanation                                                    |\n",
    "| -------------------- | -------------------------------------------------------------- |\n",
    "| Training Objective   | Models learn to predict *plausible text*, not verified facts   |\n",
    "| Missing Knowledge    | When the answer is unknown, the model fills gaps with patterns |\n",
    "| Overgeneralization   | Training correlations are mistaken for rules                   |\n",
    "| Context Ambiguity    | Weak or vague prompts increase error                           |\n",
    "| Long-Range Reasoning | Errors accumulate over multi-step generation                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Types of Hallucination\n",
    "\n",
    "| Type         | Description                                 | Example                                     |\n",
    "| ------------ | ------------------------------------------- | ------------------------------------------- |\n",
    "| Intrinsic    | False statements contradict known facts     | \"Einstein won a Nobel Prize for Relativity\" |\n",
    "| Extrinsic    | Unsupported claims outside provided context | Fabricating citations                       |\n",
    "| Entity       | Non-existent people, papers, companies      | \"Smith & Johnson 2023 study\"                |\n",
    "| Logical      | Internally inconsistent reasoning           | Contradicting earlier steps                 |\n",
    "| Mathematical | Incorrect calculations or derivations       | Wrong proofs                                |\n",
    "| Citation     | Fake or incorrect references                | Fabricated DOIs                             |\n",
    "| Contextual   | Ignores given document                      | Answers outside source material             |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Hallucination vs Error\n",
    "\n",
    "| Property      | Error                | Hallucination                  |\n",
    "| ------------- | -------------------- | ------------------------------ |\n",
    "| Confidence    | May appear uncertain | Often highly confident         |\n",
    "| Source        | Computation mistake  | Knowledge or grounding failure |\n",
    "| Detectability | Easier               | Harder                         |\n",
    "| Severity      | Local                | Can corrupt entire output      |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Demonstration\n",
    "\n",
    "**Prompt**\n",
    "\n",
    "> \"Who is the current Prime Minister of Atlantis?\"\n",
    "\n",
    "**Model Output (Hallucinated)**\n",
    "\n",
    "> \"The current Prime Minister of Atlantis is Alexander Maris...\"\n",
    "\n",
    "**Reality**\n",
    "Atlantis does not exist → hallucination.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Mathematical View\n",
    "\n",
    "LLM outputs:\n",
    "\n",
    "[\n",
    "\\hat{y} = \\arg\\max_y P(y \\mid x, \\theta)\n",
    "]\n",
    "\n",
    "But **truthfulness** is not directly optimized:\n",
    "\n",
    "[\n",
    "P(\\text{true}) \\neq P(\\text{likely})\n",
    "]\n",
    "\n",
    "Thus hallucination arises when:\n",
    "\n",
    "[\n",
    "P(\\text{false but plausible}) > P(\\text{unknown})\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Hallucination in ML Workflows\n",
    "\n",
    "```text\n",
    "User Query\n",
    "   ↓\n",
    "Prompt Encoding\n",
    "   ↓\n",
    "Neural Generation (next-token prediction)\n",
    "   ↓\n",
    "Decoding Strategy (temperature, top-k, top-p)\n",
    "   ↓\n",
    "Surface Text\n",
    "   ↓\n",
    "Possible Hallucination\n",
    "```\n",
    "\n",
    "Higher **temperature** → more hallucination risk.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Code Illustration\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"The capital of Atlantis is\"\n",
    "print(generator(prompt, max_length=30))\n",
    "```\n",
    "\n",
    "Typical output:\n",
    "\n",
    "```\n",
    "\"The capital of Atlantis is Oriona...\"\n",
    "```\n",
    "\n",
    "Model invents content because no factual grounding exists.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Why Hallucination Is Hard to Eliminate\n",
    "\n",
    "| Reason                                   |\n",
    "| ---------------------------------------- |\n",
    "| No explicit knowledge base               |\n",
    "| Lack of real-time fact checking          |\n",
    "| Training data is incomplete              |\n",
    "| Neural models lack symbolic verification |\n",
    "| Text fluency ≠ correctness               |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Mitigation Techniques\n",
    "\n",
    "| Method                               | Purpose                   |\n",
    "| ------------------------------------ | ------------------------- |\n",
    "| Retrieval-Augmented Generation (RAG) | Inject verified documents |\n",
    "| Tool Use / Search                    | Real-time grounding       |\n",
    "| Chain-of-Thought + Verification      | Catch logical errors      |\n",
    "| Constrained Decoding                 | Reduce fabrication        |\n",
    "| Self-Consistency Checks              | Sample multiple answers   |\n",
    "| Confidence Estimation                | Detect uncertainty        |\n",
    "| Human-in-the-loop                    | Final validation          |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Hallucination in Production Systems\n",
    "\n",
    "| Risk       | Impact                   |\n",
    "| ---------- | ------------------------ |\n",
    "| Legal      | False advice             |\n",
    "| Medical    | Dangerous misinformation |\n",
    "| Scientific | Invalid research         |\n",
    "| Business   | Wrong decisions          |\n",
    "| Trust      | Loss of credibility      |\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Key Takeaways\n",
    "\n",
    "* Hallucination is **structural**, not a bug.\n",
    "* It emerges from **probabilistic text generation**.\n",
    "* Fluency **does not imply truth**.\n",
    "* Effective systems **must add external grounding and verification**.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Line Summary**\n",
    "\n",
    "> **Hallucination is the inevitable byproduct of predicting language without grounding in reality.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
