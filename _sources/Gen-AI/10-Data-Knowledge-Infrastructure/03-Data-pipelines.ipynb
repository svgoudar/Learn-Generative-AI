{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58429958",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Data Pipelines \n",
    "\n",
    "---\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "A **Data Pipeline for Generative AI** is the end-to-end system that **collects, cleans, transforms, stores, and serves data** to train, fine-tune, evaluate, and operate generative models reliably at scale.\n",
    "\n",
    "It connects:\n",
    "\n",
    "```\n",
    "Raw Data → Model Training → Inference → Monitoring → Continuous Improvement\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Data Pipelines Matter in Generative AI\n",
    "\n",
    "Generative models are **data-hungry and error-amplifying**:\n",
    "\n",
    "| Problem                    | Impact                                  |\n",
    "| -------------------------- | --------------------------------------- |\n",
    "| Poor data quality          | Hallucinations, bias, unstable training |\n",
    "| Inconsistent preprocessing | Non-reproducible results                |\n",
    "| Slow data access           | Training bottlenecks                    |\n",
    "| Untracked versions         | Impossible debugging                    |\n",
    "\n",
    "A robust pipeline ensures:\n",
    "\n",
    "* **Reproducibility**\n",
    "* **Scalability**\n",
    "* **Model quality**\n",
    "* **Continuous learning**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Architecture\n",
    "\n",
    "```\n",
    "Data Sources\n",
    "   ↓\n",
    "Ingestion\n",
    "   ↓\n",
    "Validation & Cleaning\n",
    "   ↓\n",
    "Transformation & Feature Engineering\n",
    "   ↓\n",
    "Storage (Data Lake / Feature Store)\n",
    "   ↓\n",
    "Dataset Versioning\n",
    "   ↓\n",
    "Model Training / Fine-Tuning\n",
    "   ↓\n",
    "Evaluation & Monitoring\n",
    "   ↓\n",
    "Feedback Loop → Back to Data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Key Pipeline Stages\n",
    "\n",
    "### 4.1 Data Sources\n",
    "\n",
    "Typical Generative AI sources:\n",
    "\n",
    "* Text: web pages, books, code, conversations\n",
    "* Images: scraped datasets, labeled images\n",
    "* Audio: speech corpora, podcasts\n",
    "* Structured data: logs, user events, metadata\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Ingestion\n",
    "\n",
    "Goal: **reliably move data into the system**\n",
    "\n",
    "Methods:\n",
    "\n",
    "* Batch ingestion (ETL jobs)\n",
    "* Streaming ingestion (Kafka, Pub/Sub, Kinesis)\n",
    "* API collection\n",
    "* Web scraping\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "data = requests.get(\"https://example.com/data.json\").json()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Validation & Cleaning\n",
    "\n",
    "Tasks:\n",
    "\n",
    "* Remove duplicates\n",
    "* Filter low-quality samples\n",
    "* Normalize formats\n",
    "* Remove unsafe content\n",
    "* Language detection\n",
    "* PII redaction\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"http\\S+\", \"\", t)\n",
    "    return t.strip()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Transformation & Feature Engineering\n",
    "\n",
    "Examples:\n",
    "\n",
    "| Modality | Transformations                       |\n",
    "| -------- | ------------------------------------- |\n",
    "| Text     | tokenization, chunking, labeling      |\n",
    "| Images   | resizing, normalization, augmentation |\n",
    "| Audio    | resampling, spectrogram extraction    |\n",
    "| Code     | parsing, deduplication, formatting    |\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer(clean_text(\"Generative AI is powerful\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 Storage Layer\n",
    "\n",
    "| Component           | Purpose              |\n",
    "| ------------------- | -------------------- |\n",
    "| Data Lake (S3, GCS) | raw & processed data |\n",
    "| Feature Store       | model-ready features |\n",
    "| Metadata Store      | lineage & provenance |\n",
    "| Vector Store        | embeddings for RAG   |\n",
    "\n",
    "```\n",
    "Raw → Bronze → Silver → Gold datasets\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.6 Dataset Versioning\n",
    "\n",
    "Critical for reproducibility:\n",
    "\n",
    "* Hash datasets\n",
    "* Track schema changes\n",
    "* Record preprocessing steps\n",
    "\n",
    "Tools: DVC, LakeFS, MLflow\n",
    "\n",
    "---\n",
    "\n",
    "### 4.7 Training & Fine-Tuning Integration\n",
    "\n",
    "Pipeline exports **model-ready datasets**:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"my_clean_dataset\")\n",
    "trainer.train(ds)\n",
    "```\n",
    "\n",
    "Supports:\n",
    "\n",
    "* Pretraining\n",
    "* Instruction tuning\n",
    "* RLHF\n",
    "* Continual learning\n",
    "\n",
    "---\n",
    "\n",
    "### 4.8 Evaluation & Monitoring\n",
    "\n",
    "Monitor:\n",
    "\n",
    "* Data drift\n",
    "* Distribution shift\n",
    "* Label quality\n",
    "* Toxicity\n",
    "* Coverage gaps\n",
    "\n",
    "```python\n",
    "def drift_score(p, q):\n",
    "    return (p - q).abs().mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.9 Feedback Loop\n",
    "\n",
    "Production feedback improves future data:\n",
    "\n",
    "```\n",
    "User Interactions → Logging → Filtering → Re-training → Improved Model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Types of Generative AI Pipelines\n",
    "\n",
    "| Type                         | Description                  |\n",
    "| ---------------------------- | ---------------------------- |\n",
    "| Offline training pipeline    | Large-scale dataset creation |\n",
    "| Online inference pipeline    | Real-time request processing |\n",
    "| RAG pipeline                 | Retrieval + generation       |\n",
    "| Continuous learning pipeline | Automatic improvement loops  |\n",
    "| Multimodal pipeline          | Text + image + audio         |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Example: Minimal Text Generation Pipeline\n",
    "\n",
    "```python\n",
    "raw = load_raw_data()\n",
    "clean = [clean_text(x) for x in raw]\n",
    "tokens = tokenizer(clean, truncation=True)\n",
    "save_dataset(tokens)\n",
    "train_model(tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Design Principles\n",
    "\n",
    "| Principle                | Benefit                |\n",
    "| ------------------------ | ---------------------- |\n",
    "| Deterministic processing | Reproducibility        |\n",
    "| Schema enforcement       | Stability              |\n",
    "| Strong validation        | Data quality           |\n",
    "| Version control          | Traceability           |\n",
    "| Automated monitoring     | Continuous improvement |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "\n",
    "**Data Pipelines are the foundation of Generative AI systems.**\n",
    "\n",
    "They transform chaotic real-world data into:\n",
    "\n",
    "* **trusted datasets**\n",
    "* **high-quality models**\n",
    "* **scalable production systems**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
