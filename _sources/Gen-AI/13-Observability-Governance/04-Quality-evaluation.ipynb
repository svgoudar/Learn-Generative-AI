{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec99923e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Quality Evaluation\n",
    "\n",
    "### 1. Motivation\n",
    "\n",
    "Generative AI systems produce **open-ended outputs** (text, images, code, audio).\n",
    "Unlike classification, there is **no single correct answer**, making evaluation a central scientific challenge.\n",
    "\n",
    "Quality evaluation answers:\n",
    "\n",
    "* *Is the output correct?*\n",
    "* *Is it useful and aligned with the task?*\n",
    "* *Is it safe and reliable?*\n",
    "* *Is it better than previous models?*\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Dimensions of Quality\n",
    "\n",
    "| Dimension                      | Meaning                                  | Typical Failures            |\n",
    "| ------------------------------ | ---------------------------------------- | --------------------------- |\n",
    "| **Correctness / Faithfulness** | Matches facts, input, or ground truth    | Hallucinations              |\n",
    "| **Relevance**                  | Addresses the prompt intent              | Off-topic generation        |\n",
    "| **Coherence**                  | Logically consistent and well-structured | Contradictions              |\n",
    "| **Fluency**                    | Grammatically and stylistically natural  | Awkward language            |\n",
    "| **Completeness**               | Covers required information              | Missing key content         |\n",
    "| **Safety & Alignment**         | Avoids harmful or biased outputs         | Toxicity, policy violations |\n",
    "| **Usefulness**                 | Helps user accomplish task               | Vague, low utility          |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Taxonomy of Evaluation Methods\n",
    "\n",
    "### 3.1 Automatic Metrics (Reference-Based)\n",
    "\n",
    "Used when **ground truth references** exist.\n",
    "\n",
    "| Metric           | Domain           | Measures                |\n",
    "| ---------------- | ---------------- | ----------------------- |\n",
    "| BLEU             | MT, text         | n-gram precision        |\n",
    "| ROUGE            | Summarization    | n-gram recall           |\n",
    "| METEOR           | MT               | Alignment with synonyms |\n",
    "| CIDEr            | Image captioning | Consensus similarity    |\n",
    "| Exact Match / F1 | QA               | String overlap          |\n",
    "\n",
    "**Limitation:**\n",
    "Surface-level matching ≠ semantic correctness.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Automatic Metrics (Reference-Free)\n",
    "\n",
    "Used for **open-ended** generation.\n",
    "\n",
    "| Metric           | Idea                                     |\n",
    "| ---------------- | ---------------------------------------- |\n",
    "| BERTScore        | Semantic similarity via embeddings       |\n",
    "| Perplexity       | Language model confidence                |\n",
    "| MAUVE            | Distribution similarity (human vs model) |\n",
    "| Self-Consistency | Stability across multiple generations    |\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Model-Based Evaluation (LLM-as-a-Judge)\n",
    "\n",
    "A strong model scores outputs along dimensions.\n",
    "\n",
    "**Typical criteria:**\n",
    "\n",
    "* Helpfulness\n",
    "* Faithfulness\n",
    "* Coherence\n",
    "* Safety\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* Scalable\n",
    "* Captures semantics\n",
    "* Matches human judgments surprisingly well\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Human Evaluation\n",
    "\n",
    "Gold standard for:\n",
    "\n",
    "* Helpfulness\n",
    "* Safety\n",
    "* Preference\n",
    "* Alignment\n",
    "\n",
    "**Protocols**\n",
    "\n",
    "| Method              | Description          |\n",
    "| ------------------- | -------------------- |\n",
    "| Likert scoring      | Rate from 1–5        |\n",
    "| Pairwise preference | Choose better output |\n",
    "| Rubric-based review | Structured checklist |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Evaluation Workflow\n",
    "\n",
    "```text\n",
    "1. Define task & quality criteria\n",
    "2. Collect representative prompts\n",
    "3. Generate model outputs\n",
    "4. Apply automatic metrics\n",
    "5. Run LLM-judge or human review\n",
    "6. Aggregate scores\n",
    "7. Analyze failure modes\n",
    "8. Iterate model & prompts\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Practical Example: Text Generation Evaluation\n",
    "\n",
    "```python\n",
    "from bert_score import score\n",
    "\n",
    "preds = [\"The capital of France is Paris.\"]\n",
    "refs  = [\"Paris is the capital of France.\"]\n",
    "\n",
    "P, R, F1 = score(preds, refs, lang=\"en\")\n",
    "print(F1.mean().item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. LLM-as-a-Judge Example\n",
    "\n",
    "```python\n",
    "judge_prompt = f\"\"\"\n",
    "Evaluate the answer for correctness, coherence, and usefulness (1-5 each).\n",
    "\n",
    "Question: {question}\n",
    "Answer: {model_output}\n",
    "\"\"\"\n",
    "\n",
    "judge_score = llm(judge_prompt)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Composite Scoring\n",
    "\n",
    "Modern systems combine multiple signals:\n",
    "\n",
    "[\n",
    "Q = \\alpha C + \\beta R + \\gamma U + \\delta S\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* (C): Correctness\n",
    "* (R): Relevance\n",
    "* (U): Usefulness\n",
    "* (S): Safety\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Typical Failure Modes Discovered by Evaluation\n",
    "\n",
    "| Failure         | Detection                        |\n",
    "| --------------- | -------------------------------- |\n",
    "| Hallucination   | Faithfulness checks              |\n",
    "| Prompt drift    | Relevance metrics                |\n",
    "| Mode collapse   | Diversity metrics                |\n",
    "| Overconfidence  | Calibration tests                |\n",
    "| Bias / toxicity | Safety classifiers + human audit |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Summary\n",
    "\n",
    "Quality evaluation in Generative AI is:\n",
    "\n",
    "* **Multi-dimensional**\n",
    "* **Hybrid (automatic + human + LLM-based)**\n",
    "* **Task-specific**\n",
    "* **Central to safe deployment and model improvement**\n",
    "\n",
    "Reliable evaluation is what converts **impressive generation** into **trustworthy AI systems**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
