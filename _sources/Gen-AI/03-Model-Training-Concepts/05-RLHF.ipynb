{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd4d982",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "**Reinforcement Learning (RL)** is a machine learning paradigm where an *agent* learns to make decisions through **trial and error**, guided by **rewards** or **penalties**.\n",
    "\n",
    "The agent interacts with an **environment**, takes actions, receives feedback, and uses that to improve its behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Intuition of Reinforcement Learning\n",
    "\n",
    "The intuition is:\n",
    "\n",
    "> “Do an action → see what happens → repeat the actions that lead to good outcomes.”\n",
    "\n",
    "RL does not need labeled data.\n",
    "It learns from **experience**, **feedback**, and **rewards**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components (The RL System)\n",
    "\n",
    "#### **1. Agent**\n",
    "\n",
    "The learner or decision-maker (e.g., robot, LLM, software program).\n",
    "\n",
    "#### **2. Environment**\n",
    "\n",
    "The world the agent interacts with (game, robot room, user conversation).\n",
    "\n",
    "#### **3. State (s)**\n",
    "\n",
    "What the environment looks like at a given moment.\n",
    "\n",
    "#### **4. Action (a)**\n",
    "\n",
    "The choice the agent makes.\n",
    "\n",
    "#### **5. Reward (r)**\n",
    "\n",
    "A number indicating how good the action was.\n",
    "\n",
    "#### **6. Policy (π)**\n",
    "\n",
    "The strategy the agent uses to choose actions.\n",
    "\n",
    "---\n",
    "\n",
    "###  The RL Loop\n",
    "\n",
    "```\n",
    "State ----> Agent ----> Action ----> Environment ----> Reward + New State\n",
    "                 ↑-----------------------------------------------↓\n",
    "```\n",
    "\n",
    "This loop repeats thousands or millions of times until the agent learns a good policy.\n",
    "\n",
    "---\n",
    "\n",
    "###  How RL Learns\n",
    "\n",
    "The agent tries to **maximize cumulative rewards** over time.\n",
    "\n",
    "It has two tasks:\n",
    "\n",
    "#### **Exploration**\n",
    "\n",
    "Try new actions to discover if they lead to better rewards.\n",
    "\n",
    "#### **Exploitation**\n",
    "\n",
    "Use actions that already gave good rewards.\n",
    "\n",
    "The agent learns the best trade-off between the two.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example to Understand RL Quickly\n",
    "\n",
    "#### Example: Teaching a dog a trick\n",
    "\n",
    "* Dog = agent\n",
    "* You = environment\n",
    "* Action = sit\n",
    "* Reward = treat\n",
    "\n",
    "Every time the dog performs the correct action (\"sit\"), it receives a reward.\n",
    "Over many trials, the dog learns the actions that maximize treats.\n",
    "\n",
    "This is exactly how RL works.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Classical RL Methods\n",
    "\n",
    "#### **Value-Based Methods**\n",
    "\n",
    "* Q-Learning\n",
    "* Deep Q-Networks (DQN)\n",
    "\n",
    "These learn a value for each state/action pair.\n",
    "\n",
    "#### **Policy-Based Methods**\n",
    "\n",
    "* REINFORCE\n",
    "* PPO (used in RLHF for ChatGPT)\n",
    "\n",
    "These learn the policy directly.\n",
    "\n",
    "#### **Actor–Critic Methods**\n",
    "\n",
    "* A3C\n",
    "* PPO (again)\n",
    "\n",
    "These combine value and policy learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Where RL Is Used in the Real World\n",
    "\n",
    "* Robotics\n",
    "* Game AI (AlphaGo, AlphaZero)\n",
    "* Autonomous driving\n",
    "* Recommendation systems\n",
    "* Finance trading\n",
    "* Control systems\n",
    "* Conversational AI (LLMs using RLHF)\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Reinforcement Learning in LLMs (RLHF)\n",
    "\n",
    "Modern LLMs like ChatGPT use RL in a special way called **RLHF (Reinforcement Learning from Human Feedback)**.\n",
    "\n",
    "#### RLHF Pipeline:\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT)**\n",
    "   Teach the model how to answer.\n",
    "\n",
    "2. **Preference Modeling (Reward Model)**\n",
    "   Train a reward model that scores human-preferred answers higher.\n",
    "\n",
    "3. **RL Optimization (usually PPO)**\n",
    "   The LLM is optimized to maximize the Reward Model’s score.\n",
    "   This teaches the LLM to:\n",
    "\n",
    "   * be safer\n",
    "   * be more helpful\n",
    "   * avoid harmful content\n",
    "   * respond in a human-preferred style\n",
    "\n",
    "In RLHF, the LLM becomes the *agent*, the Reward Model becomes the *reward function*, and conversation becomes the *environment*.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Intuition of RLHF\n",
    "\n",
    "SFT teaches:\n",
    "\n",
    "> “Here is an example of a good answer.”\n",
    "\n",
    "Preference modeling teaches:\n",
    "\n",
    "> “Between these two answers, A is better.”\n",
    "\n",
    "RLHF teaches:\n",
    "\n",
    "> “Every time you answer like A, you get points. Answer like B, you lose points.”\n",
    "\n",
    "The LLM learns to maximize the reward—just like an RL agent.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why RL Is Powerful\n",
    "\n",
    "RL is uniquely suited for:\n",
    "\n",
    "* long-term decision making\n",
    "* learning from sparse feedback\n",
    "* optimizing behavior rather than predicting labels\n",
    "* adjusting dynamically to different environments\n",
    "\n",
    "Unlike supervised learning, RL does not need explicit targets for every action.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Reinforcement Learning is a learning paradigm where an agent learns through trial-and-error by interacting with an environment and receiving rewards, and it forms the core of how LLMs like ChatGPT become aligned using RLHF.**\n",
    "\n",
    "\n",
    "### Reinforcement Learning Demonstration (Q-Learning):\n",
    "\n",
    "The agent starts at the top-left of a 4×4 grid and must reach the goal at the bottom-right.\n",
    "\n",
    "```\n",
    "S . . .\n",
    ". . . .\n",
    ". . . .\n",
    ". . . G\n",
    "```\n",
    "\n",
    "* S = Start\n",
    "* G = Goal (reward = +10)\n",
    "* Each move costs −1 (to force the agent to find shortest path)\n",
    "\n",
    "The agent learns by **trial and error**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Create a Simple Environment\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "GRID_SIZE = 4\n",
    "GOAL_STATE = (3, 3)\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "ACTIONS = {\n",
    "    0: (-1, 0),  # up\n",
    "    1: (1, 0),   # down\n",
    "    2: (0, -1),  # left\n",
    "    3: (0, 1)    # right\n",
    "}\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Takes an action and returns (next_state, reward, done).\"\"\"\n",
    "    x, y = state\n",
    "    dx, dy = ACTIONS[action]\n",
    "    nx, ny = x + dx, y + dy\n",
    "\n",
    "    # Stay inside grid boundaries\n",
    "    nx = max(0, min(GRID_SIZE - 1, nx))\n",
    "    ny = max(0, min(GRID_SIZE - 1, ny))\n",
    "\n",
    "    next_state = (nx, ny)\n",
    "\n",
    "    if next_state == GOAL_STATE:\n",
    "        return next_state, +10, True\n",
    "\n",
    "    return next_state, -1, False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Initialize Q-Table\n",
    "\n",
    "```python\n",
    "Q = np.zeros((GRID_SIZE, GRID_SIZE, 4))  # state (x,y), action\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  3. Q-Learning Hyperparameters\n",
    "\n",
    "```python\n",
    "alpha = 0.1        # learning rate\n",
    "gamma = 0.9        # discount factor\n",
    "epsilon = 0.2      # exploration probability\n",
    "episodes = 2000\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Training Loop\n",
    "\n",
    "```python\n",
    "for ep in range(episodes):\n",
    "    state = (0, 0)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        x, y = state\n",
    "\n",
    "        # ε-greedy action\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(list(ACTIONS.keys()))\n",
    "        else:\n",
    "            action = np.argmax(Q[x, y])\n",
    "\n",
    "        next_state, reward, done = step(state, action)\n",
    "        nx, ny = next_state\n",
    "\n",
    "        # Q-learning update rule\n",
    "        Q[x, y, action] += alpha * (\n",
    "            reward + gamma * np.max(Q[nx, ny]) - Q[x, y, action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Visualize the Learned Policy\n",
    "\n",
    "```python\n",
    "directions = {0: \"↑\", 1: \"↓\", 2: \"←\", 3: \"→\"}\n",
    "\n",
    "policy_grid = []\n",
    "for i in range(GRID_SIZE):\n",
    "    row = []\n",
    "    for j in range(GRID_SIZE):\n",
    "        if (i, j) == GOAL_STATE:\n",
    "            row.append(\"G\")\n",
    "        else:\n",
    "            best_action = np.argmax(Q[i, j])\n",
    "            row.append(directions[best_action])\n",
    "    policy_grid.append(row)\n",
    "\n",
    "for row in policy_grid:\n",
    "    print(row)\n",
    "```\n",
    "\n",
    "**Expected output (your arrows may vary slightly):**\n",
    "\n",
    "```\n",
    "['→', '→', '↓', '↓']\n",
    "['↓', '→', '↓', '↓']\n",
    "['↓', '↓', '→', '↓']\n",
    "['↓', '↓', '→', 'G']\n",
    "```\n",
    "\n",
    "This shows the path the agent learned to reach the goal efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### How This Demonstration Connects to the RL Explanation\n",
    "\n",
    "| Concept         | How It Appears in Code       |\n",
    "| --------------- | ---------------------------- |\n",
    "| **Agent**       | Learns using Q-table         |\n",
    "| **Environment** | The grid + `step()` function |\n",
    "| **State**       | (x, y) grid coordinates      |\n",
    "| **Action**      | up/down/left/right           |\n",
    "| **Reward**      | −1 per move, +10 at goal     |\n",
    "| **Policy**      | best action = `argmax(Q)`    |\n",
    "| **Exploration** | `random.random() < epsilon`  |\n",
    "| **Learning**    | Q-update rule                |\n",
    "\n",
    "This is **exactly** the trial-and-error learning process used in RL.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
