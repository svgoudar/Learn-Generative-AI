{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d848d0",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Self-Supervised Learning (SSL)\n",
    "\n",
    "**Self-supervised learning** is a training paradigm where the model **creates its own labels from raw data** instead of relying on human-annotated datasets.\n",
    "\n",
    "It is the engine behind modern foundation models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Instead of humans telling the model the answers, the model **sets up puzzles for itself**.\n",
    "\n",
    "> **Use the data to supervise itself.**\n",
    "\n",
    "By solving these internal puzzles, the model learns powerful representations of the world.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "The model takes raw data and constructs **pretext tasks**:\n",
    "\n",
    "| Data  | Self-Created Task       |\n",
    "| ----- | ----------------------- |\n",
    "| Text  | Predict the next word   |\n",
    "| Image | Predict missing patches |\n",
    "| Audio | Predict future sound    |\n",
    "| Video | Predict next frame      |\n",
    "\n",
    "Solving these tasks forces the model to understand structure and meaning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why It Is Powerful**\n",
    "\n",
    "| Advantage                 | Explanation                     |\n",
    "| ------------------------- | ------------------------------- |\n",
    "| No labeling cost          | Works on massive unlabeled data |\n",
    "| Scales easily             | Internet-scale training         |\n",
    "| Learns general features   | Transferable knowledge          |\n",
    "| Enables foundation models | GPT, CLIP, DINO, BERT           |\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Natural Language Processing\n",
    "\n",
    "BERT (masked token prediction), GPT (next token prediction)\n",
    "\n",
    "#### Computer Vision\n",
    "\n",
    "DINO, MAE, SimCLR\n",
    "\n",
    "#### Multimodal AI\n",
    "\n",
    "CLIP, Flamingo, GPT-4V\n",
    "\n",
    "#### Speech & Audio\n",
    "\n",
    "wav2vec, Whisper\n",
    "\n",
    "#### Robotics\n",
    "\n",
    "World models and perception learning\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison with Supervised Learning**\n",
    "\n",
    "| Feature           | Supervised | Self-Supervised |\n",
    "| ----------------- | ---------- | --------------- |\n",
    "| Label requirement | Manual     | Automatic       |\n",
    "| Scalability       | Limited    | Massive         |\n",
    "| Generalization    | Narrow     | Broad           |\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition Summary**\n",
    "\n",
    "Self-supervised learning teaches AI to **learn from the world itself** â€” just like humans do."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
