{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f3d6dc",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)** is the process of training a *pretrained* Large Language Model (LLM) on **labeled instruction–response pairs** so that it can learn to **follow instructions**, behave like an **assistant**, and perform **specific tasks** reliably.\n",
    "\n",
    "SFT transforms a raw pretrained model (which only predicts the next token) into an **instruction-following model**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why SFT Is Needed\n",
    "\n",
    "A pretrained model:\n",
    "\n",
    "* learns language patterns, grammar, facts\n",
    "* predicts the next token in text\n",
    "* does **not** understand instructions\n",
    "* does **not** know how to respond like an assistant\n",
    "\n",
    "Example of a base model:\n",
    "\n",
    "```\n",
    "User: Summarize this paragraph.\n",
    "Model: Summarize this paragraph by saying that...\n",
    "```\n",
    "\n",
    "→ It *continues* the user text instead of answering.\n",
    "\n",
    "SFT fixes this.\n",
    "\n",
    "---\n",
    "\n",
    "### What SFT Actually Does\n",
    "\n",
    "During SFT, the model is trained on examples like:\n",
    "\n",
    "```\n",
    "Instruction: Translate to French\n",
    "Input: How are you?\n",
    "Output: Comment allez-vous ?\n",
    "```\n",
    "\n",
    "The model learns:\n",
    "\n",
    "* how to interpret a user request\n",
    "* how to generate the correct style of answer\n",
    "* how to provide structured outputs\n",
    "* how to follow the expected conversational format\n",
    "\n",
    "---\n",
    "\n",
    "### How SFT Works (Process)\n",
    "\n",
    "#### **1. Prepare an instruction dataset**\n",
    "\n",
    "Examples of:\n",
    "\n",
    "* summarization\n",
    "* classification\n",
    "* translation\n",
    "* question answering\n",
    "* reasoning\n",
    "* coding\n",
    "* safe refusal examples\n",
    "\n",
    "Each sample has:\n",
    "\n",
    "```\n",
    "instruction\n",
    "input (optional)\n",
    "response (target label)\n",
    "```\n",
    "\n",
    "#### **2. Convert to a chat template**\n",
    "\n",
    "Such as:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Summarize this text.\n",
    "### Input:\n",
    "Cats are mammals...\n",
    "### Response:\n",
    "Cats are mammals that...\n",
    "```\n",
    "\n",
    "#### **3. Fine-tune the model**\n",
    "\n",
    "Use **supervised learning** with cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\text{Train the model to predict the correct response tokens.}\n",
    "$$\n",
    "\n",
    "This adjusts the model’s behavior to match the dataset examples.\n",
    "\n",
    "#### **4. (Optional) Apply RLHF / DPO**\n",
    "\n",
    "After SFT, preference optimization further improves:\n",
    "\n",
    "* helpfulness\n",
    "* safety\n",
    "* correctness\n",
    "\n",
    "---\n",
    "\n",
    "### Techniques Used for SFT\n",
    "\n",
    "| Method               | Description                      | Usage                            |\n",
    "| -------------------- | -------------------------------- | -------------------------------- |\n",
    "| **Full Fine-Tuning** | Update *all* model weights       | Highest quality, expensive       |\n",
    "| **LoRA**             | Train only small adapter modules | Efficient, widely used           |\n",
    "| **QLoRA**            | LoRA + 4-bit quantization        | Train large models on small GPUs |\n",
    "\n",
    "---\n",
    "\n",
    "###  What SFT Achieves\n",
    "\n",
    "SFT creates a model that:\n",
    "\n",
    "* follows human instructions\n",
    "* responds in a helpful, conversational manner\n",
    "* performs domain-specific tasks\n",
    "* produces structured outputs (JSON, code)\n",
    "* generalizes across tasks\n",
    "\n",
    "Without SFT, LLMs would *not* behave like chat assistants.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Supervised Fine-Tuning (SFT) trains a pretrained LLM on labeled instruction–response examples so it learns to follow instructions, solve tasks, and act like an assistant rather than a text-completion model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b348a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
