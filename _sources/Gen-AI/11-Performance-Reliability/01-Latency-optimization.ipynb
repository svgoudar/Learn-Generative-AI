{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf4807c",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Latency Optimization\n",
    "\n",
    "Latency is the **end-to-end response time** between a user request and the generated output.\n",
    "For Generative AI systems, low latency is essential for **usability, interactivity, and scalability**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Where Latency Comes From\n",
    "\n",
    "| Component         | Description                               |\n",
    "| ----------------- | ----------------------------------------- |\n",
    "| Model Inference   | Neural network forward pass               |\n",
    "| Tokenization      | Text → tokens and tokens → text           |\n",
    "| Model Loading     | Weight initialization and memory transfer |\n",
    "| Network           | Request/response transfer                 |\n",
    "| Decoding Strategy | Greedy, beam search, sampling             |\n",
    "| Post-processing   | Formatting, safety checks, logging        |\n",
    "\n",
    "Total Latency:\n",
    "\n",
    "$$\n",
    "T = T_{network} + T_{tokenization} + T_{inference} + T_{decoding} + T_{post}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Key Latency Metrics\n",
    "\n",
    "| Metric     | Meaning                    |\n",
    "| ---------- | -------------------------- |\n",
    "| TTFT       | Time To First Token        |\n",
    "| TPOT       | Time Per Output Token      |\n",
    "| End-to-End | Total response time        |\n",
    "| Throughput | Tokens/sec or requests/sec |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Optimization Layers\n",
    "\n",
    "### A. **Model-Level Optimization**\n",
    "\n",
    "| Technique                | Effect                       |\n",
    "| ------------------------ | ---------------------------- |\n",
    "| Quantization (INT8/INT4) | Faster compute, lower memory |\n",
    "| Pruning                  | Removes redundant weights    |\n",
    "| Distillation             | Smaller student model        |\n",
    "| LoRA Adapters            | Lightweight fine-tuning      |\n",
    "| Flash Attention          | Faster attention computation |\n",
    "\n",
    "Example: 8-bit Quantization (PyTorch)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### B. **Inference Optimization**\n",
    "\n",
    "#### 1. KV Caching\n",
    "\n",
    "Avoid recomputing attention for past tokens.\n",
    "\n",
    "```python\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    use_cache=True\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. Efficient Decoding\n",
    "\n",
    "| Strategy      | Latency | Quality |\n",
    "| ------------- | ------- | ------- |\n",
    "| Greedy        | Fastest | Lower   |\n",
    "| Top-k / Top-p | Medium  | High    |\n",
    "| Beam Search   | Slow    | Highest |\n",
    "\n",
    "Use greedy or small top-k for chat systems.\n",
    "\n",
    "---\n",
    "\n",
    "### C. **System-Level Optimization**\n",
    "\n",
    "| Method              | Benefit           |\n",
    "| ------------------- | ----------------- |\n",
    "| Model Sharding      | Fits large models |\n",
    "| Tensor Parallelism  | Parallel compute  |\n",
    "| Batching            | Higher throughput |\n",
    "| Continuous Batching | Stable latency    |\n",
    "| GPU Utilization     | Maximizes compute |\n",
    "\n",
    "Example: Dynamic batching with vLLM\n",
    "\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-2-7b \\\n",
    "  --max-num-batched-tokens 8192 \\\n",
    "  --gpu-memory-utilization 0.9\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### D. **Serving & Infrastructure**\n",
    "\n",
    "| Technique            | Description                      |\n",
    "| -------------------- | -------------------------------- |\n",
    "| Warm Models          | Avoid cold starts                |\n",
    "| Async I/O            | Overlap compute & network        |\n",
    "| Pinned Memory        | Faster CPU↔GPU transfer          |\n",
    "| Speculative Decoding | Predict tokens using small model |\n",
    "| Edge Deployment      | Reduce network delay             |\n",
    "\n",
    "Speculative Decoding Concept:\n",
    "\n",
    "```text\n",
    "Small Model predicts → Large Model verifies → Accept tokens → Skip compute\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. End-to-End Latency Optimization Workflow\n",
    "\n",
    "```text\n",
    "User Request\n",
    "   ↓\n",
    "Prompt Optimization (shorter context)\n",
    "   ↓\n",
    "Tokenization Optimization\n",
    "   ↓\n",
    "KV Cache + Efficient Decoding\n",
    "   ↓\n",
    "Quantized Model on GPU\n",
    "   ↓\n",
    "Streaming First Token\n",
    "   ↓\n",
    "Continuous Batching + Async Serving\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Practical Example: Fast Chat Pipeline\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"mistralai/Mistral-7B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "result = generator(\n",
    "    \"Explain transformers in one sentence:\",\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False   # greedy decoding for low latency\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Common Latency Trade-offs\n",
    "\n",
    "| Faster          | Slower          |\n",
    "| --------------- | --------------- |\n",
    "| Smaller model   | Larger model    |\n",
    "| Quantized       | Full precision  |\n",
    "| Greedy decoding | Beam search     |\n",
    "| Short context   | Long context    |\n",
    "| Edge inference  | Cloud inference |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Target Latency Benchmarks (Interactive AI)\n",
    "\n",
    "| Use Case            | Ideal Latency |\n",
    "| ------------------- | ------------- |\n",
    "| Chat UI             | < 200 ms TTFT |\n",
    "| Voice Assistant     | < 300 ms      |\n",
    "| Search Assistant    | < 500 ms      |\n",
    "| Document Generation | < 1–2 s       |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "\n",
    "Latency optimization in Generative AI is achieved by **co-designing**:\n",
    "\n",
    "* **Models** (quantization, distillation)\n",
    "* **Inference** (KV cache, decoding strategies)\n",
    "* **Systems** (batching, parallelism)\n",
    "* **Infrastructure** (GPU utilization, edge deployment)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
