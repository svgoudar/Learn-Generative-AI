{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c6accb",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Caching\n",
    "\n",
    "### 1. Motivation\n",
    "\n",
    "LLMs are computationally expensive because **each generated token depends on all previous tokens** through self-attention.\n",
    "Caching avoids recomputing redundant intermediate results, reducing:\n",
    "\n",
    "* **Latency**\n",
    "* **Compute cost**\n",
    "* **Memory bandwidth**\n",
    "* **Energy consumption**\n",
    "\n",
    "Caching is therefore fundamental to practical LLM deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Is Cached in LLMs?\n",
    "\n",
    "At each transformer layer, self-attention computes:\n",
    "\n",
    "[\n",
    "Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "]\n",
    "\n",
    "During autoregressive generation, **past keys and values never change**.\n",
    "So we cache:\n",
    "\n",
    "| Cached Object               | Purpose                            |\n",
    "| --------------------------- | ---------------------------------- |\n",
    "| **Key vectors (K)**         | Attention scoring                  |\n",
    "| **Value vectors (V)**       | Context aggregation                |\n",
    "| **Optional:** hidden states | For partial recomputation          |\n",
    "| **Prompt embeddings**       | For repeated system / user prompts |\n",
    "\n",
    "This is called the **KV Cache**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Without vs With Cache\n",
    "\n",
    "#### Without Cache\n",
    "\n",
    "For each new token (t):\n",
    "\n",
    "[\n",
    "O(t^2 \\cdot L)\n",
    "]\n",
    "\n",
    "All previous tokens are reprocessed at every layer (L).\n",
    "\n",
    "#### With Cache\n",
    "\n",
    "For each new token:\n",
    "\n",
    "[\n",
    "O(t \\cdot L)\n",
    "]\n",
    "\n",
    "Only the **new token** is processed, while past K,V are reused.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Autoregressive Generation with KV Cache\n",
    "\n",
    "**Workflow**\n",
    "\n",
    "1. Encode prompt tokens.\n",
    "2. For each layer:\n",
    "\n",
    "   * Compute K,V and store in cache.\n",
    "3. For each new token:\n",
    "\n",
    "   * Compute K,V only for that token.\n",
    "   * Append to cache.\n",
    "   * Attend against cached K,V.\n",
    "\n",
    "**Visualization**\n",
    "\n",
    "```\n",
    "Past tokens: [ t1  t2  t3 ... t(n-1) ]\n",
    "Cached K,V:  [ K1  K2  K3 ... K(n-1) ]\n",
    "\n",
    "New token tn:\n",
    "    compute Kn, Vn\n",
    "    append to cache\n",
    "    attention over [K1 ... Kn]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Types of Caching in LLM Systems\n",
    "\n",
    "| Type                | Scope              | Purpose                               |\n",
    "| ------------------- | ------------------ | ------------------------------------- |\n",
    "| **KV Cache**        | Model-internal     | Speed up generation                   |\n",
    "| **Prompt Cache**    | Application-level  | Reuse repeated system/context prompts |\n",
    "| **Embedding Cache** | Retrieval layer    | Avoid recomputing embeddings          |\n",
    "| **Response Cache**  | API / app          | Reuse full outputs                    |\n",
    "| **Prefix Cache**    | Multi-user servers | Share common prompt prefixes          |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Prompt Caching Example\n",
    "\n",
    "If many requests start with the same system prompt:\n",
    "\n",
    "```\n",
    "\"You are a financial assistant...\"\n",
    "```\n",
    "\n",
    "The model can cache the internal representation of this prefix and only process the user-specific suffix.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Code Demonstration (KV Cache)\n",
    "\n",
    "### PyTorch-style Pseudocode\n",
    "\n",
    "```python\n",
    "cache = [None] * num_layers\n",
    "\n",
    "def generate_next_token(x, cache):\n",
    "    new_cache = []\n",
    "    for l in range(num_layers):\n",
    "        K, V = compute_kv(x, layer=l)\n",
    "\n",
    "        if cache[l] is not None:\n",
    "            K = torch.cat([cache[l][0], K], dim=1)\n",
    "            V = torch.cat([cache[l][1], V], dim=1)\n",
    "\n",
    "        x = attention(x, K, V)\n",
    "        new_cache.append((K, V))\n",
    "\n",
    "    return x, new_cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Memoryâ€“Performance Trade-off\n",
    "\n",
    "| Effect                                | Impact                             |\n",
    "| ------------------------------------- | ---------------------------------- |\n",
    "| Cache size grows with sequence length | Higher VRAM usage                  |\n",
    "| Large batch + long context            | Cache becomes dominant memory cost |\n",
    "| Quantized cache                       | Lower memory, slight quality loss  |\n",
    "| Cache eviction                        | Enables long conversations         |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Advanced Caching Strategies\n",
    "\n",
    "| Technique                | Idea                                |\n",
    "| ------------------------ | ----------------------------------- |\n",
    "| **Paged KV Cache**       | GPU-friendly memory paging          |\n",
    "| **Sliding Window Cache** | Keep only last N tokens             |\n",
    "| **Speculative Cache**    | Cache draft model tokens            |\n",
    "| **Shared Prefix Cache**  | Multi-tenant inference optimization |\n",
    "| **Offloaded Cache**      | Move old cache to CPU / disk        |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why Caching Is Fundamental\n",
    "\n",
    "Without caching:\n",
    "\n",
    "* Real-time chat is impossible\n",
    "* Inference cost explodes quadratically\n",
    "* Long-context models become unusable\n",
    "\n",
    "With caching:\n",
    "\n",
    "* Token generation becomes linear time\n",
    "* Long conversations are feasible\n",
    "* LLMs become economically deployable\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| Concept          | Role                                         |\n",
    "| ---------------- | -------------------------------------------- |\n",
    "| KV Cache         | Core acceleration of autoregressive decoding |\n",
    "| Prompt Cache     | Avoid repeated prefix computation            |\n",
    "| Embedding Cache  | Accelerates retrieval pipelines              |\n",
    "| System Cache     | Scales multi-user LLM services               |\n",
    "| Cache Management | Controls memory & performance                |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
