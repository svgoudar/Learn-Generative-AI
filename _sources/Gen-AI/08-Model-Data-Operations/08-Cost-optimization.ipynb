{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d25a10e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Cost Optimization\n",
    "\n",
    "Cost optimization is the systematic design of **models, data pipelines, infrastructure, and inference workflows** to minimize total operational expense **while preserving output quality and latency constraints**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Cost Optimization Matters\n",
    "\n",
    "Generative AI systems incur costs from:\n",
    "\n",
    "| Cost Component | Source                                       |\n",
    "| -------------- | -------------------------------------------- |\n",
    "| Training       | GPUs, storage, electricity, engineering time |\n",
    "| Inference      | GPU/CPU runtime, memory, bandwidth           |\n",
    "| Model size     | Storage, loading latency, memory footprint   |\n",
    "| Prompting      | Token usage in API-based systems             |\n",
    "| Data           | Collection, labeling, cleaning               |\n",
    "| Deployment     | Autoscaling, monitoring, redundancy          |\n",
    "\n",
    "**Observation:**\n",
    "For production systems, **inference cost dominates** (often >80% of total spend).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cost Structure of a Generative AI System\n",
    "\n",
    "```\n",
    "Total Cost =\n",
    "Training Cost\n",
    "+ Inference Cost\n",
    "+ Data Cost\n",
    "+ Infrastructure Overhead\n",
    "+ Maintenance & Monitoring\n",
    "```\n",
    "\n",
    "#### Typical Breakdown (Production)\n",
    "\n",
    "| Category    | % of Spend |\n",
    "| ----------- | ---------- |\n",
    "| Inference   | 60–85%     |\n",
    "| Training    | 10–25%     |\n",
    "| Data        | 5–10%      |\n",
    "| Infra & Ops | 5–10%      |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Cost Optimization Strategies\n",
    "\n",
    "| Layer          | Strategy                            | Purpose                     |\n",
    "| -------------- | ----------------------------------- | --------------------------- |\n",
    "| Model          | Distillation, quantization, pruning | Reduce model size & compute |\n",
    "| Data           | Dataset curation, synthetic data    | Reduce training cost        |\n",
    "| Training       | Mixed precision, early stopping     | Reduce GPU hours            |\n",
    "| Inference      | Caching, batching, routing          | Reduce per-request cost     |\n",
    "| Prompting      | Prompt compression, RAG             | Reduce token usage          |\n",
    "| Infrastructure | Autoscaling, spot instances         | Reduce idle cost            |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Model-Level Optimization\n",
    "\n",
    "#### A. Knowledge Distillation\n",
    "\n",
    "Train a **small model (student)** to imitate a **large model (teacher)**.\n",
    "\n",
    "```python\n",
    "loss = α * CE(student_logits, labels) + (1-α) * KL(student_logits, teacher_logits)\n",
    "```\n",
    "\n",
    "**Benefit:**\n",
    "Up to **10× cheaper inference** with minimal quality loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Quantization\n",
    "\n",
    "Reduce numeric precision.\n",
    "\n",
    "| Type | Precision | Speedup | Memory Reduction |\n",
    "| ---- | --------- | ------- | ---------------- |\n",
    "| FP32 | 32-bit    | 1×      | baseline         |\n",
    "| FP16 | 16-bit    | ~2×     | 2×               |\n",
    "| INT8 | 8-bit     | ~4×     | 4×               |\n",
    "| INT4 | 4-bit     | ~6–8×   | 8×               |\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Pruning\n",
    "\n",
    "Remove unimportant weights or neurons.\n",
    "\n",
    "**Effect:**\n",
    "Reduces FLOPs → faster inference → lower energy & hardware cost.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Training-Level Optimization\n",
    "\n",
    "#### A. Mixed Precision Training\n",
    "\n",
    "```python\n",
    "with torch.cuda.amp.autocast():\n",
    "    loss = model(inputs)\n",
    "```\n",
    "\n",
    "**Result:** ~50% memory reduction, ~30–50% speedup.\n",
    "\n",
    "#### B. Early Stopping\n",
    "\n",
    "Stop when validation loss plateaus.\n",
    "\n",
    "```python\n",
    "if val_loss > best_loss for 5 epochs:\n",
    "    stop_training()\n",
    "```\n",
    "\n",
    "Prevents unnecessary GPU spending.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Inference-Level Optimization (Biggest Savings)\n",
    "\n",
    "#### A. Request Batching\n",
    "\n",
    "Combine requests.\n",
    "\n",
    "```\n",
    "8 separate requests → 1 batched request → ~5× throughput\n",
    "```\n",
    "\n",
    "#### B. KV Cache Reuse\n",
    "\n",
    "Reuse attention keys/values for long conversations.\n",
    "\n",
    "```python\n",
    "model.generate(input_ids, use_cache=True)\n",
    "```\n",
    "\n",
    "#### C. Response Caching\n",
    "\n",
    "```\n",
    "Same query → cached output → zero inference cost\n",
    "```\n",
    "\n",
    "#### D. Model Routing\n",
    "\n",
    "Use small model by default; escalate only when needed.\n",
    "\n",
    "```python\n",
    "if confidence < threshold:\n",
    "    use_large_model()\n",
    "else:\n",
    "    use_small_model()\n",
    "```\n",
    "\n",
    "**Result:** 50–80% cost reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Prompt & Token Optimization\n",
    "\n",
    "#### A. Prompt Compression\n",
    "\n",
    "```\n",
    "Verbose prompt → compressed instructions → fewer tokens\n",
    "```\n",
    "\n",
    "#### B. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Instead of huge context windows:\n",
    "\n",
    "```\n",
    "Query → Retrieve relevant docs → Small context → Generate\n",
    "```\n",
    "\n",
    "Reduces token count by **5–20×**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Infrastructure Optimization\n",
    "\n",
    "| Technique          | Impact                 |\n",
    "| ------------------ | ---------------------- |\n",
    "| Spot Instances     | 60–90% cheaper GPUs    |\n",
    "| Autoscaling        | No idle GPU cost       |\n",
    "| Cold-start control | Reduce wasted runtime  |\n",
    "| Model sharding     | Efficient memory usage |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Cost vs Quality Trade-off Curve\n",
    "\n",
    "```\n",
    "High cost ──● Large model\n",
    "           │\n",
    "           │\n",
    "           ● Optimized model\n",
    "           │\n",
    "Low cost ──● Over-optimized (quality loss)\n",
    "```\n",
    "\n",
    "Goal: operate near the **Pareto frontier**.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Example: End-to-End Cost Optimization Pipeline\n",
    "\n",
    "```\n",
    "Large model (70B)\n",
    "    ↓ Distillation\n",
    "Medium model (13B)\n",
    "    ↓ Quantization (INT8)\n",
    "Small efficient model (13B INT8)\n",
    "    ↓ Routing + Caching + RAG\n",
    "Production deployment\n",
    "```\n",
    "\n",
    "**Observed Results (Typical):**\n",
    "\n",
    "| Metric             | Before | After   |\n",
    "| ------------------ | ------ | ------- |\n",
    "| Latency            | 900 ms | 150 ms  |\n",
    "| Cost per 1K tokens | $0.12  | $0.02   |\n",
    "| Throughput         | 50 rps | 400 rps |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Summary Table\n",
    "\n",
    "| Layer     | Main Techniques                     |\n",
    "| --------- | ----------------------------------- |\n",
    "| Model     | Distillation, quantization, pruning |\n",
    "| Training  | Mixed precision, early stopping     |\n",
    "| Inference | Caching, batching, routing          |\n",
    "| Prompting | Token compression, RAG              |\n",
    "| Infra     | Autoscaling, spot GPUs              |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Principle\n",
    "\n",
    "> **Most real-world GenAI savings come from inference optimization, not model training.**\n",
    "\n",
    "Optimizing the serving pipeline yields the largest and most sustainable cost reductions.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
