{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6d2647",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Model Serving\n",
    "\n",
    "Model **serving** is the engineering discipline of making trained GenAI models reliably available for real-world use: low-latency inference, scalable traffic handling, observability, safety controls, and continuous updates.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Model Serving Matters\n",
    "\n",
    "Training builds intelligence.\n",
    "**Serving delivers intelligence.**\n",
    "\n",
    "| Challenge    | Why It matters                                 |\n",
    "| ------------ | ---------------------------------------------- |\n",
    "| Latency      | Users expect responses in milliseconds–seconds |\n",
    "| Scalability  | Traffic is bursty and unpredictable            |\n",
    "| Cost control | GPUs are expensive                             |\n",
    "| Reliability  | Production failures destroy trust              |\n",
    "| Versioning   | Models evolve continuously                     |\n",
    "| Safety       | Output must be filtered and governed           |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Core Serving Pipeline\n",
    "\n",
    "```\n",
    "User → API Gateway → Preprocessing → Model Inference → Postprocessing → Response\n",
    "                          ↑                ↓\n",
    "                     Feature Store     Vector DB / Cache\n",
    "```\n",
    "\n",
    "**Responsibilities**\n",
    "\n",
    "1. **Request handling**\n",
    "2. **Tokenization / input shaping**\n",
    "3. **Inference execution**\n",
    "4. **Decoding & safety filters**\n",
    "5. **Caching & logging**\n",
    "6. **Monitoring & rollback**\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Serving Architectures\n",
    "\n",
    "| Architecture         | Description                   | When to use         |\n",
    "| -------------------- | ----------------------------- | ------------------- |\n",
    "| Single Model Server  | One service hosts one model   | Simple deployments  |\n",
    "| Multi-Model Server   | One service hosts many models | Cost optimization   |\n",
    "| Microservice Mesh    | Each component is separate    | Large-scale systems |\n",
    "| Serverless Inference | Auto-scaling functions        | Spiky workloads     |\n",
    "| Edge Serving         | Model runs near users         | Ultra-low latency   |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Inference Modes\n",
    "\n",
    "| Mode      | Behavior                    | Use case           |\n",
    "| --------- | --------------------------- | ------------------ |\n",
    "| Batch     | Process many inputs at once | Offline jobs       |\n",
    "| Online    | One request at a time       | Chat, APIs         |\n",
    "| Streaming | Token-by-token output       | ChatGPT-style UX   |\n",
    "| Async     | Fire-and-forget             | Long running tasks |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Key Serving Techniques for GenAI\n",
    "\n",
    "#### A. Dynamic Batching\n",
    "\n",
    "Combine multiple user requests into one GPU batch.\n",
    "\n",
    "```\n",
    "Requests → Batch → GPU → Split responses\n",
    "```\n",
    "\n",
    "Improves throughput without hurting latency.\n",
    "\n",
    "#### B. KV Cache Reuse\n",
    "\n",
    "Stores attention keys/values to avoid recomputation during decoding.\n",
    "\n",
    "Massive speedup for long prompts and streaming.\n",
    "\n",
    "#### C. Quantization\n",
    "\n",
    "Reduce model precision:\n",
    "\n",
    "| Type | Precision |\n",
    "| ---- | --------- |\n",
    "| FP32 | 32-bit    |\n",
    "| FP16 | 16-bit    |\n",
    "| INT8 | 8-bit     |\n",
    "| INT4 | 4-bit     |\n",
    "\n",
    "Reduces memory and increases throughput.\n",
    "\n",
    "#### D. Speculative Decoding\n",
    "\n",
    "Small model drafts tokens → large model verifies.\n",
    "2–4× decoding speedup.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. End-to-End Example (FastAPI + vLLM)\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Llama-3-8B\")\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(prompt: str):\n",
    "    params = SamplingParams(temperature=0.7, max_tokens=200)\n",
    "    output = llm.generate(prompt, params)\n",
    "    return {\"text\": output[0].outputs[0].text}\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```bash\n",
    "uvicorn app:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "This provides:\n",
    "\n",
    "* Continuous batching\n",
    "* GPU scheduling\n",
    "* Streaming support\n",
    "* KV caching\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Production Tooling Ecosystem\n",
    "\n",
    "| Layer             | Popular Tools               |\n",
    "| ----------------- | --------------------------- |\n",
    "| Inference engine  | vLLM, TensorRT-LLM, TGI     |\n",
    "| Serving framework | FastAPI, Ray Serve, BentoML |\n",
    "| Orchestration     | Kubernetes, KServe          |\n",
    "| Monitoring        | Prometheus, Grafana         |\n",
    "| Caching           | Redis                       |\n",
    "| Vector store      | FAISS, Milvus, Pinecone     |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Model Versioning & Rollout\n",
    "\n",
    "| Technique         | Purpose                        |\n",
    "| ----------------- | ------------------------------ |\n",
    "| Shadow deployment | Test new model on live traffic |\n",
    "| Canary release    | Gradual rollout                |\n",
    "| A/B testing       | Compare performance            |\n",
    "| Rollback          | Immediate recovery             |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Safety & Governance in Serving\n",
    "\n",
    "* Prompt injection detection\n",
    "* Content filtering\n",
    "* Rate limiting\n",
    "* Logging & auditing\n",
    "* Output moderation pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Performance Metrics\n",
    "\n",
    "| Metric            | Meaning             |\n",
    "| ----------------- | ------------------- |\n",
    "| P50 / P99 latency | Response time       |\n",
    "| Throughput        | Requests/sec        |\n",
    "| GPU utilization   | Hardware efficiency |\n",
    "| Token/sec         | Generation speed    |\n",
    "| Cost/request      | Business viability  |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Serving vs Training\n",
    "\n",
    "| Aspect             | Training            | Serving                  |\n",
    "| ------------------ | ------------------- | ------------------------ |\n",
    "| Primary goal       | Learn parameters    | Deliver predictions      |\n",
    "| Compute pattern    | Heavy batch compute | Low-latency compute      |\n",
    "| Failure tolerance  | High                | Very low                 |\n",
    "| Optimization focus | Convergence         | Speed, cost, reliability |\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Mental Model\n",
    "\n",
    "> **Model training creates intelligence.\n",
    "> Model serving operationalizes intelligence.**\n",
    "\n",
    "Without robust serving, GenAI remains a research artifact.\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, the next logical topic is **RAG serving pipelines** or **high-throughput LLM inference optimization**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
