{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d914016",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## KV Cache\n",
    "\n",
    "The **KV Cache** (Key–Value Cache) is an **inference-time optimization** in autoregressive language models.\n",
    "It stores the **Key (K)** and **Value (V)** vectors computed during attention so they do **not** need to be recomputed at every new token generation step.\n",
    "\n",
    "---\n",
    "\n",
    "### Why KV Cache Is Needed\n",
    "\n",
    "In an autoregressive model (GPT-like):\n",
    "\n",
    "* Tokens are generated **one-by-one**.\n",
    "* Each new token must attend to **all previous tokens**.\n",
    "\n",
    "Without caching, when generating token *t*, the model recomputes:\n",
    "\n",
    "```\n",
    "K1, V1\n",
    "K2, V2\n",
    "...\n",
    "K(t-1), V(t-1)\n",
    "Kt , Vt\n",
    "```\n",
    "\n",
    "This means **repeating the same computation hundreds or thousands of times**.\n",
    "\n",
    "This makes decoding **O(n²)** and extremely slow.\n",
    "\n",
    "---\n",
    "\n",
    "### What KV Cache Does\n",
    "\n",
    "#### **It stores all previously computed K and V vectors.**\n",
    "\n",
    "So during step *t*, the model computes **only the new token's** K and V:\n",
    "\n",
    "```\n",
    "K_cache = [K1, K2, ..., K(t-1)]\n",
    "V_cache = [V1, V2, ..., V(t-1)]\n",
    "```\n",
    "\n",
    "At the new step:\n",
    "\n",
    "1. Compute Kt, Vt\n",
    "2. Append them to the cache\n",
    "3. Run attention only between:\n",
    "\n",
    "   * Query Qt (new token)\n",
    "   * Cached Ks & Vs\n",
    "   * Newly computed Kt & Vt\n",
    "\n",
    "Thus, the model **never recomputes** K1…K(t−1) again.\n",
    "\n",
    "This makes decoding **O(n)** instead of **O(n²)**.\n",
    "\n",
    "---\n",
    "\n",
    "### How KV Cache Works Internally\n",
    "\n",
    "#### Prefill Phase\n",
    "\n",
    "* The prompt (initial input) is processed *all at once*.\n",
    "* K and V for each prompt token are computed and placed in cache.\n",
    "* Fast because it is parallel.\n",
    "\n",
    "#### Decode Phase\n",
    "\n",
    "* Model predicts one token at a time.\n",
    "* For each step:\n",
    "\n",
    "  * Reuse cached Ks and Vs\n",
    "  * Compute only K/V for the newest token\n",
    "  * Append them to the cache\n",
    "\n",
    "This step is memory-bandwidth heavy, not compute-heavy.\n",
    "\n",
    "---\n",
    "\n",
    "### Why KV Cache Improves Speed\n",
    "\n",
    "#### Without KV cache:\n",
    "\n",
    "To generate 100 tokens, attention is recomputed over:\n",
    "\n",
    "```\n",
    "1 + 2 + 3 + ... + 100 = 5050 token-steps\n",
    "```\n",
    "\n",
    "#### With KV cache:\n",
    "\n",
    "We compute only:\n",
    "\n",
    "```\n",
    "100 new token-steps\n",
    "```\n",
    "\n",
    "This results in **10×–50× faster inference**, depending on hardware.\n",
    "\n",
    "---\n",
    "\n",
    "### Memory Tradeoff\n",
    "\n",
    "KV cache grows with:\n",
    "\n",
    "* Sequence length\n",
    "* Batch size\n",
    "* Model depth (layers)\n",
    "* Attention heads\n",
    "\n",
    "It can consume several GBs of GPU memory for long sequences.\n",
    "\n",
    "Because of this, models use:\n",
    "\n",
    "* **Multi-Query Attention (MQA)**\n",
    "* **Grouped-Query Attention (GQA)**\n",
    "* **PagedAttention (vLLM)**\n",
    "  to reduce KV memory requirements.\n",
    "\n",
    "---\n",
    "\n",
    "### Simple PyTorch-Style Pseudocode\n",
    "\n",
    "```python\n",
    "class KVCache:\n",
    "    def __init__(self):\n",
    "        self.k = None\n",
    "        self.v = None\n",
    "\n",
    "    def append(self, k_new, v_new):\n",
    "        if self.k is None:\n",
    "            self.k = k_new      # (1, 1, dim)\n",
    "            self.v = v_new\n",
    "        else:\n",
    "            self.k = torch.cat([self.k, k_new], dim=1)\n",
    "            self.v = torch.cat([self.v, v_new], dim=1)\n",
    "\n",
    "def generate_step(x_t, cache):\n",
    "    K_t = W_K(x_t).unsqueeze(1)\n",
    "    V_t = W_V(x_t).unsqueeze(1)\n",
    "    cache.append(K_t, V_t)\n",
    "\n",
    "    Q_t = W_Q(x_t).unsqueeze(1)\n",
    "\n",
    "    scores = Q_t @ cache.k.transpose(-2, -1)\n",
    "    attn = torch.softmax(scores, dim=-1)\n",
    "    out = attn @ cache.v\n",
    "    return out, cache\n",
    "```\n",
    "\n",
    "This snippet shows:\n",
    "\n",
    "* Only K/V for the **new** token are computed\n",
    "* Cached K/V supply all previous context\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**The KV cache stores all past attention keys and values so the model never recomputes them, reducing autoregressive decoding from quadratic to linear time and enabling fast text generation.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
