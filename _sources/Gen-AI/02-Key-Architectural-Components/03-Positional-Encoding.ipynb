{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2700569",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Positional Encodings\n",
    "\n",
    "Transformers use **self-attention**, which treats all tokens **independently** and **in parallel**.\n",
    "\n",
    "Unlike RNNs or CNNs:\n",
    "\n",
    "* There is **no recurrence** (no left→right order)\n",
    "* There is **no convolution window** (no locality)\n",
    "* Tokens have **no inherent notion of position**\n",
    "\n",
    "This means:\n",
    "\n",
    "```\n",
    "[\"The\", \"cat\", \"sat\"]\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```\n",
    "[\"sat\", \"cat\", \"The\"]\n",
    "```\n",
    "\n",
    "produce the **same attention behavior** if embeddings are identical.\n",
    "\n",
    "### Problem:\n",
    "\n",
    "**Self-attention alone cannot understand sequences.**\n",
    "There is no way to know:\n",
    "\n",
    "* which word comes first\n",
    "* which word comes after\n",
    "* long-range dependencies\n",
    "* grammar structure\n",
    "\n",
    "Thus, Transformers need a mechanism to inject **order information** into token embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### What Are Positional Encodings?\n",
    "\n",
    "Positional encodings are **vectors added to token embeddings** to give the model information about **word positions in a sequence**.\n",
    "\n",
    "If:\n",
    "\n",
    "* token embedding = *content meaning*\n",
    "* positional encoding = *position meaning*\n",
    "\n",
    "Then:\n",
    "\n",
    "```\n",
    "final_embedding = token_embedding + positional_encoding\n",
    "```\n",
    "\n",
    "This preserves:\n",
    "\n",
    "* semantic meaning (from token)\n",
    "* sequence order (from position)\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Positional Encodings\n",
    "\n",
    "#### **Absolute Positional Encoding** (Original Transformer)\n",
    "\n",
    "Uses **sinusoidal patterns**:\n",
    "\n",
    "For each position (pos) and dimension (i):\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)\n",
    "$$\n",
    "\n",
    "**Why sinusoidal?**\n",
    "\n",
    "* Allows model to generalize to longer sequences\n",
    "* Same pattern at different scales\n",
    "* Easy for model to compute relative distances\n",
    "\n",
    "Example of intuition:\n",
    "\n",
    "* sin-wave lets the model detect periodic patterns\n",
    "* difference of sin/cos encodings gives position offsets\n",
    "\n",
    "---\n",
    "\n",
    "#### **Learned Positional Embeddings**\n",
    "\n",
    "Instead of predefined sin/cos, the model learns a position embedding table:\n",
    "\n",
    "```\n",
    "position_embedding = nn.Embedding(max_length, embedding_dim)\n",
    "```\n",
    "\n",
    "This is used in BERT, GPT, RoBERTa, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Relative Positional Encodings**\n",
    "\n",
    "Used in Transformer-XL, T5, and modern LLMs.\n",
    "\n",
    "These represent **distance between tokens**, not absolute positions.\n",
    "\n",
    "Example:\n",
    "\n",
    "* “7 tokens away” matters more than “token #73”\n",
    "\n",
    "These are better for:\n",
    "\n",
    "* long sequences\n",
    "* efficient memory reuse\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Implementation: Absolute Sinusoidal Positional Encodings\n",
    "\n",
    "#### Minimal and faithful version:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def sinusoidal_positional_encoding(seq_len, d_model):\n",
    "    pos = torch.arange(seq_len).unsqueeze(1)                    # (seq_len, 1)\n",
    "    i = torch.arange(d_model).unsqueeze(0)                      # (1, d_model)\n",
    "\n",
    "    angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model)\n",
    "    angles = pos * angle_rates                                  # (seq_len, d_model)\n",
    "\n",
    "    # Apply sin to even indices (0,2,4,...), cos to odd (1,3,5,...)\n",
    "    angles[:, 0::2] = torch.sin(angles[:, 0::2])\n",
    "    angles[:, 1::2] = torch.cos(angles[:, 1::2])\n",
    "\n",
    "    return angles\n",
    "\n",
    "# Example usage\n",
    "seq_len = 10\n",
    "d_model = 16\n",
    "\n",
    "pe = sinusoidal_positional_encoding(seq_len, d_model)\n",
    "print(pe.shape)     # (10, 16)\n",
    "print(pe)           # positional encodings\n",
    "```\n",
    "\n",
    "You would **add** these to token embeddings:\n",
    "\n",
    "```python\n",
    "x = torch.randn(seq_len, d_model)\n",
    "x = x + pe\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Implementation: Learned Positional Embeddings\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)               # batch, seq, d_model\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_encoding = self.pos_embed(positions)\n",
    "        return x + pos_encoding\n",
    "\n",
    "# Example\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 32\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "pe_layer = LearnedPositionalEncoding(max_len=100, d_model=d_model)\n",
    "out = pe_layer(x)\n",
    "print(out.shape)   # (2, 5, 32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Demonstration: Why Positional Encoding Helps\n",
    "\n",
    "Without positional encoding:\n",
    "\n",
    "```\n",
    "\"The dog chased the cat\"\n",
    "\"The cat chased the dog\"\n",
    "```\n",
    "\n",
    "After embedding → self-attention → both sentences look similar because:\n",
    "\n",
    "* self-attention sees tokens but not order\n",
    "* no directional flow\n",
    "* no way to know who is subject or object\n",
    "\n",
    "With positional encoding added:\n",
    "\n",
    "```\n",
    "emb(\"The\") + pos[0]\n",
    "emb(\"dog\") + pos[1]\n",
    "emb(\"chased\") + pos[2]\n",
    "...\n",
    "```\n",
    "\n",
    "The model now learns:\n",
    "\n",
    "* position 1 usually holds the subject\n",
    "* position 2 holds the verb\n",
    "* relative relationships\n",
    "* dependency chains\n",
    "\n",
    "This enables grammatical understanding and correct generation.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Feature             | Self-Attention Alone | With Positional Encoding |\n",
    "| ------------------- | -------------------- | ------------------------ |\n",
    "| Order Awareness     | None                 | Yes                      |\n",
    "| Understand syntax   | No                   | Yes                      |\n",
    "| Parallelism         | High                 | High (unchanged)         |\n",
    "| Long-range modeling | Strong               | Stronger                 |\n",
    "\n",
    "Positional encodings **solve the core limitation of attention**:\n",
    "the inability to encode order on its own.\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super().__init__()\n",
    "        self.position_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch, seq_len, d_model = x.shape\n",
    "\n",
    "        # positions: [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)  \n",
    "        # shape: (1, seq_len)\n",
    "\n",
    "        pos_enc = self.position_embed(positions)  # (1, seq_len, d_model)\n",
    "\n",
    "        # Add positional encoding to token embeddings\n",
    "        return x + pos_enc\n",
    "\n",
    "batch = 2\n",
    "seq_len = 5\n",
    "d_model = 16\n",
    "\n",
    "x = torch.randn(batch, seq_len, d_model)\n",
    "\n",
    "pos_enc_layer = LearnedPositionalEncoding(max_len=100, d_model=d_model)\n",
    "out = pos_enc_layer(x)\n",
    "\n",
    "print(out.shape)   # (2, 5, 16)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
