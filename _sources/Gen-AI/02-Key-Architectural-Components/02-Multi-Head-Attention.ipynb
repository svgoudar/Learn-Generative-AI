{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98ebfdb",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "**Multi-Head Attention = Multiple attention mechanisms running in parallel.**\n",
    "\n",
    "Each head learns to focus on **different relationships** between tokens.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Head 1: learns subject–verb alignment\n",
    "* Head 2: learns long-range dependencies\n",
    "* Head 3: learns coreference (“it” → “cat”)\n",
    "* Head 4: learns punctuation/syntax patterns\n",
    "\n",
    "The outputs of all heads are concatenated → projected → passed to next layer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why multiple heads?\n",
    "\n",
    "One attention head can only learn **one type** of relation.\n",
    "Multiple heads allow the model to process **different patterns simultaneously**.\n",
    "\n",
    "For example, in the sentence:\n",
    "\n",
    "**“The cat that I adopted sleeps.”**\n",
    "\n",
    "A good LLM needs to learn:\n",
    "\n",
    "* subject relation: cat → sleeps\n",
    "* relative clause: I adopted → cat\n",
    "* article relations: The → cat\n",
    "* semantic meaning: sleeps → cat\n",
    "\n",
    "One head alone cannot learn all this.\n",
    "\n",
    "---\n",
    "\n",
    "### How Multi-Head Attention works\n",
    "\n",
    "Suppose we have **h heads**.\n",
    "For each head:\n",
    "\n",
    "#### 1. Create separate projection matrices:\n",
    "\n",
    "* $W_Q^1, W_K^1, W_V^1$\n",
    "* $W_Q^2, W_K^2, W_V^2$\n",
    "* ...\n",
    "* $W_Q^h, W_K^h, W_V^h$\n",
    "\n",
    "#### 2. Compute attention independently for each head:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)\n",
    "$$\n",
    "\n",
    "### 3. Concatenate all heads:\n",
    "\n",
    "$$\n",
    "\\text{concat} = [\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h]\n",
    "$$\n",
    "\n",
    "#### 4. Apply a final output projection:\n",
    "\n",
    "$$\n",
    "\\text{MHAoutput} = \\text{concat} \\cdot W_O\n",
    "$$\n",
    "\n",
    "Where $W_O$ is another learned matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visual Overview (simple)**\n",
    "\n",
    "```\n",
    "                ┌─────────────┐\n",
    "Input Embedding →  Head 1      ─┐\n",
    "                ├─────────────┤ │\n",
    "                │  Head 2      │ │\n",
    "                ├─────────────┤ │\n",
    "                │  Head 3      │ │\n",
    "                └─────────────┘ │\n",
    "                                 ↓\n",
    "                    Concatenate Outputs\n",
    "                                 ↓\n",
    "                       Linear Projection\n",
    "                                 ↓\n",
    "                         MHA Output\n",
    "```\n",
    "\n",
    "Each head sees the same input but learns different patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Mini Numerical Example (2 Heads)**\n",
    "\n",
    "To keep it simple:\n",
    "\n",
    "* Only **one token**\n",
    "* Model dimension = 4\n",
    "* Each head dimension = 2\n",
    "* We show how heads create different outputs\n",
    "\n",
    "#### Input token embedding:\n",
    "\n",
    "```\n",
    "X = [1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Head 1 projection matrices**\n",
    "\n",
    "Pick simple values:\n",
    "\n",
    "```\n",
    "WQ1 = [[1,0],[0,1],[0,0],[1,0]]\n",
    "WK1 = [[1,1],[0,1],[1,0],[0,1]]\n",
    "WV1 = [[1,0],[0,2],[1,1],[0,1]]\n",
    "```\n",
    "\n",
    "Compute:\n",
    "\n",
    "```\n",
    "Q1 = X @ WQ1\n",
    "K1 = X @ WK1\n",
    "V1 = X @ WV1\n",
    "```\n",
    "\n",
    "After attention calculation → head1_output\n",
    "(Details skipped to keep it short)\n",
    "\n",
    "Assume:\n",
    "\n",
    "```\n",
    "head1_output = [0.5, 1.2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Head 2 projection matrices**\n",
    "\n",
    "Different values:\n",
    "\n",
    "```\n",
    "WQ2 = [[0,1],[1,0],[1,0],[0,1]]\n",
    "WK2 = [[0,1],[1,1],[0,0],[1,0]]\n",
    "WV2 = [[0,1],[1,1],[0,2],[1,0]]\n",
    "```\n",
    "\n",
    "Compute:\n",
    "\n",
    "```\n",
    "Q2 = X @ WQ2\n",
    "K2 = X @ WK2\n",
    "V2 = X @ WV2\n",
    "```\n",
    "\n",
    "Assume:\n",
    "\n",
    "```\n",
    "head2_output = [−0.4, 2.3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Concatenate heads**\n",
    "\n",
    "```\n",
    "concat = [0.5, 1.2, −0.4, 2.3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Final output projection**\n",
    "\n",
    "With some matrix (W_O):\n",
    "\n",
    "```\n",
    "MHA_output = concat @ W_O\n",
    "```\n",
    "\n",
    "This produces the final vector passed to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Key points to remember**\n",
    "\n",
    "#### **A. Each head has different WQ, WK, WV**\n",
    "\n",
    "So each head attends to different features in the sequence.\n",
    "\n",
    "#### **B. All heads see the full input**\n",
    "\n",
    "But learn different attention patterns.\n",
    "\n",
    "#### **C. Multi-head attention == multi-perspective understanding**\n",
    "\n",
    "This is why LLMs can:\n",
    "\n",
    "* resolve pronouns\n",
    "* understand relationships\n",
    "* perform reasoning\n",
    "* encode structure\n",
    "* remember long context\n",
    "\n",
    "### **D. Outputs are merged**\n",
    "\n",
    "Concatenation → linear projection → next layer.\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Multi-head attention =\n",
    "**“Run attention several times with different learned projections, so the model can focus on multiple aspects of the text at once.”**\n",
    "\n",
    "Each head learns something different.\n",
    "Combine all → richer understanding.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also demonstrate:\n",
    "\n",
    "* A full multi-head numerical example (Q, K, V per head)\n",
    "* How multi-head differs from single-head attention\n",
    "* How multi-head works in GPT specifically\n",
    "\n",
    "### 1. Why Attention Exists (Core Intuition)\n",
    "\n",
    "Before explaining self-attention and cross-attention, understand the **problem** they solve.\n",
    "\n",
    "Neural Machine Translation (NMT) requires:\n",
    "\n",
    "* Reading a source sentence (English)\n",
    "* Understanding it as a whole\n",
    "* Generating a target sentence (French)\n",
    "\n",
    "RNNs and LSTMs struggled because:\n",
    "\n",
    "* They compress the entire meaning of a sentence into a *single* hidden vector.\n",
    "* Long sentences are hard to encode correctly.\n",
    "* They process words sequentially, slowing down training.\n",
    "\n",
    "**Attention** solved this by allowing the model to:\n",
    "\n",
    "* Look back at specific words it needs,\n",
    "* Weigh them differently depending on context,\n",
    "* And process many words in parallel.\n",
    "\n",
    "This idea became the foundation of Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Q, K, V Represent (Intuitive View)\n",
    "\n",
    "In all attention mechanisms, we project token embeddings into:\n",
    "\n",
    "* **Query (Q)** → What I am looking for\n",
    "* **Key (K)** → What information I offer\n",
    "* **Value (V)** → The actual information content\n",
    "\n",
    "Analogy:\n",
    "Imagine researching in a library.\n",
    "\n",
    "* Query = the question you're trying to answer\n",
    "* Key = the index of each book\n",
    "* Value = the content inside the book\n",
    "\n",
    "Attention computes similarity between Query and Key, and uses that to decide how much of Value to read.\n",
    "\n",
    "The formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "This is:\n",
    "\n",
    "1. Compare Q with every K\n",
    "2. Convert similarities into probabilities (softmax)\n",
    "3. Blend the Value vectors using these probabilities\n",
    "\n",
    "This blending produces **contextual embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Self-Attention (Detailed, Practical Intuition)\n",
    "\n",
    "Self-attention means:\n",
    "**A token looks at every other token in the same sentence to understand its contextual meaning.**\n",
    "\n",
    "All Q, K, V come from the *same* sentence.\n",
    "\n",
    "#### Why do we need this?\n",
    "\n",
    "Words change meaning based on context:\n",
    "\n",
    "* *“bank”* (river bank vs monetary bank)\n",
    "* *“trains”* (verb or noun)\n",
    "\n",
    "Self-attention adjusts the embedding of each word based on the other words around it.\n",
    "\n",
    "#### How it works in translation\n",
    "\n",
    "Example:\n",
    "“The boy **trains** the puppy.”\n",
    "\n",
    "The raw word embedding of “trains” is ambiguous.\n",
    "Self-attention allows “trains” to check:\n",
    "\n",
    "* \"boy\" → subject\n",
    "* \"puppy\" → object\n",
    "* \"the\" → determiner\n",
    "\n",
    "Because it sees these words, the layer learns that “trains” is a *verb*, not a noun.\n",
    "\n",
    "**Effect:**\n",
    "A new enriched, context-aware embedding of “trains” is produced.\n",
    "This enriched embedding is what the encoder passes to the decoder.\n",
    "\n",
    "#### Bidirectional vs Masked\n",
    "\n",
    "* **Encoder self-attention**: Can look left and right (bidirectional).\n",
    "* **Decoder self-attention**: Only looks left (causal mask), to prevent cheating by seeing future words when predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cross-Attention (Detailed, Practical Intuition)\n",
    "\n",
    "Cross-attention connects the decoder with the encoder.\n",
    "\n",
    "**The decoder uses its own Query, and attends to the encoder's Key and Value.**\n",
    "\n",
    "#### Why?\n",
    "\n",
    "When generating a translation, the decoder needs to look back at the source sentence.\n",
    "\n",
    "Example:\n",
    "Translating to French:\n",
    "\n",
    "“The boy trains the puppy.” → “Le garçon entraîne le chiot.”\n",
    "\n",
    "When the decoder is about to output the French equivalent of “trains”:\n",
    "\n",
    "* Query = the decoder’s current hidden state\n",
    "* Keys = encoder’s representation of each English word\n",
    "* Values = same encoder representations\n",
    "\n",
    "Cross-attention determines which source word is most relevant.\n",
    "\n",
    "#### What happens internally?\n",
    "\n",
    "Decoder asks:\n",
    "\n",
    "> “Which English word should I focus on now?”\n",
    "\n",
    "The attention score becomes highest for the source word “trains”.\n",
    "\n",
    "So the decoder retrieves that part of the encoder's output and uses it to output the correct French verb form “entraîne”.\n",
    "\n",
    "#### Why cross-attention is critical\n",
    "\n",
    "Without cross-attention:\n",
    "\n",
    "* Decoder would generate output blindly\n",
    "* Translation quality would collapse\n",
    "* Long-range dependencies would be lost\n",
    "\n",
    "Cross-attention is a learnable lookup into encoder memory.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Putting Both Together (Full Translation Process)\n",
    "\n",
    "#### Step 1: Encoder (Self-Attention)\n",
    "\n",
    "The encoder reads the English sentence.\n",
    "\n",
    "Self-attention refines each word:\n",
    "\n",
    "* “trains” becomes a verb representation\n",
    "* “boy” becomes a subject representation\n",
    "* “puppy” becomes an object representation\n",
    "\n",
    "It outputs a sequence of embeddings that represent the whole sentence meaningfully.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Decoder (Masked Self-Attention)\n",
    "\n",
    "When generating output token by token:\n",
    "\n",
    "* The decoder uses masked self-attention to understand what it has generated so far.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Cross-Attention (Connecting encoder and decoder)\n",
    "\n",
    "At each decoding step:\n",
    "\n",
    "* Decoder Q looks at encoder K, V\n",
    "* Retrieves most relevant part of the source sentence\n",
    "* Uses that to produce the next word\n",
    "\n",
    "This is how alignment between languages emerges.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept                      | Source of Q    | Source of K & V | Purpose                             |\n",
    "| ---------------------------- | -------------- | --------------- | ----------------------------------- |\n",
    "| **Self-attention (encoder)** | Encoder tokens | Encoder tokens  | Understand source sentence context  |\n",
    "| **Self-attention (decoder)** | Decoder tokens | Decoder tokens  | Understand partial output so far    |\n",
    "| **Cross-attention**          | Decoder tokens | Encoder tokens  | Link source meaning to output words |\n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Intuition**\n",
    "\n",
    "* **Self-attention helps each word understand its meaning by looking at surrounding words.**\n",
    "* **Cross-attention helps the decoder retrieve the right source information at the right time.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
