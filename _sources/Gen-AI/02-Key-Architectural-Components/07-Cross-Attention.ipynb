{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777a6ecc",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Cross-Attention\n",
    "\n",
    "**Cross-attention** is an attention mechanism where one sequence (the **query**) attends to a *different* sequence (the **context / source**) in order to extract relevant information.\n",
    "\n",
    "It is the mechanism that allows models to **connect two different information sources**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Imagine writing a summary while reading a document.\n",
    "\n",
    "* Your **current sentence** = query\n",
    "* The **document you're reading** = keys & values\n",
    "* Your brain focuses on the most relevant parts of the document for each word you write\n",
    "\n",
    "That focus process is **cross-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "Given:\n",
    "\n",
    "* Query matrix (Q) from one sequence\n",
    "* Key (K) and Value (V) from another sequence\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Placement**\n",
    "\n",
    "#### Encoder–Decoder Transformers\n",
    "\n",
    "```\n",
    "Encoder Output ──► K, V\n",
    "Decoder State ───► Q\n",
    "```\n",
    "\n",
    "The decoder queries the encoder’s output to generate each new token.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why It Is Essential**\n",
    "\n",
    "Cross-attention allows:\n",
    "\n",
    "* Translation (target language attends to source language)\n",
    "* Summarization (summary attends to document)\n",
    "* Multimodal grounding (text attends to images, audio, video)\n",
    "* Retrieval-Augmented Generation (LLM attends to retrieved documents)\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Machine Translation\n",
    "\n",
    "Target sentence attends to source sentence.\n",
    "\n",
    "#### Question Answering\n",
    "\n",
    "Answer attends to the relevant parts of the passage.\n",
    "\n",
    "#### RAG Systems\n",
    "\n",
    "Generated output attends to retrieved documents.\n",
    "\n",
    "#### Multimodal AI\n",
    "\n",
    "Text attends to image features or audio embeddings.\n",
    "\n",
    "#### Vision-Language Models\n",
    "\n",
    "Captioning, visual question answering.\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits**\n",
    "\n",
    "| Benefit               | Explanation                               |\n",
    "| --------------------- | ----------------------------------------- |\n",
    "| Information grounding | Connects output to external knowledge     |\n",
    "| Precision             | Focuses on only the most relevant content |\n",
    "| Modularity            | Decouples source and target sequences     |\n",
    "| Scalability           | Efficient information fusion              |\n",
    "\n",
    "---\n",
    "\n",
    "### **Cross-Attention vs Self-Attention**\n",
    "\n",
    "| Feature         | Self-Attention              | Cross-Attention             |\n",
    "| --------------- | --------------------------- | --------------------------- |\n",
    "| Source of Q,K,V | Same sequence               | Different sequences         |\n",
    "| Purpose         | Understand internal context | Fuse external context       |\n",
    "| Used in         | Encoder & Decoder           | Decoder & multimodal models |\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition Summary**\n",
    "\n",
    "Cross-attention is the model’s way of **looking up relevant information from another source while generating each output token**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
