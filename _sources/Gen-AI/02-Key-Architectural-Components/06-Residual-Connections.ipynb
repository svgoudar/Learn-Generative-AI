{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb2469d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Residual Connection (Skip Connection)\n",
    "\n",
    "A **Residual Connection** is a structural design in neural networks that allows the input of a layer to be **added directly to its output**.\n",
    "It enables very deep networks to train effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Core Intuition**\n",
    "\n",
    "Deep networks should not have to **relearn the identity function**.\n",
    "\n",
    "A residual connection lets the model choose:\n",
    "\n",
    "> **Learn something new OR keep what already works.**\n",
    "\n",
    "This dramatically improves training stability and depth.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Form**\n",
    "\n",
    "Without residual:\n",
    "[\n",
    "y = F(x)\n",
    "]\n",
    "\n",
    "With residual:\n",
    "[\n",
    "y = F(x) + x\n",
    "]\n",
    "\n",
    "Where (F(x)) is the transformation learned by the layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why It Works**\n",
    "\n",
    "####Gradient Flow\n",
    "\n",
    "Gradients can flow directly through the identity path, preventing:\n",
    "\n",
    "* Vanishing gradients\n",
    "* Exploding gradients\n",
    "\n",
    "####Optimization Simplicity\n",
    "\n",
    "Learning a small correction is easier than learning an entire transformation from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### **Architecture Example**\n",
    "\n",
    "```\n",
    "x ────────────────┐\n",
    "                  │\n",
    "      F(x)        ▼\n",
    "x → [ Layer ] → (+) → y\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "\n",
    "#### Transformers\n",
    "\n",
    "Every attention and feedforward block uses residual connections.\n",
    "\n",
    "#### CNNs (ResNet)\n",
    "\n",
    "Enabled training of networks with 100+ layers.\n",
    "\n",
    "#### VAEs & GANs\n",
    "\n",
    "Improves stability and convergence.\n",
    "\n",
    "#### Reinforcement Learning\n",
    "\n",
    "Deep policy networks rely on residuals.\n",
    "\n",
    "---\n",
    "\n",
    "### **Benefits**\n",
    "\n",
    "| Benefit               | Explanation                |\n",
    "| --------------------- | -------------------------- |\n",
    "| Enables deep networks | Hundreds of layers         |\n",
    "| Stable training       | Prevents gradient collapse |\n",
    "| Faster convergence    | Easier optimization        |\n",
    "| Better generalization | Smoother learning          |\n",
    "\n",
    "---\n",
    "\n",
    "### **Residual + LayerNorm (Transformer Block)**\n",
    "\n",
    "```\n",
    "x → LayerNorm → Attention → + → LayerNorm → FFN → +\n",
    "```\n",
    "\n",
    "This combination is the backbone of modern deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition Summary**\n",
    "\n",
    "Residual connections give the network a **shortcut path** that preserves useful information and makes deep learning practical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcaa782",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
