{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fdc02c1",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## **Tokenization**\n",
    "\n",
    "Tokenization is the process of converting raw text into **discrete units (tokens)** that a machine learning model can process.\n",
    "It is the **first step** in almost every NLP and LLM pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Why Tokenization Exists**\n",
    "\n",
    "Neural networks operate on numbers, not text.\n",
    "Tokenization bridges human language and numerical computation:\n",
    "\n",
    "```\n",
    "Text → Tokens → Token IDs → Embeddings → Model\n",
    "```\n",
    "\n",
    "Without good tokenization, even the best model fails.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Core Intuition**\n",
    "\n",
    "Tokenization breaks language into the **smallest meaningful building blocks** that balance:\n",
    "\n",
    "* Expressiveness\n",
    "* Vocabulary size\n",
    "* Computational efficiency\n",
    "\n",
    "> Too large → huge vocabulary\n",
    "> Too small → long sequences, slow models\n",
    "\n",
    "---\n",
    "\n",
    "### **3. What Is a Token?**\n",
    "\n",
    "A **token** can be:\n",
    "\n",
    "* A word\n",
    "* A subword\n",
    "* A character\n",
    "* A byte\n",
    "* A combination\n",
    "\n",
    "Modern LLMs primarily use **subword tokens**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Tokenization Pipeline**\n",
    "\n",
    "```\n",
    "Raw Text\n",
    "   ↓\n",
    "Normalization (lowercasing, unicode cleanup)\n",
    "   ↓\n",
    "Pre-tokenization (split by whitespace / punctuation)\n",
    "   ↓\n",
    "Subword segmentation\n",
    "   ↓\n",
    "Token IDs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Major Tokenization Methods**\n",
    "\n",
    "#### 5.1 Word-Level Tokenization\n",
    "\n",
    "```\n",
    "\"I love NLP\" → [\"I\", \"love\", \"NLP\"]\n",
    "```\n",
    "\n",
    "**Problems:**\n",
    "\n",
    "* Huge vocabulary\n",
    "* Unknown words (OOV)\n",
    "* Poor multilingual support\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.2 Character-Level Tokenization\n",
    "\n",
    "```\n",
    "\"cat\" → [\"c\", \"a\", \"t\"]\n",
    "```\n",
    "\n",
    "**Pros:** No OOV\n",
    "**Cons:** Long sequences, weak semantics\n",
    "\n",
    "---\n",
    "\n",
    "#### 5.3 Subword Tokenization (Modern Standard)\n",
    "\n",
    "Splits text into **frequent fragments**:\n",
    "\n",
    "```\n",
    "\"unhappiness\" → [\"un\", \"happi\", \"ness\"]\n",
    "```\n",
    "\n",
    "Balances vocabulary size and sequence length.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Popular Subword Algorithms**\n",
    "\n",
    "| Algorithm | Used By       | Key Idea                      |\n",
    "| --------- | ------------- | ----------------------------- |\n",
    "| BPE       | GPT, GPT-2    | Merge frequent symbol pairs   |\n",
    "| WordPiece | BERT          | Likelihood-based merges       |\n",
    "| Unigram   | SentencePiece | Probabilistic token selection |\n",
    "| Byte-BPE  | GPT-2, LLaMA  | Byte-level + BPE              |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Example: BPE Tokenization**\n",
    "\n",
    "```\n",
    "Text: \"lower\"\n",
    "Initial: l o w e r\n",
    "Merge frequent pairs → lo w e r → low e r → lower\n",
    "```\n",
    "\n",
    "Resulting tokens become part of the learned vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. From Tokens to Model Input**\n",
    "\n",
    "After tokenization:\n",
    "\n",
    "```\n",
    "Tokens → Token IDs → Embedding Vectors → Model\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"Hello world\"\n",
    "→ [\"Hello\", \"world\"]\n",
    "→ [15496, 995]\n",
    "→ [[0.12, -0.04, ...], [...]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Special Tokens**\n",
    "\n",
    "| Token   | Purpose               |\n",
    "| ------- | --------------------- |\n",
    "| `<BOS>` | Beginning of sequence |\n",
    "| `<EOS>` | End of sequence       |\n",
    "| `<PAD>` | Padding               |\n",
    "| `<UNK>` | Unknown               |\n",
    "| `<CLS>` | Classification        |\n",
    "| `<SEP>` | Separator             |\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Tokenization Challenges**\n",
    "\n",
    "* Multilingual text\n",
    "* Rare words\n",
    "* Emojis & symbols\n",
    "* Domain-specific vocabulary\n",
    "* Efficiency vs expressiveness\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Why Tokenization Quality Matters**\n",
    "\n",
    "| Impact               | Explanation                           |\n",
    "| -------------------- | ------------------------------------- |\n",
    "| Model accuracy       | Poor tokens → poor understanding      |\n",
    "| Training speed       | Sequence length controls cost         |\n",
    "| Generalization       | Good subwords handle new words        |\n",
    "| Multilingual support | Robust segmentation improves coverage |\n",
    "\n",
    "---\n",
    "\n",
    "### **12. Tokenization in Modern LLMs**\n",
    "\n",
    "* GPT, LLaMA: Byte-level BPE\n",
    "* BERT: WordPiece\n",
    "* T5: SentencePiece (Unigram)\n",
    "* Whisper: Multilingual BPE\n",
    "\n",
    "---\n",
    "\n",
    "### **13. Summary**\n",
    "\n",
    "| Aspect          | Description                       |\n",
    "| --------------- | --------------------------------- |\n",
    "| Purpose         | Convert text to model-ready units |\n",
    "| Modern approach | Subword tokenization              |\n",
    "| Design goal     | Compact, expressive, efficient    |\n",
    "| Criticality     | Foundational to LLM performance   |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
