{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e21f4a8",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Sampling Stratergies\n",
    "\n",
    "\n",
    "### Why Sampling Strategies Are Needed\n",
    "\n",
    "During generation, an LLM outputs a **probability distribution** over its entire vocabulary (50k–100k tokens).\n",
    "For example:\n",
    "\n",
    "```\n",
    "Next-token probabilities:\n",
    "mat    → 0.62\n",
    "floor  → 0.18\n",
    "sofa   → 0.09\n",
    "dog    → 0.02\n",
    "…\n",
    "```\n",
    "\n",
    "Sampling strategies determine **which token to pick** from this distribution.\n",
    "\n",
    "Different strategies control:\n",
    "\n",
    "* **creativity**\n",
    "* **stability**\n",
    "* **coherence**\n",
    "* **randomness**\n",
    "* **repetition**\n",
    "\n",
    "---\n",
    "\n",
    "### The Main Sampling Strategies\n",
    "\n",
    "---\n",
    "\n",
    "#### Greedy Sampling (Argmax)\n",
    "\n",
    "##### How it works\n",
    "\n",
    "Pick the token with the **highest probability**.\n",
    "\n",
    "```\n",
    "token = argmax(probabilities)\n",
    "```\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Deterministic\n",
    "* Simple\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Repetitive\n",
    "* Low creativity\n",
    "* Can get stuck (“the the the the…”)\n",
    "\n",
    "**Use case**\n",
    "\n",
    "* Factual extraction\n",
    "* Classification\n",
    "* Low-risk outputs\n",
    "\n",
    "---\n",
    "\n",
    "### Temperature Scaling\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Adjusts the “sharpness” of the probability distribution using a **temperature value T**:\n",
    "\n",
    "* **T < 1** → makes distribution sharper → more predictable\n",
    "* **T > 1** → makes distribution flatter → more random\n",
    "\n",
    "**Example**\n",
    "\n",
    "* T = 0.7 → safer, more focused\n",
    "* T = 1.0 → neutral\n",
    "* T = 1.5 → creative, risky\n",
    "\n",
    "**Use case**\n",
    "\n",
    "Control level of creativity.\n",
    "\n",
    "---\n",
    "\n",
    "### Top-k Sampling\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Keep only the **top k** tokens with highest probability.\n",
    "\n",
    "Example: k=5\n",
    "Keep:\n",
    "\n",
    "```\n",
    "mat, floor, sofa, dog, wall\n",
    "```\n",
    "\n",
    "Drop the rest → re-normalize → sample from these.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Avoids low-quality tokens\n",
    "* Good control over randomness\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* k is fixed → may include too many or too few tokens\n",
    "\n",
    "### Use case\n",
    "\n",
    "General text generation with moderate creativity.\n",
    "\n",
    "---\n",
    "\n",
    "### Top-p Sampling (Nucleus Sampling)\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Choose the **smallest set of tokens** whose total probability ≥ p (e.g. 0.9).\n",
    "\n",
    "Example with p = 0.9:\n",
    "\n",
    "```\n",
    "mat (0.62)\n",
    "floor (0.18)\n",
    "sofa (0.09)\n",
    "----------------\n",
    "sum = 0.89 → add one more token\n",
    "```\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Adaptive (dynamic set size)\n",
    "* Keeps meaningful tokens\n",
    "* Higher-quality output than top-k\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Slightly more complex\n",
    "\n",
    "**Use case**\n",
    "\n",
    "Most modern chat and creativity tasks (default in many LLMs).\n",
    "\n",
    "---\n",
    "\n",
    "### Typical Sampling\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Keeps tokens that fall within a **typical entropy range** of language.\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Removes tokens that are too predictable\n",
    "* Removes tokens that are too surprising\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Very natural sentences\n",
    "* Good balance of creativity + coherence\n",
    "\n",
    "**Use case**\n",
    "\n",
    "Story writing, dialogue, long-form content.\n",
    "\n",
    "---\n",
    "\n",
    "### Repetition Penalty / No-Repeat N-Gram\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Penalizes tokens (or sequences) that were generated recently.\n",
    "\n",
    "Example penalty:\n",
    "\n",
    "```\n",
    "If token \"cat\" appeared many times → reduce its probability next time.\n",
    "```\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Reduces loops or stuck patterns\n",
    "\n",
    "**Use case**\n",
    "\n",
    "Long documents, storytelling, chatbot conversations.\n",
    "\n",
    "---\n",
    "\n",
    "### Beam Search (Less common in LLMs)\n",
    "\n",
    "**How it works**\n",
    "\n",
    "Keeps multiple candidate sequences (“beams”) and expands them in parallel.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Useful in translation tasks\n",
    "* Tries to find best global sequence\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Slow\n",
    "* Produces bland text\n",
    "* Not used in modern LLM conversation\n",
    "\n",
    "---\n",
    "\n",
    "### What Modern LLMs Actually Use\n",
    "\n",
    "Most production LLMs (GPT, Claude, LLaMA, Mistral) use a combination:\n",
    "\n",
    "* **Top-p sampling**\n",
    "* **Temperature**\n",
    "* **Repetition penalty**\n",
    "\n",
    "Example default configuration:\n",
    "\n",
    "```\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "repetition_penalty = 1.1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Strategy               | Randomness  | Creativity  | Quality    | Notes                     |\n",
    "| ---------------------- | ----------- | ----------- | ---------- | ------------------------- |\n",
    "| **Greedy**             | None        | Very low    | Low–medium | Deterministic, repetitive |\n",
    "| **Temperature**        | Adjustable  | Adjustable  | Good       | Soft adjustments          |\n",
    "| **Top-k**              | Medium      | Medium      | Good       | Limits candidate tokens   |\n",
    "| **Top-p**              | Medium–High | Medium–High | Excellent  | Adaptive, most used       |\n",
    "| **Typical**            | Medium      | High        | Very High  | Human-like phrasing       |\n",
    "| **Repetition Penalty** | N/A         | N/A         | Higher     | Prevents loops            |\n",
    "| **Beam Search**        | Low         | Low         | Medium     | Structured tasks only     |\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Explanation**\n",
    "\n",
    "**Sampling strategies decide how an LLM chooses its next token, balancing randomness, coherence, and creativity to produce high-quality text.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
