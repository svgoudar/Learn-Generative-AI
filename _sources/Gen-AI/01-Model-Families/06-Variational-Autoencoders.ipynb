{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89834aee",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Autoencoders and Variational Autoencoders (VAE)\n",
    "\n",
    "Both belong to the **Generative AI** family, designed to **learn compact, meaningful representations (latent space)** of input data and reconstruct it — often leading to powerful generative abilities.\n",
    "\n",
    "---\n",
    "\n",
    "### Autoencoders (AE)\n",
    "\n",
    "#### Core Intuition\n",
    "\n",
    "An **Autoencoder** learns to **compress** data (encode) into a small internal representation and then **rebuild** (decode) the original data from that compressed form.\n",
    "It captures the **most important features** of the input — removing redundancy and noise.\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> “Learning to copy the input efficiently through a bottleneck.”\n",
    "\n",
    "---\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "An Autoencoder has three main components:\n",
    "\n",
    "```\n",
    "Input  →  Encoder  →  Latent Space  →  Decoder  →  Output\n",
    "   x         ↓             z             ↓          x̂\n",
    "```\n",
    "\n",
    "* **Encoder:**\n",
    "  A neural network that compresses input $x$ into a smaller latent vector $z$.\n",
    "  Mathematically:\n",
    "  $$\n",
    "  z = f_\\theta(x)\n",
    "  $$\n",
    "\n",
    "* **Latent Space:**\n",
    "  A compressed “information capsule” representing key features of $x$.\n",
    "\n",
    "* **Decoder:**\n",
    "  Another neural network that reconstructs data from $z$.\n",
    "  $$\n",
    "  \\hat{x} = g_\\phi(z)\n",
    "  $$\n",
    "\n",
    "* **Objective:**\n",
    "  Minimize **Reconstruction Loss**:\n",
    "  $$\n",
    "  L = |x - \\hat{x}|^2\n",
    "  $$\n",
    "  (often Mean Squared Error or Cross-Entropy)\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition in Simple Terms\n",
    "\n",
    "Imagine giving an artist a photo and asking them to draw it from memory.\n",
    "\n",
    "* The **encoder** is like how their brain stores only key features (shape, color, structure).\n",
    "* The **decoder** is their hand drawing it again — not pixel-perfect, but close.\n",
    "\n",
    "---\n",
    "\n",
    "#### Applications\n",
    "\n",
    "* **Denoising:** Remove noise from corrupted images.\n",
    "* **Dimensionality reduction:** Alternative to PCA for nonlinear data.\n",
    "* **Anomaly detection:** Large reconstruction error = anomaly.\n",
    "* **Image compression / feature extraction.**\n",
    "\n",
    "---\n",
    "\n",
    "### Variational Autoencoder (VAE)\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Standard autoencoders learn **a fixed deterministic mapping**: each input → one latent code.\n",
    "→ That’s **not ideal for generating new data** — there’s no smooth “latent space” to sample from.\n",
    "\n",
    "#### VAE Solution\n",
    "\n",
    "VAEs make the latent space **probabilistic**, not deterministic.\n",
    "\n",
    "---\n",
    "\n",
    "#### Architecture Overview\n",
    "\n",
    "Still has **encoder → latent space → decoder**, but with probabilistic modeling:\n",
    "\n",
    "```\n",
    "Input x → Encoder → (μ, σ) → z ~ N(μ, σ²) → Decoder → x̂\n",
    "```\n",
    "\n",
    "* **Encoder:**\n",
    "  Instead of producing a single latent vector, it outputs two:\n",
    "\n",
    "  * Mean vector $\\mu(x)$\n",
    "  * Standard deviation vector $\\sigma(x)$\n",
    "\n",
    "  These define a **Gaussian distribution** in latent space:\n",
    "  $$\n",
    "  z \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "  $$\n",
    "\n",
    "* **Reparameterization Trick:**\n",
    "  To make sampling differentiable for backpropagation:\n",
    "  $$\n",
    "  z = \\mu + \\sigma \\odot \\epsilon,\\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "  $$\n",
    "\n",
    "* **Decoder:**\n",
    "  Generates new samples $\\hat{x} = g_\\phi(z)$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "VAE balances two goals:\n",
    "\n",
    "1. **Reconstruction Loss:** Rebuild input accurately.\n",
    "   $$\n",
    "   L_{recon} = |x - \\hat{x}|^2\n",
    "   $$\n",
    "2. **KL Divergence Loss:** Make latent distribution $q(z|x)$ close to a standard Gaussian $p(z)$:\n",
    "   $$\n",
    "   L_{KL} = D_{KL}(q(z|x) | p(z))\n",
    "   $$\n",
    "\n",
    "**Total Loss:**\n",
    "$$\n",
    "L = L_{recon} + \\beta L_{KL}\n",
    "$$\n",
    "(β is a scaling factor in β-VAE)\n",
    "\n",
    "---\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "* The **encoder** learns a distribution, not a fixed point.\n",
    "* Sampling from this smooth latent space allows you to **generate new, realistic data**.\n",
    "* Points close in latent space correspond to **semantically similar samples**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Applications\n",
    "\n",
    "* **Image generation:** Produce realistic new faces, objects, or digits.\n",
    "* **Data compression:** Learn meaningful latent representations.\n",
    "* **Anomaly detection:** Outliers have high reconstruction + KL loss.\n",
    "* **Latent space arithmetic:**\n",
    "  E.g., “smiling face” − “neutral face” + “male” = “smiling male face”.\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparison: Autoencoder vs VAE\n",
    "\n",
    "| Feature            | Autoencoder            | Variational Autoencoder             |\n",
    "| ------------------ | ---------------------- | ----------------------------------- |\n",
    "| Latent Space       | Deterministic          | Probabilistic                       |\n",
    "| Output             | Reconstructed input    | Reconstructed + Generative          |\n",
    "| Generative Ability | Poor                   | Excellent                           |\n",
    "| Loss Function      | MSE / Cross-Entropy    | Reconstruction + KL Divergence      |\n",
    "| Use Case           | Compression, denoising | Generation, representation learning |\n",
    "\n",
    "---\n",
    "\n",
    "#### Example (MNIST)\n",
    "\n",
    "* Input: Handwritten digit image (28×28)\n",
    "* Encoder: Compresses to latent vector (say 2D)\n",
    "* Decoder: Reconstructs digit\n",
    "* In VAE, you can sample random latent vectors → generate **new digits** never seen before."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
