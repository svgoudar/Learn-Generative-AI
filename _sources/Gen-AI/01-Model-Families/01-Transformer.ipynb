{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b3cd46",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Transformer Architecture\n",
    "\n",
    "The **Transformer** is a deep learning architecture introduced by Vaswani et al. in *“Attention Is All You Need” (2017)*.\n",
    "It completely **replaces recurrence (RNNs)** and **convolution (CNNs)** with a mechanism called **Self-Attention**, making it highly parallel, scalable, and efficient for sequence processing (e.g., text, audio, code).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Transformers\n",
    "\n",
    "Before Transformers:\n",
    "\n",
    "* **RNNs / LSTMs** processed data **sequentially**, so training couldn’t be parallelized.\n",
    "* Long sequences led to **vanishing gradients** and loss of long-term context.\n",
    "* **CNNs** handled local dependencies well but struggled with long-range relationships.\n",
    "\n",
    "**Transformers solve all of this** using **attention**, which allows the model to directly relate any two positions in a sequence — regardless of their distance.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "> Instead of processing sequences step-by-step, the Transformer looks at the *entire sequence at once* and decides which parts are important for understanding each word or token.\n",
    "\n",
    "This is done using **self-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The Transformer follows an **Encoder–Decoder** structure:\n",
    "\n",
    "```\n",
    "Input Sequence  ──► Encoder Stack ──► Context Representation ──► Decoder Stack ──► Output Sequence\n",
    "```\n",
    "\n",
    "Each **Encoder** and **Decoder** is made of **N = 6 identical layers** (in the original paper).\n",
    "\n",
    "\n",
    "![alt text](transformer.png)\n",
    "---\n",
    "\n",
    "### Encoder Structure\n",
    "\n",
    "Each encoder layer has **two main sub-layers**:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "\n",
    "   * Allows each token to “look at” every other token in the sequence and learn relationships.\n",
    "   * Example: In “The cat sat on the mat,” the model can learn that “cat” relates strongly to “sat”.\n",
    "\n",
    "2. **Feed-Forward Neural Network (FFN)**\n",
    "\n",
    "   * Applies a position-wise fully connected network to each token representation.\n",
    "\n",
    "Each sub-layer has a **residual connection** followed by **Layer Normalization**:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$\n",
    "\n",
    "\n",
    "![alt text](encoder.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder Structure\n",
    "\n",
    "Each decoder layer has **three sub-layers**:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "\n",
    "   * Similar to encoder self-attention, but uses **masking** so a token can only attend to *previous* tokens.\n",
    "   * This preserves the autoregressive property (no “peeking ahead”).\n",
    "\n",
    "2. **Encoder–Decoder Attention**\n",
    "\n",
    "   * Allows the decoder to attend to encoder outputs — i.e., focus on relevant parts of the input sequence.\n",
    "\n",
    "3. **Feed-Forward Neural Network (FFN)**\n",
    "\n",
    "   * Same as encoder FFN.\n",
    "\n",
    "Again, residual connections and layer normalization are applied after each sub-layer.\n",
    "\n",
    "\n",
    "![alt text](decoder.png)\n",
    "---\n",
    "\n",
    "### The Self-Attention Mechanism\n",
    "\n",
    "Self-Attention computes a weighted combination of token representations based on **query (Q)**, **key (K)**, and **value (V)** matrices.\n",
    "\n",
    "Given:\n",
    "$$\n",
    "Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "$$\n",
    "where ( X ) is the input sequence embedding matrix.\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "**Key Intuition:**\n",
    "\n",
    "* $QK^T$ gives **similarity scores** between tokens.\n",
    "* The **softmax** turns these scores into **attention weights**.\n",
    "* The model then takes a **weighted sum** of all token representations (( V )) to produce context-aware outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Rather than performing one attention calculation, Transformers use multiple **attention heads** in parallel.\n",
    "\n",
    "Each head focuses on **different relationships** (e.g., grammar, context, position).\n",
    "\n",
    "The outputs of all heads are concatenated and linearly projected:\n",
    "$$\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W_O\n",
    "$$\n",
    "where each head ($i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$.\n",
    "\n",
    "---\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since the Transformer has no recurrence, it needs to know **token order**.\n",
    "\n",
    "A **positional encoding** is added to the input embeddings:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad\n",
    "PE_{(pos, 2i+1)} = \\cos!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "These sinusoidal patterns allow the model to infer both **absolute** and **relative** positions of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Feed-Forward Network (FFN)\n",
    "\n",
    "Each token is independently passed through:\n",
    "$$\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "It provides **non-linearity** and helps mix information within each token representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Residual Connection + Layer Normalization\n",
    "\n",
    "Each sub-layer output is:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$\n",
    "This stabilizes training and helps gradient flow through deep networks (solving vanishing gradient problems).\n",
    "\n",
    "---\n",
    "\n",
    "### Training and Objective\n",
    "\n",
    "Transformers are trained to **predict the next token** (autoregressive training) using **cross-entropy loss**:\n",
    "$$\n",
    "L = -\\sum_{t} \\log P(y_t | y_{<t}, x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "| **Aspect**         | **Transformer**              | **RNN/LSTM**      |\n",
    "| ------------------ | ---------------------------- | ----------------- |\n",
    "| Parallelization    | Fully parallel               | Sequential        |\n",
    "| Long-range context | Excellent                    | Weak              |\n",
    "| Training speed     | Fast (GPU-optimized)         | Slow              |\n",
    "| Interpretability   | Attention maps interpretable | Hard to visualize |\n",
    "\n",
    "---\n",
    "\n",
    "### Variants of Transformers\n",
    "\n",
    "| **Model**                    | **Type**        | **Application**                                 |\n",
    "| ---------------------------- | --------------- | ----------------------------------------------- |\n",
    "| **BERT**                     | Encoder-only    | Text understanding (classification, QA)         |\n",
    "| **GPT**                      | Decoder-only    | Text generation                                 |\n",
    "| **T5 / BART**                | Encoder–Decoder | Text-to-text tasks (translation, summarization) |\n",
    "| **Vision Transformer (ViT)** | Encoder-only    | Image classification                            |\n",
    "| **Perceiver / AudioLM**      | Multimodal      | Audio, video, text fusion                       |\n",
    "\n",
    "---\n",
    "\n",
    "### Intuitive Analogy\n",
    "\n",
    "Think of **attention** as a \"spotlight\" in a text:\n",
    "\n",
    "* When the model reads a word like “bank,” it scans the whole sentence (“river bank” vs “money bank”) to find which context is most relevant.\n",
    "* The attention mechanism lets it focus on the right context dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| **Component**                       | **Function**                               |\n",
    "| ----------------------------------- | ------------------------------------------ |\n",
    "| **Embedding + Positional Encoding** | Represent tokens and their positions       |\n",
    "| **Self-Attention**                  | Capture relationships between tokens       |\n",
    "| **Multi-Head Attention**            | Look at multiple relationships in parallel |\n",
    "| **Feed-Forward Network**            | Nonlinear transformation for each token    |\n",
    "| **Residual + LayerNorm**            | Stabilize and speed up training            |\n",
    "| **Encoder–Decoder**                 | Map input sequence to output sequence      |\n",
    "\n",
    "---\n",
    "\n",
    "In short, **Transformers revolutionized deep learning** by removing recurrence and convolution entirely, replacing them with **attention mechanisms** that enable massive parallelization and long-context understanding — the foundation of models like **BERT**, **GPT**, and **T5**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bb1a2",
   "metadata": {},
   "source": [
    "\n",
    "###  Encoder-Only Architecture\n",
    "\n",
    "**What it is**\n",
    "\n",
    "* Uses **only the Transformer encoder stack**\n",
    "* Learns **bidirectional context** (looks left and right)\n",
    "\n",
    "**How it works**\n",
    "\n",
    "* Self-attention attends to **all tokens simultaneously**\n",
    "* No masking\n",
    "* Produces contextual embeddings for each token\n",
    "\n",
    "**Best suited for**\n",
    "\n",
    "* Understanding tasks (not generation)\n",
    "\n",
    "**Typical tasks**\n",
    "\n",
    "* Text classification\n",
    "* Sentiment analysis\n",
    "* Named Entity Recognition (NER)\n",
    "* Semantic similarity\n",
    "* Embeddings generation\n",
    "\n",
    "**Examples**\n",
    "\n",
    "* **BERT**\n",
    "* RoBERTa\n",
    "* DistilBERT\n",
    "* ALBERT\n",
    "\n",
    "**Key limitation**\n",
    "\n",
    "* Cannot generate text autoregressively\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder-Only Architecture\n",
    "\n",
    "**What it is**\n",
    "\n",
    "* Uses **only the Transformer decoder stack**\n",
    "* Designed for **autoregressive generation**\n",
    "\n",
    "**How it works**\n",
    "\n",
    "* Uses **causal (masked) self-attention**\n",
    "* Each token can attend **only to previous tokens**\n",
    "* Generates text **token by token**\n",
    "\n",
    "**Best suited for**\n",
    "\n",
    "* Text generation and reasoning\n",
    "\n",
    "**Typical tasks**\n",
    "\n",
    "* Chatbots\n",
    "* Code generation\n",
    "* Story writing\n",
    "* Math reasoning\n",
    "* SQL generation\n",
    "\n",
    "**Examples**\n",
    "\n",
    "* **GPT-2 / GPT-3 / GPT-4**\n",
    "* LLaMA\n",
    "* Mistral\n",
    "* Falcon\n",
    "\n",
    "**Key limitation**\n",
    "\n",
    "* Weaker at pure “understanding” tasks compared to encoder models\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Encoder–Decoder Architecture (Seq2Seq)\n",
    "\n",
    "**What it is**\n",
    "\n",
    "* Combines **encoder + decoder**\n",
    "* Encoder understands input\n",
    "* Decoder generates output conditioned on encoder output\n",
    "\n",
    "**How it works**\n",
    "\n",
    "1. Encoder builds contextual representation of input\n",
    "2. Decoder uses:\n",
    "\n",
    "   * Masked self-attention\n",
    "   * **Cross-attention** over encoder outputs\n",
    "\n",
    "**Best suited for**\n",
    "\n",
    "* Input → Output transformation tasks\n",
    "\n",
    "**Typical tasks**\n",
    "\n",
    "* Machine translation\n",
    "* Summarization\n",
    "* Question answering\n",
    "* Text rewriting\n",
    "* Speech-to-text\n",
    "\n",
    "**Examples**\n",
    "\n",
    "* **T5**\n",
    "* BART\n",
    "* Marian\n",
    "* FLAN-T5\n",
    "\n",
    "**Key limitation**\n",
    "\n",
    "* Heavier and slower than decoder-only for chat use cases\n",
    "\n",
    "---\n",
    "\n",
    "**Side-by-Side Comparison (Interview Table)**\n",
    "\n",
    "| Aspect              | Encoder-Only   | Decoder-Only   | Encoder–Decoder             |\n",
    "| ------------------- | -------------- | -------------- | --------------------------- |\n",
    "| Context             | Bidirectional  | Unidirectional | Bidirectional (encoder)     |\n",
    "| Masking             | No             | Causal         | Encoder: No, Decoder: Yes   |\n",
    "| Text Generation     | ❌              | ✅              | ✅                           |\n",
    "| Understanding Tasks | ✅ Excellent    | ⚠️ Moderate    | ✅ Excellent                 |\n",
    "| Cross-Attention     | ❌              | ❌              | ✅                           |\n",
    "| Architecture Size   | Medium         | Large          | Largest                     |\n",
    "| Latency             | Low            | Medium         | High                        |\n",
    "| Common Use          | Classification | Chat / Code    | Translation / Summarization |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
