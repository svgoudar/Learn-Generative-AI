{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d704130",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Mixture of Experts (MoE)\n",
    "\n",
    "**Mixture of Experts (MoE)** is a neural network architecture that improves **scalability and efficiency** by activating **only a small subset of specialized sub-networks (\"experts\")** for each input instead of using the entire model every time.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "Instead of one massive model processing everything:\n",
    "\n",
    "> **A router selects the most relevant experts → only those experts run → outputs are combined.**\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "1. **Experts**\n",
    "   Independent neural networks, each specialized in certain patterns or knowledge.\n",
    "\n",
    "2. **Router (Gating Network)**\n",
    "   Chooses which experts should process the current token or input.\n",
    "\n",
    "3. **Sparse Activation**\n",
    "   Only **k experts out of N** are activated per token (e.g., 2 out of 64).\n",
    "\n",
    "---\n",
    "\n",
    "### Why MoE Works\n",
    "\n",
    "| Benefit               | Explanation                             |\n",
    "| --------------------- | --------------------------------------- |\n",
    "| Massive capacity      | Total parameters can be extremely large |\n",
    "| Efficient computation | Only a few experts run per token        |\n",
    "| Better specialization | Experts learn distinct patterns         |\n",
    "| Lower training cost   | Compute stays manageable                |\n",
    "\n",
    "---\n",
    "\n",
    "### Where MoE is Used\n",
    "\n",
    "* GPT-4 (rumored)\n",
    "* Mixtral\n",
    "* Google’s Switch Transformer\n",
    "* DeepSpeed-MoE\n",
    "\n",
    "---\n",
    "\n",
    "### Example Intuition\n",
    "\n",
    "Imagine a hospital:\n",
    "\n",
    "* Many specialists (experts)\n",
    "* One triage nurse (router)\n",
    "* Only the right doctors treat each patient\n",
    "\n",
    "---\n",
    "\n",
    "### Key Tradeoffs\n",
    "\n",
    "| Advantage             | Challenge             |\n",
    "| --------------------- | --------------------- |\n",
    "| Scales to huge models | Routing complexity    |\n",
    "| High performance      | Load balancing issues |\n",
    "| Lower inference cost  | Harder training       |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use MoE\n",
    "\n",
    "When building **very large models** that must remain computationally efficient while increasing knowledge and capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd2109d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
