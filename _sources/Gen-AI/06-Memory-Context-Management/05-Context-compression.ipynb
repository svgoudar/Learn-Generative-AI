{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c753e13e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Context Compression\n",
    "\n",
    "### 1. Definition\n",
    "\n",
    "**Context Compression** is the process of **reducing the size of input context** while **preserving the information necessary** for a model to generate accurate, coherent, and task-relevant outputs.\n",
    "\n",
    "It is essential because large language models (LLMs) have **finite context windows**, yet real-world applications often involve **long documents, conversations, and knowledge bases**.\n",
    "\n",
    "> Goal:\n",
    "> **Maximize useful information per token**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why Context Compression Is Necessary\n",
    "\n",
    "| Constraint     | Description                                       |\n",
    "| -------------- | ------------------------------------------------- |\n",
    "| Context Window | Models accept a limited number of tokens          |\n",
    "| Latency        | Larger prompts increase inference time            |\n",
    "| Cost           | More tokens = higher compute cost                 |\n",
    "| Noise          | Irrelevant information degrades reasoning quality |\n",
    "| Memory         | Long-term interactions exceed window capacity     |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Core Principles\n",
    "\n",
    "Context compression aims to maintain:\n",
    "\n",
    "* **Semantic fidelity** — same meaning\n",
    "* **Task relevance** — preserve task-critical facts\n",
    "* **Structural cues** — entities, relations, constraints\n",
    "* **Reasoning support** — information required for inference\n",
    "\n",
    "While minimizing:\n",
    "\n",
    "* Redundancy\n",
    "* Irrelevance\n",
    "* Surface-level verbosity\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Types of Context Compression\n",
    "\n",
    "| Type             | Description                           | Example                    |\n",
    "| ---------------- | ------------------------------------- | -------------------------- |\n",
    "| **Extractive**   | Select most important segments        | Top-k passages             |\n",
    "| **Abstractive**  | Generate condensed summary            | Executive summary          |\n",
    "| **Symbolic**     | Convert text → structured form        | Triples, tables            |\n",
    "| **Hierarchical** | Multi-level summarization             | Chapter → section → bullet |\n",
    "| **Adaptive**     | Dynamic compression per task          | Query-aware summary        |\n",
    "| **Learned**      | Neural models trained for compression | Transformer compressors    |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Compression Workflow\n",
    "\n",
    "```\n",
    "Raw Context\n",
    "   ↓\n",
    "Relevance Estimation\n",
    "   ↓\n",
    "Redundancy Removal\n",
    "   ↓\n",
    "Abstraction / Encoding\n",
    "   ↓\n",
    "Compressed Context\n",
    "   ↓\n",
    "LLM Inference\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Mathematical View\n",
    "\n",
    "Let original context be ( C ) and compressed context be ( \\hat{C} ).\n",
    "\n",
    "We want:\n",
    "\n",
    "[\n",
    "\\arg\\min_{\\hat{C}} |\\hat{C}| \\quad \\text{subject to} \\quad\n",
    "\\text{Utility}(\\hat{C}, T) \\ge \\text{Utility}(C, T)\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( T ) = task\n",
    "* Utility = model performance on task\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Compression Techniques\n",
    "\n",
    "#### 7.1 Heuristic Methods\n",
    "\n",
    "* Keyword filtering\n",
    "* Sentence ranking (TF-IDF, BM25)\n",
    "* Sliding window + pruning\n",
    "\n",
    "#### 7.2 Model-Based Compression\n",
    "\n",
    "* Encoder–decoder summarizers\n",
    "* Query-aware summarization\n",
    "* Neural retrievers with learned scoring\n",
    "\n",
    "#### 7.3 Symbolic Compression\n",
    "\n",
    "Convert text to compact structures:\n",
    "\n",
    "| Format            | Example                  |\n",
    "| ----------------- | ------------------------ |\n",
    "| Knowledge triples | (Company, founded, 1976) |\n",
    "| Tables            | Entity → attributes      |\n",
    "| JSON schemas      | Key-value constraints    |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Practical Demonstration\n",
    "\n",
    "### Example: Query-Aware Context Compression\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def compress_context(context, query):\n",
    "    prompt = f\"Summarize the following for answering the query:\\nQuery: {query}\\n\\n{context}\"\n",
    "    summary = summarizer(prompt, max_length=200, min_length=80, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "```\n",
    "\n",
    "**Before (1,500 tokens)**\n",
    "Long document describing climate policy, history, effects, statistics.\n",
    "\n",
    "**After (180 tokens)**\n",
    "Concise summary focusing only on emissions policies relevant to the query.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Context Compression vs Related Concepts\n",
    "\n",
    "| Concept                    | Difference                                         |\n",
    "| -------------------------- | -------------------------------------------------- |\n",
    "| **Summarization**          | General form of compression                        |\n",
    "| **Retrieval**              | Selects relevant context, does not compress        |\n",
    "| **Knowledge Distillation** | Compresses model, not context                      |\n",
    "| **Prompt Engineering**     | Structures context but does not reduce information |\n",
    "| **Memory Management**      | Uses compression as a core tool                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Applications\n",
    "\n",
    "| System          | Role of Context Compression      |\n",
    "| --------------- | -------------------------------- |\n",
    "| RAG systems     | Fit retrieved docs into window   |\n",
    "| Chatbots        | Maintain long conversations      |\n",
    "| Agents          | Preserve task memory             |\n",
    "| Code assistants | Compress large codebases         |\n",
    "| Search engines  | Generate compact knowledge views |\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Advanced: Learned Context Compressors\n",
    "\n",
    "Modern pipelines use **dedicated neural compressors**:\n",
    "\n",
    "```\n",
    "Document → Compressor Transformer → Latent Summary → LLM\n",
    "```\n",
    "\n",
    "Trained using:\n",
    "\n",
    "* Task loss\n",
    "* Reconstruction loss\n",
    "* Information bottleneck objectives\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Failure Modes\n",
    "\n",
    "| Issue                   | Cause                       |\n",
    "| ----------------------- | --------------------------- |\n",
    "| Information loss        | Over-aggressive compression |\n",
    "| Hallucination           | Missing key facts           |\n",
    "| Bias amplification      | Distorted summaries         |\n",
    "| Loss of reasoning steps | Over-abstraction            |\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Design Guidelines\n",
    "\n",
    "* Compress **after retrieval**\n",
    "* Make compression **query-aware**\n",
    "* Preserve **entities, numbers, constraints**\n",
    "* Keep **intermediate reasoning**\n",
    "* Continuously evaluate task performance\n",
    "\n",
    "---\n",
    "\n",
    "### 14. One-Sentence Summary\n",
    "\n",
    "> **Context compression enables LLMs to operate over large information spaces by preserving only the information that matters for the current task, using principled information reduction strategies.**\n",
    "\n",
    "---\n",
    "\n",
    "If you want, next I can explain:\n",
    "**Context Compression vs Prompt Compression**,\n",
    "**Context Windows vs Long-Term Memory**,\n",
    "or **How modern LLMs implement compression internally.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
