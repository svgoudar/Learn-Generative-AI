{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a677759",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Long-Term Memory\n",
    "\n",
    "Long-term memory (LTM) enables generative AI systems to **retain, retrieve, and utilize information across interactions, documents, and time**, beyond a single model context window. It is a core architectural component for building **stateful, personalized, and knowledge-grounded AI systems**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Motivation & Intuition\n",
    "\n",
    "Large language models are **stateless** at inference time:\n",
    "\n",
    "| Limitation      | Explanation                              |\n",
    "| --------------- | ---------------------------------------- |\n",
    "| Context window  | Fixed size; older information is lost    |\n",
    "| No persistence  | Model forgets user history after session |\n",
    "| Hallucination   | Model lacks grounding in real data       |\n",
    "| Personalization | Cannot remember user preferences         |\n",
    "\n",
    "**Long-term memory solves this** by storing external knowledge and interaction history that the model can retrieve when generating responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conceptual Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "   │\n",
    "   ▼\n",
    "Retriever ──────► Long-Term Memory Store\n",
    "   │                  (Vector DB, SQL, Files, APIs)\n",
    "   ▼\n",
    "Prompt Constructor (Query + Retrieved Memory)\n",
    "   │\n",
    "   ▼\n",
    "LLM → Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Types of Long-Term Memory\n",
    "\n",
    "| Type                  | Purpose                  | Examples               |\n",
    "| --------------------- | ------------------------ | ---------------------- |\n",
    "| **Episodic Memory**   | User interaction history | Chat logs, preferences |\n",
    "| **Semantic Memory**   | World / domain knowledge | Documents, manuals     |\n",
    "| **Procedural Memory** | Skills / workflows       | Tool usage patterns    |\n",
    "| **Reflective Memory** | Model self-feedback      | Evaluations, summaries |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Memory Storage Techniques\n",
    "\n",
    "| Method          | Description                                         |\n",
    "| --------------- | --------------------------------------------------- |\n",
    "| Vector Store    | Embedding-based retrieval (FAISS, Pinecone, Chroma) |\n",
    "| Relational DB   | Structured memory (Postgres, SQLite)                |\n",
    "| Document Store  | Raw text (S3, filesystem)                           |\n",
    "| Knowledge Graph | Entity relationships                                |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Retrieval Workflow (RAG + Memory)\n",
    "\n",
    "1. **Encode query → embedding**\n",
    "2. **Search memory store**\n",
    "3. **Select top-k relevant memories**\n",
    "4. **Inject into prompt**\n",
    "5. **Generate response**\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Minimal Working Example\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 1. Create memory\n",
    "docs = [\"User likes concise answers\", \"Project uses PyTorch\"]\n",
    "emb = OpenAIEmbeddings()\n",
    "store = FAISS.from_texts(docs, emb)\n",
    "\n",
    "# 2. Query with memory\n",
    "query = \"How should I explain this model?\"\n",
    "memory = store.similarity_search(query, k=2)\n",
    "\n",
    "# 3. Build prompt\n",
    "context = \"\\n\".join([m.page_content for m in memory])\n",
    "prompt = f\"Context:\\n{context}\\n\\nAnswer the question: {query}\"\n",
    "\n",
    "# 4. Generate\n",
    "llm = OpenAI()\n",
    "response = llm(prompt)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Memory Lifecycle\n",
    "\n",
    "| Stage     | Description                   |\n",
    "| --------- | ----------------------------- |\n",
    "| Ingestion | Store interactions, documents |\n",
    "| Encoding  | Convert to embeddings         |\n",
    "| Indexing  | Build searchable structure    |\n",
    "| Retrieval | Query-time fetch              |\n",
    "| Update    | Summarize, prune, refresh     |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Design Challenges\n",
    "\n",
    "| Issue         | Solution                     |\n",
    "| ------------- | ---------------------------- |\n",
    "| Memory bloat  | Summarization & pruning      |\n",
    "| Stale info    | Time decay, versioning       |\n",
    "| Privacy       | Encryption, scoped access    |\n",
    "| Hallucination | Strict grounding from memory |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Evaluation Metrics\n",
    "\n",
    "| Metric       | What it measures            |\n",
    "| ------------ | --------------------------- |\n",
    "| Recall@k     | Relevant memory retrieved   |\n",
    "| Faithfulness | Response grounded in memory |\n",
    "| Consistency  | Stable personalization      |\n",
    "| Latency      | Retrieval overhead          |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Applications\n",
    "\n",
    "* Personalized assistants\n",
    "* Autonomous agents\n",
    "* Enterprise knowledge bots\n",
    "* Long-term planning systems\n",
    "* Scientific research copilots\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "**Long-term memory transforms LLMs from stateless text generators into persistent, personalized, knowledge-grounded cognitive systems.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
